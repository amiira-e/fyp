{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc296ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007b26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a907c9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.0</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.0</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.0</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>7817.71</td>\n",
       "      <td>C90045638</td>\n",
       "      <td>53860.0</td>\n",
       "      <td>46042.29</td>\n",
       "      <td>M573487274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>7107.77</td>\n",
       "      <td>C154988899</td>\n",
       "      <td>183195.0</td>\n",
       "      <td>176087.23</td>\n",
       "      <td>M408069119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36   \n",
       "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72   \n",
       "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   \n",
       "3     1  CASH_OUT    181.00   C840083671          181.0            0.00   \n",
       "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86   \n",
       "5     1   PAYMENT   7817.71    C90045638        53860.0        46042.29   \n",
       "6     1   PAYMENT   7107.77   C154988899       183195.0       176087.23   \n",
       "\n",
       "      nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
       "0  M1979787155             0.0             0.0        0               0  \n",
       "1  M2044282225             0.0             0.0        0               0  \n",
       "2   C553264065             0.0             0.0        1               0  \n",
       "3    C38997010         21182.0             0.0        1               0  \n",
       "4  M1230701703             0.0             0.0        0               0  \n",
       "5   M573487274             0.0             0.0        0               0  \n",
       "6   M408069119             0.0             0.0        0               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2a0d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6362620, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12cddab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"isFraud\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21266eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2acc2e",
   "metadata": {},
   "source": [
    "# Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28527a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for duplicates\n",
    "# duplicates = df.duplicated()\n",
    "\n",
    "# # print the duplicated rows\n",
    "# print(df[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc9b65",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c725d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Calculate the correlation matrix\n",
    "# corr_matrix = df.corr()\n",
    "\n",
    "# # Plot the correlation matrix as a heatmap\n",
    "# sns.heatmap(corr_matrix, cmap='mako', center=0, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20a415",
   "metadata": {},
   "source": [
    "# Unimodial vs Multimodial (Shape distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "plt.figure(figsize=(20,8))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for feature in features:\n",
    "    plt.subplot(2,3,features.index(feature)+1)\n",
    "    sns.distplot(df[feature],hist=True,color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb115b0f",
   "metadata": {},
   "source": [
    "# Payment Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef814e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create New DataFrame with Count\n",
    "# new_df = df[\"type\"].value_counts().rename_axis('types_of_transaction').reset_index(name='counts')\n",
    "# new_df.head()\n",
    "\n",
    "# #Set lables and values\n",
    "# my_labels = new_df.types_of_transaction\n",
    "# my_values = new_df.counts\n",
    "\n",
    "# #Visualize the pie chart\n",
    "# fig=plt.figure(figsize=(3,3)) # Resize\n",
    "# wp= {'linewidth':0.5,'edgecolor':\"black\"}\n",
    "# ax=fig.add_axes([0,0,1,1]) # Add axis to the figure\n",
    "# ax.axis('equal')\n",
    "# explode=(0.1,0.1,0.1,0.1,0.1)\n",
    "# ax.pie(my_values, labels=my_labels, autopct='%1.2f%%',explode=explode,shadow=True,wedgeprops=wp)\n",
    "# font = {'fontname':'Comic Sans MS'} # Change font\n",
    "# plt.title('Type of Transactions',fontsize=20,color='purple',**font,fontweight='bold')\n",
    "# plt.legend(['CASH_OUT', 'PAYMENT', 'CASH_IN','TRANSFER','DEBIT'])\n",
    "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2521db2",
   "metadata": {},
   "source": [
    "# Barplot to show class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9373fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the values in the isFraud column to Non-Fraud and Fraud\n",
    "df[\"isFraud\"] = df[\"isFraud\"].map({0: \"Non-Fraud\", 1: \"Fraud\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a72ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts of the isFraud column\n",
    "counts = df['isFraud'].value_counts().rename_axis('isFraud').reset_index(name='count')\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d87092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the count of Non-Fraud and Fraud using a barplot\n",
    "plt.figure(figsize=(5.5, 5.5))\n",
    "sns.barplot(x='isFraud', y='count', data=counts, color='pink', edgecolor =\"b\")\n",
    "plt.title('Count of Non-Fraudulent & Fraudulent Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"isFraud\"] = df[\"isFraud\"].replace({\"Non-Fraud\": 0, \"Fraud\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd371e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"isFraud\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cdf4e8",
   "metadata": {},
   "source": [
    "# Feature Encoding on entire dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64965a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9d8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LabelEncoder from Sklearn\n",
    "# library from preprocessing Module.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Creating a instance of label Encoder.\n",
    "le = LabelEncoder()\n",
    "# Using .fit_transform function to fit label\n",
    "# encoder and return encoded label\n",
    "label = le.fit_transform(df['type'])\n",
    "# printing label\n",
    "label\n",
    "# removing the column 'type' from df\n",
    "# as it is of no use now.\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "# Appending the array to our dataFrame\n",
    "# with column name 'type'\n",
    "df[\"type\"] = label\n",
    "# printing Dataframe\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5778b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameDest'])\n",
    "label\n",
    "df.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df[\"nameDest\"] = label\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc78ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameOrig'])\n",
    "label\n",
    "df.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df[\"nameOrig\"] = label\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17994959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 6362620\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows in the dataset\n",
    "num_rows_df = df.shape[0]\n",
    "print(\"Number of rows in the dataset:\", num_rows_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f57a6d",
   "metadata": {},
   "source": [
    "# Split into train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0a04c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998709\n",
      "1    0.001291\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998709\n",
      "1    0.001291\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.99871\n",
      "1    0.00129\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=18)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6662a3ab",
   "metadata": {},
   "source": [
    "# Save train, test and validation sets to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\train.csv\", index=False)\n",
    "X_test.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e07573",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552f334",
   "metadata": {},
   "source": [
    "# OUTLIER LABELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763bcf48",
   "metadata": {},
   "source": [
    "# IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99047393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lower outliers:\n",
      " amount            0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Number of upper outliers:\n",
      " amount             304201\n",
      "oldbalanceOrg     1001209\n",
      "newbalanceOrig     948230\n",
      "oldbalanceDest     707395\n",
      "newbalanceDest     664380\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select the columns you want to check for outliers\n",
    "columns_to_trim = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Calculate IQR for each column\n",
    "q1 = X_train[columns_to_trim].quantile(0.25)\n",
    "q3 = X_train[columns_to_trim].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Determine the lower and upper boundaries for outliers\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Count the number of lower and upper outliers in each column\n",
    "num_lower_outliers = (X_train[columns_to_trim] < lower_bound).sum()\n",
    "num_upper_outliers = (X_train[columns_to_trim] > upper_bound).sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of lower outliers:\\n\",num_lower_outliers)\n",
    "print(f\"\\n\")\n",
    "print(f\"Number of upper outliers:\\n\",num_upper_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3aed3",
   "metadata": {},
   "source": [
    "# Modified Z-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "042d62a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column amount\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 705559\n",
      "\n",
      "\n",
      "Column oldbalanceOrg\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 1733470\n",
      "\n",
      "\n",
      "Column newbalanceOrig\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 2477489\n",
      "\n",
      "\n",
      "Column oldbalanceDest\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 1787314\n",
      "\n",
      "\n",
      "Column newbalanceDest\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 1522615\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the columns you want to check for outliers\n",
    "columns_to_trim = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Calculate modified Z-score for each column\n",
    "for col in columns_to_trim:\n",
    "    # Extract column values\n",
    "    col_values = X_train[col].values\n",
    "\n",
    "    # Calculate median and MAD\n",
    "    med = np.median(col_values)\n",
    "    mad = np.median(np.abs(col_values - med))\n",
    "    if mad == 0:\n",
    "        mad = 1e-6  # Set MAD to a small non-zero value to avoid division by zero\n",
    "\n",
    "    # Calculate modified Z-score\n",
    "    z_score = 0.6745 * (col_values - med) / mad\n",
    "\n",
    "    # Count number of lower and upper outliers\n",
    "    lower_outliers = np.sum(z_score < -2.5)\n",
    "    upper_outliers = np.sum(z_score > 2.5)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Column {col}\")\n",
    "    print(f\"Number of lower outliers for column: {lower_outliers}\")\n",
    "    print(f\"Number of upper outliers: {upper_outliers}\")\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8be4bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import weibull_min\n",
    "\n",
    "# # Select the columns you want to check for outliers\n",
    "# columns_to_trim = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# # Calculate modified Z-score for each column\n",
    "# for col in columns_to_trim:\n",
    "#     # Extract column values\n",
    "#     col_values = X_train[col].values\n",
    "\n",
    "#     # Fit Weibull distribution and extract shape and scale parameters\n",
    "#     shape, loc, scale = weibull_min.fit(col_values, loc=0)\n",
    "\n",
    "#     # Calculate median and WMAD\n",
    "#     med = weibull_min.median(shape, loc=loc, scale=scale)\n",
    "#     n = len(col_values)\n",
    "#     #k = 0.5 / (np.log(np.log(1 / (1 - 0.5 ** (1/n)))) - np.log(np.log(1 / (1 - 0.95 ** (1/n)))))\n",
    "#     MAD = np.median(np.abs(col_values - np.median(col_values)))\n",
    "\n",
    "#     k = 1.4826 * MAD\n",
    "    \n",
    "#     wmad = k * np.power(np.log(2), 1/k) * med\n",
    "\n",
    "#     # Calculate modified Z-score\n",
    "#     mod_z_score = k * (col_values - med) / wmad\n",
    "\n",
    "#     # Count number of lower and upper outliers\n",
    "#     lower_outliers = np.sum(mod_z_score < -2.5)\n",
    "#     upper_outliers = np.sum(mod_z_score > 2.5)\n",
    "\n",
    "#     # Print results\n",
    "#     print(f\"Column {col}\")\n",
    "#     print(f\"Number of lower outliers for column: {lower_outliers}\")\n",
    "#     print(f\"Number of upper outliers: {upper_outliers}\")\n",
    "#     print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a28180cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column amount\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 1218337\n",
      "\n",
      "\n",
      "Column oldbalanceOrg\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 628607\n",
      "\n",
      "\n",
      "Column oldbalanceDest\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 960027\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:49: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column newbalanceOrig\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 0\n",
      "\n",
      "\n",
      "Column newbalanceDest\n",
      "Number of lower outliers for column: 0\n",
      "Number of upper outliers: 3528692\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import weibull_min, beta, lognorm, chi2,gamma,pareto\n",
    "\n",
    "# Select the columns you want to check for outliers\n",
    "columns_to_trim = ['amount', 'oldbalanceOrg', 'oldbalanceDest', 'newbalanceOrig', 'newbalanceDest']\n",
    "\n",
    "# Calculate modified Z-score for each column\n",
    "for col in columns_to_trim:\n",
    "    # Extract column values\n",
    "    col_values = X_train[col].values\n",
    "    \n",
    "    # Replace 0 or negative values with a small positive value\n",
    "    #col_values[col_values <= 0] = 1e-9\n",
    "    col_values += 1e-9\n",
    "    col_values[col_values == 0] = 1e-9\n",
    "\n",
    "\n",
    "    # Fit distribution and extract relevant parameters\n",
    "    if col == 'amount' or col == 'oldbalanceOrg' or col == 'oldbalanceDest':\n",
    "        shape, loc, scale = weibull_min.fit(col_values, loc=0)\n",
    "        med = weibull_min.median(shape, loc=loc, scale=scale)\n",
    "        n = len(col_values)\n",
    "        MAD = np.median(np.abs(col_values - np.median(col_values)))\n",
    "        k = 1.4826 * MAD\n",
    "        wmad = k * np.power(np.log(2), 1/k) * med\n",
    "        mod_z_score = k * (col_values - med) / wmad\n",
    "    \n",
    "    elif col == 'newbalanceDest':\n",
    "        # Fit gamma distribution and extract relevant parameters\n",
    "        shape, loc, scale = gamma.fit(col_values, floc=0)\n",
    "        med = gamma.median(shape, loc=loc, scale=scale)\n",
    "        n = len(col_values)\n",
    "        MAD = np.median(np.abs(col_values - np.median(col_values)))\n",
    "        k = 1.4826 * MAD\n",
    "        wmad = k * np.power(np.log(2), 1/k) * med\n",
    "        if wmad == 0:\n",
    "            #mod_z_score = np.zeros(n)\n",
    "            mod_z_score = np.full(n, np.nan)\n",
    "        else:\n",
    "            mod_z_score = k * (col_values - med) / wmad\n",
    "\n",
    "    elif col == 'newbalanceOrig':\n",
    "        col_values[col_values == 0] = 1e-9\n",
    "        # Fit Pareto distribution and extract relevant parameters\n",
    "        b, loc, scale = pareto.fit(col_values, floc=0)\n",
    "        med = pareto.median(b, loc=loc, scale=scale)\n",
    "        n = len(col_values)\n",
    "        MAD = np.median(np.abs(col_values - np.median(col_values)))\n",
    "        k = 1.4826 * MAD\n",
    "        wmad = k * np.power(np.log(2), 1/k) * med\n",
    "        mod_z_score = k * (col_values - med) / wmad\n",
    "   \n",
    "    # Count number of lower and upper outliers\n",
    "    lower_outliers = np.sum(mod_z_score < -2.5)\n",
    "    upper_outliers = np.sum(mod_z_score > 2.5)\n",
    "    \n",
    "    #Print results\n",
    "    print(f\"Column {col}\")\n",
    "    print(f\"Number of lower outliers for column: {lower_outliers}\")\n",
    "    print(f\"Number of upper outliers: {upper_outliers}\")\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d02f67",
   "metadata": {},
   "source": [
    "# Mean Absolute Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c7b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # Define the columns of interest\n",
    "# columns_to_trim = ['amount', 'oldbalanceOrg', 'newbalanceOrig','oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# # Define the threshold for outliers\n",
    "# threshold = 2\n",
    "\n",
    "# # Loop over the columns of interest and calculate the number of outliers for each column in the train set\n",
    "# for column in columns_to_trim:\n",
    "#     # Calculate the median and MAD for the column in the train set\n",
    "#     median_train = np.median(X_train[column])\n",
    "#     abs_deviation_train = np.abs(X_train[column] - median_train)\n",
    "#     mad_train = np.median(abs_deviation_train)\n",
    "#     # Calculate the upper and lower thresholds based on the MAD\n",
    "#     upper_threshold = median_train + threshold * mad_train\n",
    "#     lower_threshold = median_train - threshold * mad_train\n",
    "#     # Identify the number of outliers in the train set for each threshold\n",
    "#     num_upper_outliers_train = np.sum(X_train[column] > upper_threshold)\n",
    "#     num_lower_outliers_train = np.sum(X_train[column] < lower_threshold)\n",
    "    \n",
    "#     print(f\"Number of lower outliers in {column} in train set:{num_lower_outliers_train}\")\n",
    "#     print(f\"Number of upper outliers in {column} in train set:{num_upper_outliers_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5077793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract the columns of interest from X_train\n",
    "cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "data = X_train[cols]\n",
    "\n",
    "# Loop over each column and find outliers using the IQR method\n",
    "for col in cols:\n",
    "    # Calculate the median and IQR of the column\n",
    "    median = np.median(data[col])\n",
    "    iqr = np.percentile(data[col], 75) - np.percentile(data[col], 25)\n",
    "\n",
    "    # Calculate the lower and upper bounds for outliers\n",
    "    lower_bound = np.percentile(data[col], 25) - 1.5 * iqr\n",
    "    upper_bound = np.percentile(data[col], 75) + 1.5 * iqr\n",
    "\n",
    "    # Identify outliers in the column using a boolean mask\n",
    "    outliers_lower = data[col] < lower_bound\n",
    "    outliers_upper = data[col] > upper_bound\n",
    "    outliers = outliers_lower | outliers_upper\n",
    "\n",
    "    # Print the number of outliers and their proportion in the column\n",
    "    num_outliers = outliers.sum()\n",
    "    prop_outliers = num_outliers / data.shape[0]\n",
    "    print(f\"Column {col} has {num_outliers} outliers ({prop_outliers:.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79701b7a",
   "metadata": {},
   "source": [
    "# Generalized Boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd46810",
   "metadata": {},
   "source": [
    "# Estimate direction and proportion of trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf002f",
   "metadata": {},
   "source": [
    "Is Kolmogorov-Smirnov (K-S) test or the Anderson-Darling (A-D) appropriate when I have extremely lots of data? won't the p-values be misleading if i have too many data\n",
    "When you have a very large amount of data, the p-values from the Kolmogorov-Smirnov (K-S) test or the Anderson-Darling (A-D) test may be misleading due to their sensitivity to sample size. In large samples, even small differences between the distributions of the two tails may lead to rejecting the null hypothesis and concluding that the distributions are significantly different, even if the differences are not practically meaningful.\n",
    "\n",
    "In such cases, you may consider using visual methods such as histograms, boxplots, or quantile-quantile (Q-Q) plots to assess the symmetry and tail behavior of the distribution. These visual methods can help you get a better sense of the shape and spread of the data and make a more informed decision about whether to use symmetric or asymmetric trimming.\n",
    "\n",
    "Alternatively, you can consider using robust statistical methods that are less sensitive to outliers and heavy-tailed distributions. For example, you can use trimmed means or medians to estimate the center of the distribution instead of the sample mean or median, and use trimmed standard deviations or interquartile ranges to estimate the spread of the distribution instead of the sample standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125178f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract the columns of interest\n",
    "cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "data = X_train[cols]\n",
    "\n",
    "# Loop over each column and determine the direction of trimming\n",
    "for col in cols:\n",
    "    # Calculate the median and interquartile range of the column\n",
    "    median = np.median(data[col])\n",
    "    iqr = np.percentile(data[col], 75) - np.percentile(data[col], 25)\n",
    "\n",
    "    # Calculate the skewness of the column\n",
    "    skewness = data[col].skew()\n",
    "\n",
    "    # Determine whether to use symmetric or asymmetric trimming based on the skewness and IQR\n",
    "    if abs(skewness) > 1.5 or iqr > 2 * abs(median):\n",
    "        print(f\"For column {col}, the data is skewed and heavy-tailed, so asymmetric trimming may be appropriate.\")\n",
    "    else:\n",
    "        print(f\"For column {col}, the data is roughly symmetric, so symmetric trimming may be appropriate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a00be1",
   "metadata": {},
   "source": [
    "# Winsorized bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf60132",
   "metadata": {},
   "source": [
    "This output is for the column 'amount' and contains the following information:\n",
    "\n",
    "Trimmed Mean: This is the mean of the bootstrapped samples after trimming the outliers based on the specified trimming proportion. It is equal to 91803.12538081642 for the 'amount' column.\n",
    "95% Bootstrap Confidence Interval (T-Distribution): This is the range of values within which the true mean of the population lies with 95% confidence. It is calculated using the t-distribution and is equal to [179959.81816440757, 180125.3084492971] for the 'amount' column. This means that if we were to repeat this sampling process multiple times and compute the 95% confidence interval each time, we would expect the true mean of the population to lie within this interval 95% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c763ec2",
   "metadata": {},
   "source": [
    "The trimmed mean is an estimation of the central tendency of the amount column after trimming the outliers. The 95% Bootstrap Confidence Interval (T-Distribution) is an interval estimate of the population mean based on the bootstrapped samples.\n",
    "\n",
    "The interval [179959.81816440757, 180125.3084492971] suggests that if we were to repeat the bootstrap process many times and create 95% confidence intervals for each of these iterations, then 95% of the intervals would contain the true population mean.\n",
    "\n",
    "So, in this case, we can interpret the interval as a measure of the precision of the estimate of the population mean. The interval is relatively narrow, which indicates that the estimate of the population mean is relatively precise. However, it is worth noting that the interval is based on a relatively small number of bootstrap samples (50), and it is possible that the interval could be wider with a larger number of bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy library\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# # Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the trimming proportions for each column\n",
    "trim_props = {'amount': 0.27, 'oldbalanceOrg': 0.3, 'newbalanceOrig': 0.3, 'oldbalanceDest': 0.35, 'newbalanceDest': 0.34}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column separately for the train and test set\n",
    "train_trimmed_means = {}\n",
    "#test_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the train and test set\n",
    "    train_bootstrapped_samples = []\n",
    "    #test_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    #test_trimmed_means_list = []\n",
    "    \n",
    "      # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train.index, size=len(X_train), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        # is a line of code that creates a bootstrapped sample for a particular column (col_name) in the training set.\n",
    "        train_sample = X_train.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Randomly select indices from the column in the test set\n",
    "        #test_sample_indices = np.random.choice(X_test.index, size=len(X_test), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the test set\n",
    "        #test_sample = X_test.loc[test_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the train and test set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        #test_bootstrapped_samples.append(test_sample)\n",
    "        \n",
    "        # Calculate the trimmed mean of the bootstrapped sample for the train set\n",
    "        train_trimmed_mean = np.mean(train_sample[(train_sample >= np.percentile(train_sample, 100*trim_props[col_name])) & (train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name])))] )\n",
    "        train_trimmed_means_list.append(train_trimmed_mean)\n",
    "        \n",
    "        # Calculate the trimmed mean of the bootstrapped sample for the test set\n",
    "        #test_trimmed_mean = np.mean(test_sample[(test_sample >= np.percentile(test_sample, 100*trim_props[col_name])) & (test_sample <= np.percentile(test_sample, 100*(1-trim_props[col_name])))] )\n",
    "        #test_trimmed_means_list.append(test_trimmed_mean)\n",
    "\n",
    "    # Calculate the mean of the trimmed means for the train set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "    \n",
    "    # Calculate the mean of the trimmed means for the test set and add it to the dictionary\n",
    "    #test_trimmed_means[col_name] = np.mean(test_trimmed_means_list)\n",
    "    \n",
    "# Print the trimmed means for each column separately for the train and test set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)\n",
    "#print(\"Test set trimmed means: \", test_trimmed_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d88c46",
   "metadata": {},
   "source": [
    "# Perform asymmetric trimming by replacing outliers by bootstrapped trimmed mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the trimming proportions for each column\n",
    "trim_props = {'amount': 0.2, 'oldbalanceOrg': 0.2, 'newbalanceOrig': 0.3, 'oldbalanceDest': 0.27, 'newbalanceDest': 0.3}\n",
    "\n",
    "# Sort the training set by each column\n",
    "sorted_train = X_train.sort_values(by=cols_with_outliers)\n",
    "\n",
    "# Replace the upper-end outliers with the trimmed means\n",
    "for col_name in cols_with_outliers:\n",
    "    if trim_props[col_name] == 0:\n",
    "        continue\n",
    "    upper_percentile = np.percentile(sorted_train[col_name], 100*(1-trim_props[col_name]))\n",
    "    lower_percentile = np.percentile(sorted_train[col_name], trim_props[col_name]*100)\n",
    "    trimmed_mean = train_trimmed_means[col_name]\n",
    "    \n",
    "#     #Symmetric trimming for amount\n",
    "#     if col_name == 'amount':\n",
    "#         sorted_train[col_name] = np.where(sorted_train[col_name] > upper_percentile, trimmed_mean, sorted_train[col_name])\n",
    "#         sorted_train[col_name] = np.where(sorted_train[col_name] < lower_percentile, trimmed_mean, sorted_train[col_name])\n",
    "#     else:\n",
    "#         sorted_train[col_name] = np.where(sorted_train[col_name] > upper_percentile, trimmed_mean, sorted_train[col_name])\n",
    "\n",
    "# Reorder the rows in the training set to their original order\n",
    "X_train = sorted_train.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015025fe",
   "metadata": {},
   "source": [
    "### Test if the trimmed mean has been replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff3b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = (X_train['amount'] == 84412.3409051267).sum()\n",
    "print(count)\n",
    "count = (X_train['oldbalanceOrg'] == 24471.482817391847).sum()\n",
    "print(count)\n",
    "count = (X_train['newbalanceOrig'] == 8985.825872442161).sum()\n",
    "print(count)\n",
    "count = (X_train['oldbalanceDest'] == 156941.59590827962).sum()\n",
    "print(count)\n",
    "count = (X_train['newbalanceDest'] == 162129.3829213959).sum()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dc5d8",
   "metadata": {},
   "source": [
    "# Save train file after handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f358784f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # save the trimmed train to new files\n",
    "X_train.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\trainAFTERHANDLINGOUTLIERS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddbb386",
   "metadata": {},
   "source": [
    "# Check number of outliers after handling the outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb6252",
   "metadata": {},
   "source": [
    "# Modified Z-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379acd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Select the columns you want to check for outliers\n",
    "# columns_to_trim = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# # Convert data to a numpy array if necessary\n",
    "# if not isinstance(X_train, np.ndarray):\n",
    "#     X_train = np.array(X_train)\n",
    "\n",
    "# # Calculate modified Z-score for each column\n",
    "# for i, col in enumerate(columns_to_trim):\n",
    "#     # Extract column values\n",
    "#     col_values = X_train[:, i]\n",
    "\n",
    "#     # Calculate median and MAD\n",
    "#     med = np.median(col_values)\n",
    "#     mad = np.median(np.abs(col_values - med))\n",
    "#     if mad == 0:\n",
    "#         mad = 1e-6  # Set MAD to a small non-zero value to avoid division by zero\n",
    "\n",
    "#     # Calculate modified Z-score\n",
    "#     z_score = 0.6745 * (col_values - med) / mad\n",
    "\n",
    "#     # Count number of lower and upper outliers\n",
    "#     lower_outliers = np.sum(z_score < -3)\n",
    "#     upper_outliers = np.sum(z_score > 3)\n",
    "\n",
    "#     # Print results\n",
    "#     print(col)\n",
    "#     print(f\"Number of lower outliers: {lower_outliers}\")\n",
    "#     print(f\"Number of upper outliers: {upper_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407639ff",
   "metadata": {},
   "source": [
    "# Mean Absolute Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d40270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the columns of interest\n",
    "columns_to_trim = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Define the threshold for outliers\n",
    "threshold = 2\n",
    "\n",
    "# Loop over the columns of interest and calculate the number of outliers for each column in the train set\n",
    "for column in columns_to_trim:\n",
    "    # Calculate the median and MAD for the column in the train set\n",
    "    median_train = np.median(X_train[column])\n",
    "    abs_deviation_train = np.abs(X_train[column] - median_train)\n",
    "    mad_train = np.median(abs_deviation_train)\n",
    "\n",
    "    # Calculate the upper and lower thresholds based on the MAD\n",
    "    upper_threshold = median_train + threshold * mad_train\n",
    "    lower_threshold = median_train - threshold * mad_train\n",
    "\n",
    "    # Identify the number of outliers in the train set for each threshold\n",
    "    num_upper_outliers_train = np.sum(X_train[column] > upper_threshold)\n",
    "    num_lower_outliers_train = np.sum(X_train[column] < lower_threshold)\n",
    "\n",
    "    print(f\"Number of upper outliers in {column} in train set: {num_upper_outliers_train}\")\n",
    "    print(f\"Number of lower outliers in {column} in train set: {num_lower_outliers_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafd6cd5",
   "metadata": {},
   "source": [
    "# Feature Selection-It is generally recommended to perform feature selection after splitting the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cbc99",
   "metadata": {},
   "source": [
    "Yes, you should perform feature selection on your training set, not on your test set. The purpose of feature selection is to identify a subset of features that are most relevant to predicting the target variable, which can help to improve model performance and reduce overfitting.\n",
    "\n",
    "If you perform feature selection on your test set, you risk overfitting to the test set and introducing bias into your model. This is because the test set should be used solely for evaluating model performance, not for making decisions about which features to include in your model.\n",
    "\n",
    "Instead, you should split your data into training and test sets, perform feature selection on the training set, and then use the selected features to train your model on the training set. You can then evaluate the performance of your model on the test set to see how well it generalizes to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdda04c",
   "metadata": {},
   "source": [
    "# NOTE: For feature selection, run the model on all features then show how using feature selection, we get better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99bae9",
   "metadata": {},
   "source": [
    "To make sure that we are not removing any valuable features from the dataset\n",
    "1. Run multiple FS methods and compare\n",
    "2. Evaluate feature importance: Many feature selection methods provide a measure of feature importance or contribution to the model. It is important to carefully examine the feature importance rankings and compare them to domain knowledge or prior research to ensure that important features are not being removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d2b891",
   "metadata": {},
   "source": [
    "# Concept 1-Feature Selection using random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35692974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234dea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier and fit it to the training data\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=18)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
    "\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "y_pred_val = rfc.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "precision = precision_score(y_val, y_pred_val)\n",
    "recall = recall_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d627a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the random forest\n",
    "importances = rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23733dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the features in decreasing order of importance\n",
    "indices = importances.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5dc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names and importance scores in two separate arrays\n",
    "feature_names = X_train.columns\n",
    "feature_importances = importances[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative importance scores\n",
    "cumulative_importances = np.cumsum(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variable importance scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(feature_importances)), feature_importances, align='center')\n",
    "plt.xticks(range(len(feature_importances)), feature_names[indices], rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Variable Importance - Mean Decrease in Gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8018c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative importance scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(feature_importances)), cumulative_importances)\n",
    "plt.xticks(range(len(feature_importances)), feature_names[indices], rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Cumulative Importance Score')\n",
    "plt.title('Cumulative Variable Importance - Mean Decrease in Gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean decrease in accuracy for each feature\n",
    "perm_importances = rfc.feature_importances_\n",
    "perm_scores = []\n",
    "for i in X_train.columns:\n",
    "    X_train_perm = X_train.copy()\n",
    "    X_train_perm[i] = np.random.permutation(X_train_perm[i])\n",
    "    perm_score = rfc.score(X_train_perm, y_train)\n",
    "    perm_scores.append(perm_score)\n",
    "    \n",
    "mean_decrease_accuracy = np.array(perm_scores) - rfc.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the features in decreasing order of importance\n",
    "indices = mean_decrease_accuracy.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names and importance scores in two separate arrays\n",
    "feature_names = X_train.columns\n",
    "feature_importances = mean_decrease_accuracy[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative importance scores\n",
    "cumulative_importances = np.cumsum(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the variable importance scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(feature_importances)), feature_importances, align='center')\n",
    "plt.xticks(range(len(feature_importances)), feature_names[indices], rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.title('Variable Importance - Mean Decrease in Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdca1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative importance scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(feature_importances)), cumulative_importances)\n",
    "plt.xticks(range(len(feature_importances)), feature_names[indices], rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Cumulative Importance Score')\n",
    "plt.title('Cumulative Variable Importance - Mean Decrease in Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ba9eb",
   "metadata": {},
   "source": [
    "# Concept 1- Feature selection using Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Select the top k features based on mutual information\n",
    "k = 5\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = X_train.columns[selected_feature_indices]\n",
    "\n",
    "# Print the names of the selected features\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0605f",
   "metadata": {},
   "source": [
    "# Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e6713",
   "metadata": {},
   "source": [
    "As we can see, there are hyperparameters which can be adjusted:\n",
    "The RandomForestClassifier in scikit-learn has a number of hyperparameters that can be tuned to optimize model performance. Here are some of the most commonly used ones:\n",
    "1. n_estimators\n",
    "2.max_depth\n",
    "3.min_samples_split\n",
    "4.bootstrap\n",
    "5.min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the classifier with the desired hyperparameters\n",
    "rfc = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_split=2, min_samples_leaf=1, max_features='auto', bootstrap=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af848be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7810042",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Mean Decrease in Gini Importance\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "#plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xticks(range(X_train.shape[1]), X_train.columns[indices])\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20049b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(rfc, X_train, y_train, n_repeats=10, random_state=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Mean Decrease in Accuracy Importance\")\n",
    "plt.bar(range(X_train.shape[1]), result.importances_mean[indices],\n",
    "       color=\"r\", align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4d13b",
   "metadata": {},
   "source": [
    "# Concept 1- Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad1292",
   "metadata": {},
   "source": [
    "For classification with small training samples and\n",
    "high dimensionality, feature selection plays an\n",
    "important role in avoiding overfitting problems and\n",
    "improving classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "sel=RFE(RandomForestClassifier(n_estimators=10,random_state=18,n_jobs=-1),n_features_to_select=5)\n",
    "sel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0037830",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294a594",
   "metadata": {},
   "source": [
    "# Concept 1 - Feature Selection by Gradient Boost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ca2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "605f1152",
   "metadata": {},
   "source": [
    "# Feature Selection using ROC and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a5b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy, roc_auc_score\n",
    "from sklearn.feature_selection import Variancethreshold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19a046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59347669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf84a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3001f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1abfdef4",
   "metadata": {},
   "source": [
    "# Concept 2- Feature Extraction -> PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b4c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "484591bf",
   "metadata": {},
   "source": [
    "# Handle class imbalance during training using threshold moving algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c31d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1218c17",
   "metadata": {},
   "source": [
    "# Cost Sensitive Learning to handle class imbalance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
