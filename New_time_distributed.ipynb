{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bbe5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca576dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6362620, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd44e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6354407\n",
      "1       8213\n",
      "Name: isFraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count the number of 0s and 1s in the 'isFraud' column\n",
    "fraud_counts = df['isFraud'].value_counts()\n",
    "\n",
    "# Print the result\n",
    "print(fraud_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2625d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['type'])\n",
    "label\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "df[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameDest'])\n",
    "label\n",
    "df.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameOrig'])\n",
    "label\n",
    "df.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2faee02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac3de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# #Upsampling via SMOTE\n",
    "# smote = SMOTE(sampling_strategy=0.55, random_state=0)\n",
    "\n",
    "# # Fit and apply the resampler to the entire dataset\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0094790",
   "metadata": {},
   "source": [
    "## Reservoir sampling without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3833542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# number of data points to sample from original dataset\n",
    "k= 1500000\n",
    "\n",
    "def reservoir_sampling(iterable, k, header=True):\n",
    "    reservoir = []\n",
    "    for i, item in enumerate(iterable):\n",
    "        if i < k:\n",
    "            reservoir.append(item)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < k:\n",
    "                reservoir[j] = item\n",
    "    return reservoir\n",
    "\n",
    "# Open the input CSV file\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\") as f:\n",
    "    # Check if header line exists\n",
    "    header = True\n",
    "    first_line = f.readline()\n",
    "    if not first_line.startswith('step,type,amount,nameOrig,oldbalanceOrg,newbalanceOrig,nameDest,oldbalanceDest,newbalanceDest,isFraud,isFlaggedFraud'):\n",
    "        header = False\n",
    "        f.seek(0)  # Rewind file pointer to beginning\n",
    "\n",
    "    # Sample from remaining lines\n",
    "    sampled_lines = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i < k:\n",
    "            sampled_lines.append(line)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < k:\n",
    "                sampled_lines[j] = line\n",
    "\n",
    "# Open the output CSV file and write the subsample to it\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\", mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    if header:\n",
    "        writer.writerow(first_line.strip().split(','))\n",
    "    for line in sampled_lines:\n",
    "        writer.writerow(line.strip().split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329a5aa",
   "metadata": {},
   "source": [
    "## Pre-process larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed453d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "846a5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['type'])\n",
    "label\n",
    "df_sample_big.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample_big[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameDest'])\n",
    "label\n",
    "df_sample_big.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameOrig'])\n",
    "label\n",
    "df_sample_big.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4fdb235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998707\n",
      "1    0.001293\n",
      "Name: isFraud, dtype: float64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_resampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8728\\4283395836.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Assuming X contains your features and y contains your target variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Print class distribution after split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_resampled' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample_big.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample_big['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# #Upsampling via SMOTE\n",
    "# smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "# #Downsample via RandomUnderSampler\n",
    "# rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "# #Application of the resampling methods\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "# X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da522fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big = df_sample_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1c38e",
   "metadata": {},
   "source": [
    "### Big dataset: Pre-train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b23b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# define number of folds for cross-validation\n",
    "num_folds = 2\n",
    "\n",
    "# create KFold cross-validation object\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# create arrays to store training and validation loss for each epoch\n",
    "train_losses = np.zeros((num_folds, 4))\n",
    "val_losses = np.zeros((num_folds, 4))\n",
    "\n",
    "# loop over the folds\n",
    "fold_no = 1\n",
    "for train, val in kfold.split(X_train_resampled_final, y_train_resampled_final): \n",
    "\n",
    "    # create model\n",
    "    pre_trained_model = Sequential()\n",
    "    # add convolutional layer\n",
    "    pre_trained_model.add(Conv1D(filters=16, kernel_size=2, input_shape=(10, 1)))\n",
    "    # add batch normalization layer\n",
    "    pre_trained_model.add(BatchNormalization())\n",
    "    # add activation function\n",
    "    pre_trained_model.add(Conv1D(8, 2, activation='tanh'))\n",
    "    pre_trained_model.add(BatchNormalization())\n",
    "    # add pooling layer\n",
    "    pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "    # flatten output to feed into fully connected layer\n",
    "    pre_trained_model.add(Flatten()) \n",
    "    # add fully connected layer\n",
    "    pre_trained_model.add(Dense(5, activation='tanh', kernel_regularizer=l1(0.0001714)))\n",
    "    # add dropout layer\n",
    "    layer = Dropout(0.1)\n",
    "    pre_trained_model.add(layer)\n",
    "    # add output layer\n",
    "    pre_trained_model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt_new = keras.optimizers.RMSprop(learning_rate=0.000871)\n",
    "    # opt_new = keras.optimizers.Adam(learning_rate=0.0171)\n",
    "\n",
    "    # compile model\n",
    "    pre_trained_model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "    \n",
    "    # record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    # train model for each fold\n",
    "    history = pre_trained_model.fit(X_train_resampled_final[train], y_train_resampled_final[train],\n",
    "                                     epochs=4, batch_size=32, validation_data=(X_train_resampled_final[val], y_train_resampled_final[val]))\n",
    "    \n",
    "    # record end time and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'Fold {fold_no} elapsed time: {elapsed_time:.2f} seconds')\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # make predictions on validation set\n",
    "    y_pred = pre_trained_model.predict(X_train_resampled_final[val])\n",
    "\n",
    "    # convert predictions to binary labels\n",
    "    y_pred = np.round(y_pred)\n",
    "\n",
    "    # print classification report\n",
    "    print(classification_report(y_train_resampled_final[val], y_pred))\n",
    "    \n",
    "    # store training and validation loss for each epoch\n",
    "    train_losses[fold_no-1] = history.history['loss']\n",
    "    val_losses[fold_no-1] = history.history['val_loss']\n",
    "    \n",
    "    # increment fold number\n",
    "    fold_no += 1\n",
    "    \n",
    "# calculate mean training and validation loss across all folds for each epoch\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "\n",
    "# plot training and validation loss curves for each epoch\n",
    "plt.plot(mean_train_loss, label='Training Loss')\n",
    "plt.plot(mean_val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e627288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = model.layers[0]\n",
    "batch_layer = model.layers[1]\n",
    "conv2_layer = model.layers[2]\n",
    "batch2_layer = model.layers[3]\n",
    "maxpooling_layer = model.layers[4]\n",
    "flatten_layer = model.layers[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f73a8",
   "metadata": {},
   "source": [
    "## Go back to smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac218053",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.55, random_state=0)\n",
    "\n",
    "# Fit and apply the resampler to the entire dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036edbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled,  y_resampled, test_size=0.1, stratify= y_resampled, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbaa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bdbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29beb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "     X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "   # X_train_resampled_final[:, i] = replace_outliers_with_mad(X_train_resampled_final[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import module\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # compute required values\n",
    "# scaler = StandardScaler()\n",
    "# model = scaler.fit(X_train_resampled_final)\n",
    "# X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute required values\n",
    "# scaler = StandardScaler()\n",
    "# model = scaler.fit(X_test)\n",
    "# X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5908adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.keras.layers import Reshape, LSTM, Dense, TimeDistributed, Conv1D, BatchNormalization, MaxPooling1D, Flatten\n",
    "\n",
    "encoding_dim = 6\n",
    "num_features = 10 # number of input variables\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "encoder = tf.keras.layers.Dense(units=10, activation='elu', input_shape=input_shape)\n",
    "first_hidden_layer = tf.keras.layers.Dense(units=encoding_dim, activation='elu')\n",
    "decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# Freeze all trainable layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "V = np.zeros((num_features, encoding_dim))\n",
    "\n",
    "# 1st for loop: Encoding\n",
    "for i in range(num_features):\n",
    "    V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Reshape the input data to match the expected input shape of the CNN model\n",
    "V = np.reshape(V, (1, 1, num_features, encoding_dim))\n",
    "V = np.transpose(V, (0, 1, 3, 2))\n",
    "V = np.reshape(V, (1, -1, num_features, 1))\n",
    "\n",
    "# Pass the input data to the pre-trained CNN model\n",
    "C = CNN(V)\n",
    "\n",
    "# Reshape the output of the CNN layer to match the expected input shape of the LSTM layer\n",
    "C = np.reshape(C, (1, C.shape[1], C.shape[2]))\n",
    "\n",
    "# Add the LSTM layers to the model\n",
    "LSTM_model = tf.keras.Sequential()\n",
    "LSTM1 = LSTM(units=128, return_sequences=True, input_shape=(None, num_features*encoding_dim+1))\n",
    "LSTM_model.add(LSTM1)\n",
    "\n",
    "# Add a fully connected layer\n",
    "LSTM_model.add(Dense(units=20, activation='tanh', kernel_regularizer=l1(0.00114)))\n",
    "\n",
    "# Add a dropout layer\n",
    "LSTM_model.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "# Add the output layer\n",
    "LSTM_model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.0015\n",
    "\n",
    "# Create the Adam optimizer with the defined learning rate\n",
    "opt_LSTM = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "LSTM_model.compile(optimizer=opt_LSTM, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3cb287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(LSTM_model, to_file='LSTM_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67120c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc49cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, Flatten, LSTM, Concatenate, TimeDistributed, Dense\n",
    "\n",
    "# cnn_input = Input(shape=(None, 10, 1))\n",
    "# cnn_layer = TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'))(cnn_input)\n",
    "# cnn_layer = TimeDistributed(BatchNormalization())(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'))(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(BatchNormalization())(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(MaxPooling1D(pool_size=2))(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(Flatten())(cnn_layer)\n",
    "\n",
    "# # lstm_input = Input(shape=(None, num_features*encoding_dim+1))\n",
    "# lstm_layer = LSTM(units=128, return_sequences=True)(cnn_layer)\n",
    "\n",
    "# concat_layer = Concatenate()([cnn_layer, lstm_layer])\n",
    "# dense_layer = Dense(64)(concat_layer)\n",
    "# output_layer = Dense(1)(dense_layer)\n",
    "\n",
    "# model = Model(inputs=[cnn_input, lstm_input], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = model.layers[0]\n",
    "batch_layer = model.layers[1]\n",
    "conv2_layer = model.layers[2]\n",
    "batch2_layer = model.layers[3]\n",
    "maxpooling_layer = model.layers[4]\n",
    "flatten_layer = model.layers[5]\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "# cnn_input = Input(shape=(None, 10, 1))\n",
    "# cnn_layer = TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'))(cnn_input)\n",
    "# cnn_layer = TimeDistributed(BatchNormalization())(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(Conv1D(64, kernel_size=3, activation='relu'))(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(BatchNormalization())(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(MaxPooling1D(pool_size=2))(cnn_layer)\n",
    "# cnn_layer = TimeDistributed(Flatten())(cnn_layer)\n",
    "\n",
    "reshape_layer = Reshape((-1, flatten_layer.output_shape[-1]))(flatten_layer.output)\n",
    "lstm_layer = LSTM(64)(reshape_layer)\n",
    "dense_layer = Dense(32, activation='relu')(lstm_layer)\n",
    "output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
    "\n",
    "new_model = Model(inputs=model.inputs, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.0015\n",
    "\n",
    "# Create the Adam optimizer with the defined learning rate\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "new_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "new_model.fit(X_train_resampled_final, y_train_resampled_final, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967efcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(new_model, to_file='new_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# # Define the learning rate\n",
    "# learning_rate = 0.0015\n",
    "\n",
    "# # Create the Adam optimizer with the defined learning rate\n",
    "# optimizer = Adam(lr=learning_rate)\n",
    "\n",
    "# # Compile the model\n",
    "# new_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# # Fit the model\n",
    "# new_model.fit([X_cnn_input, X_lstm_input], y_train_resampled_final, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27745e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# # Get predictions on X_test\n",
    "# y_pred = new_model.predict([X_test_cnn_input, X_test_lstm_input])\n",
    "\n",
    "# # Convert probabilities to binary predictions\n",
    "# y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# # Calculate evaluation metrics\n",
    "# f1 = f1_score(y_test, y_pred_binary)\n",
    "# precision = precision_score(y_test, y_pred_binary)\n",
    "# recall = recall_score(y_test, y_pred_binary)\n",
    "\n",
    "# print('F1 score:', f1)\n",
    "# print('Precision:', precision)\n",
    "# print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc49de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cnn_input = np.expand_dims(X_test, axis=2) # shape: (None, 10, 1)\n",
    "X_test_lstm_input = np.expand_dims(X_test, axis=1) # shape: (None, 1, 10)\n",
    "X_test_lstm_input = np.repeat(X_test_lstm_input, 61, axis=1) # shape: (None, 61, 10)\n",
    "\n",
    "y_pred = new_model.predict([X_test_cnn_input, X_test_lstm_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79711fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cnn_input = np.expand_dims(X_test, axis=2) # shape: (None, 10, 1)\n",
    "X_test_lstm_input = np.expand_dims(X_test, axis=1) # shape: (None, 1, 10)\n",
    "X_test_lstm_input = np.transpose(X_test_lstm_input, (0, 2, 1)) # shape: (None, 10, 1)\n",
    "X_test_lstm_input = np.repeat(X_test_lstm_input, 61, axis=2) # shape: (None, 10, 61)\n",
    "\n",
    "y_pred = new_model.predict([X_test_cnn_input, X_test_lstm_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Generate predicted probabilities for the test set\n",
    "# y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# # Compute ROC curve and ROC area for each class\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot ROC curve\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
