{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6aee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946fd948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7df038d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>259</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>246917.44</td>\n",
       "      <td>C1498821826</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1385924981</td>\n",
       "      <td>3175848.97</td>\n",
       "      <td>3422766.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161</td>\n",
       "      <td>CASH_IN</td>\n",
       "      <td>165577.48</td>\n",
       "      <td>C1959842869</td>\n",
       "      <td>242080.31</td>\n",
       "      <td>407657.78</td>\n",
       "      <td>C125997794</td>\n",
       "      <td>653770.69</td>\n",
       "      <td>488193.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>331</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11147.81</td>\n",
       "      <td>C690176304</td>\n",
       "      <td>21271.00</td>\n",
       "      <td>10123.19</td>\n",
       "      <td>M1395131038</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>3758.44</td>\n",
       "      <td>C1065078045</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>M1924230681</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>355</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>295770.07</td>\n",
       "      <td>C278303549</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1920150261</td>\n",
       "      <td>429733.98</td>\n",
       "      <td>725504.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699995</th>\n",
       "      <td>400</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>12484.57</td>\n",
       "      <td>C2051987337</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>M1300803541</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699996</th>\n",
       "      <td>187</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>27274.11</td>\n",
       "      <td>C404473655</td>\n",
       "      <td>10231.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>M700503255</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699997</th>\n",
       "      <td>225</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>29426.83</td>\n",
       "      <td>C1350671817</td>\n",
       "      <td>49601.00</td>\n",
       "      <td>20174.17</td>\n",
       "      <td>M161969198</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699998</th>\n",
       "      <td>354</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>21747.21</td>\n",
       "      <td>C1288280262</td>\n",
       "      <td>3309.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C929278329</td>\n",
       "      <td>58996.31</td>\n",
       "      <td>80743.52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699999</th>\n",
       "      <td>254</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>157659.00</td>\n",
       "      <td>C2083088314</td>\n",
       "      <td>230708.00</td>\n",
       "      <td>73049.00</td>\n",
       "      <td>C2117904231</td>\n",
       "      <td>93659.92</td>\n",
       "      <td>251318.91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        step      type     amount     nameOrig  oldbalanceOrg  newbalanceOrig  \\\n",
       "0        259  CASH_OUT  246917.44  C1498821826           0.00            0.00   \n",
       "1        161   CASH_IN  165577.48  C1959842869      242080.31       407657.78   \n",
       "2        331   PAYMENT   11147.81   C690176304       21271.00        10123.19   \n",
       "3        304   PAYMENT    3758.44  C1065078045           0.00            0.00   \n",
       "4        355  CASH_OUT  295770.07   C278303549           0.00            0.00   \n",
       "...      ...       ...        ...          ...            ...             ...   \n",
       "699995   400   PAYMENT   12484.57  C2051987337           0.00            0.00   \n",
       "699996   187   PAYMENT   27274.11   C404473655       10231.00            0.00   \n",
       "699997   225   PAYMENT   29426.83  C1350671817       49601.00        20174.17   \n",
       "699998   354  CASH_OUT   21747.21  C1288280262        3309.24            0.00   \n",
       "699999   254  CASH_OUT  157659.00  C2083088314      230708.00        73049.00   \n",
       "\n",
       "           nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud  \n",
       "0       C1385924981      3175848.97      3422766.41        0               0  \n",
       "1        C125997794       653770.69       488193.21        0               0  \n",
       "2       M1395131038            0.00            0.00        0               0  \n",
       "3       M1924230681            0.00            0.00        0               0  \n",
       "4       C1920150261       429733.98       725504.05        0               0  \n",
       "...             ...             ...             ...      ...             ...  \n",
       "699995  M1300803541            0.00            0.00        0               0  \n",
       "699996   M700503255            0.00            0.00        0               0  \n",
       "699997   M161969198            0.00            0.00        0               0  \n",
       "699998   C929278329        58996.31        80743.52        0               0  \n",
       "699999  C2117904231        93659.92       251318.91        0               0  \n",
       "\n",
       "[700000 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917400ef",
   "metadata": {},
   "source": [
    "## Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40ccfec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a74228b",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf2a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ffe88c",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74715a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f2b2b",
   "metadata": {},
   "source": [
    "## Noisy samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb4d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939913aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30bdaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ed2ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # flatten the array using the ravel function\n",
    "# y_train_resampled_final_flattened = np.ravel(y_train_resampled_final)\n",
    "\n",
    "# counts = np.bincount(y_train_resampled_final_flattened)\n",
    "# print(\"Class 0 count:\", counts[0])\n",
    "# print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9187674",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d223e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set trimmed means:  {'amount': 592784.1178142517, 'oldbalanceOrg': 265538.61685055046, 'newbalanceOrig': 176058.12972740884, 'oldbalanceDest': 673540.5901208639, 'newbalanceDest': 1031960.2081236535}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "random.seed(0)\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.01, 'oldbalanceOrg': 0.07, 'newbalanceOrig': 0.015, 'oldbalanceDest': 0.015, 'newbalanceDest': 0.01}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train_resampled_final.loc[X_train_resampled_final[col_name] > np.percentile(X_train_resampled_final[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "    # Replace the outliers in the test set with the trimmed means obtained from the train set\n",
    "    test_outliers = X_test.loc[X_test[col_name] > np.percentile(X_test[col_name], 100*(1-trim_props[col_name])), col_name]\n",
    "    X_test.loc[test_outliers.index, col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf78d39",
   "metadata": {},
   "source": [
    "## Scale train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46c3e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b4b44",
   "metadata": {},
   "source": [
    "## Scale test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed2b1c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83707cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c9b81",
   "metadata": {},
   "source": [
    "## GOOD ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905908fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# create the autoencoder\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 20\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "output_layer = Dense(input_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=32)\n",
    "\n",
    "# create a new model that outputs the hidden layer\n",
    "hidden_layer_output = autoencoder.layers[1].output\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=hidden_layer_output)\n",
    "\n",
    "# get the hidden layer output for a sample\n",
    "sample_hidden_output = hidden_layer_model.predict(X_train_resampled_final[0].reshape(1, -1))\n",
    "print(sample_hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# create the autoencoder\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 20\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=32)\n",
    "\n",
    "# extract hidden layer output from autoencoder\n",
    "hidden_layer_output = autoencoder.layers[1].output\n",
    "\n",
    "# create the mediator network with the hidden layer output as input\n",
    "mediator_input_layer = Input(shape=hidden_layer_output.shape[1:])\n",
    "mediator_hidden_layer = Dense(11, activation='relu')(mediator_input_layer)\n",
    "mediator_output_layer = Dense(2, activation='sigmoid')(mediator_hidden_layer)\n",
    "\n",
    "mediator_network = Model(inputs=mediator_input_layer, outputs=mediator_output_layer)\n",
    "mediator_network.compile(optimizer='adam', loss='mse')\n",
    "mediator_network.fit(hidden_layer_model.predict(X_train_resampled_final), y_train_resampled_final, epochs=1, batch_size=32)\n",
    "\n",
    "# extract hidden layer output from mediator network\n",
    "agent_hidden_layer_output = mediator_network.layers[1].output\n",
    "\n",
    "agent_input_layer = Input(shape=agent_hidden_layer_output.shape[1:])\n",
    "agent_hidden_layer = Dense(5, activation='relu')(agent_input_layer)\n",
    "agent_output_layer = Dense(2, activation='softmax')(agent_hidden_layer)\n",
    "\n",
    "agent_network = Model(inputs=agent_input_layer, outputs=agent_output_layer)\n",
    "agent_network.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92952ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediator_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c503ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f96cb8",
   "metadata": {},
   "source": [
    "## Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62477721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer.pop(0)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size), size=batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d602597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def REWARD(action, label, fraud_class='DF', non_fraud_class='DN', reward_fraud=1, reward_non_fraud=0.5, penalty=-1):\n",
    "#     terminal = 0\n",
    "#     if label == fraud_class:\n",
    "#         if action == label:\n",
    "#             reward = reward_fraud\n",
    "#         else:\n",
    "#             reward = penalty\n",
    "#             terminal = 1\n",
    "#     else:\n",
    "#         if action == label:\n",
    "#             reward = reward_non_fraud\n",
    "#         else:\n",
    "#             reward = penalty * reward_non_fraud\n",
    "#     return reward, terminal\n",
    "\n",
    "def REWARD(action, label, lambda_val, is_fraud):\n",
    "    \"\"\"\n",
    "    Calculates the reward for a given action and label.\n",
    "    \n",
    "    Parameters:\n",
    "    - action: The action taken by the agent.\n",
    "    - label: The true label of the transaction.\n",
    "    - lambda_val: The reward value when the agent correctly classifies a non-fraudulent transaction.\n",
    "    - is_fraud: A boolean value indicating whether the transaction is fraudulent or not.\n",
    "    \n",
    "    Returns:\n",
    "    - reward: The reward value.\n",
    "    - terminal: A boolean indicating whether the episode is over or not.\n",
    "    \"\"\"\n",
    "    if is_fraud:\n",
    "        if action == label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = True\n",
    "    else:\n",
    "        if action == label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "        terminal = False\n",
    "    \n",
    "    return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42b5f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_shape = X_train_resampled_final.shape[1]\n",
    "output_shape =X_train_resampled_final.shape[1]\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.9999\n",
    "epsilon_min = 0.01\n",
    "batch_size = 32\n",
    "num_episodes = 2\n",
    "\n",
    "# Initialize replay memory D with M capacity\n",
    "M = 10000\n",
    "replay_memory = ReplayBuffer(M)\n",
    "\n",
    "# Randomly initialize parameters Î¸\n",
    "theta_agent = np.random.normal(0, 0.1, size=(input_shape, output_shape))\n",
    "\n",
    "# Loop over episodes\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "    # Shuffle D\n",
    "    np.random.shuffle(replay_memory.buffer)\n",
    "\n",
    "\n",
    "    # Initialize state s1\n",
    "    state = X_train_resampled_final[0]\n",
    "\n",
    "    # Reset the episode\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "\n",
    "    # Loop over time steps\n",
    "    while not done:\n",
    "\n",
    "        # Choose an action: at = Ï€Î¸(st)\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(output_shape)\n",
    "        else:\n",
    "            q_values = Q(state, theta_agent)\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state = X_train_resampled_final[t+1]\n",
    "        reward, terminal = REWARD(action, y[t+1])\n",
    "\n",
    "        # Store (st, at, rt, st+1, terminalt) to M\n",
    "        replay_memory.add(state, action, reward, next_state, terminal)\n",
    "\n",
    "        # Randomly sample (sj, aj, rj, sj+1, terminalj) from M\n",
    "        batch = replay_memory.sample(batch_size)\n",
    "\n",
    "        # Set yj\n",
    "        y = np.zeros(batch_size)\n",
    "        next_state_q_values = np.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            if batch['terminal'][i]:\n",
    "                y[i] = batch['reward'][i]\n",
    "            else:\n",
    "                next_state_q_values[i] = Q(batch['next_state'][i], theta_agent)\n",
    "                y[i] = batch['reward'][i] + gamma * np.max(next_state_q_values[i])\n",
    "\n",
    "        # Perform a gradient descent step\n",
    "        loss = agent_loss(batch['state'], batch['action'], y, theta_agent)\n",
    "        theta_agent -= 0.001 * loss\n",
    "\n",
    "        # Update the state and total reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "\n",
    "        # Break if the episode is done\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print the total reward for the episode\n",
    "    print('Episode', episode + 1, '- Total Reward:', total_reward)\n",
    "\n",
    "reward, terminal = REWARD(action, y[t+1], lambda_val, X[t+1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189e47b",
   "metadata": {},
   "source": [
    "# Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb3394",
   "metadata": {},
   "source": [
    "## Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab161b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assuming df is your dataframe\n",
    "new_order = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFlaggedFraud', 'type', 'nameDest', 'nameOrig', 'isFraud']\n",
    "\n",
    "# create a new dataframe with columns in the desired order\n",
    "df_sample = df_sample.drop(columns=['isFraud']).assign(isFraud=df_sample['isFraud'])\n",
    "\n",
    "# check that the new dataframe has columns in the desired order\n",
    "print(df_sample.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b180cc",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_resampled_final = df_sample.iloc[:, :10] # extract all rows and first 10 columns\n",
    "# y_train_resampled_final = df_sample.iloc[:, -1] # extract last column of entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c301a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba4f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65f0d8e",
   "metadata": {},
   "source": [
    "## Treat outliers on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "random.seed(0)\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.01, 'oldbalanceOrg': 0.07, 'newbalanceOrig': 0.015, 'oldbalanceDest': 0.015, 'newbalanceDest': 0.01}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train_resampled_final.loc[X_train_resampled_final[col_name] > np.percentile(X_train_resampled_final[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "    # Replace the outliers in the test set with the trimmed means obtained from the train set\n",
    "    test_outliers = X_test.loc[X_test[col_name] > np.percentile(X_test[col_name], 100*(1-trim_props[col_name])), col_name]\n",
    "    X_test.loc[test_outliers.index, col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cbb6c",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)\n",
    "\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# create the autoencoder\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 20\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "output_layer = Dense(input_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=32)\n",
    "\n",
    "# create a new model that outputs the hidden layer\n",
    "hidden_layer_output = autoencoder.layers[1].output\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=hidden_layer_output)\n",
    "\n",
    "# get the hidden layer output for a sample\n",
    "sample_hidden_output = hidden_layer_model.predict(X_train_resampled_final[0].reshape(1, -1))\n",
    "print(sample_hidden_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "\n",
    "# # create the autoencoder\n",
    "# input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "# encoding_dim = 20\n",
    "# decoding_dim = 10\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "# output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "# autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=32)\n",
    "\n",
    "# # extract hidden layer output from autoencoder\n",
    "# hidden_layer_output = autoencoder.layers[1].output\n",
    "\n",
    "# # create the mediator network with the hidden layer output as input\n",
    "# mediator_input_layer = Input(shape=hidden_layer_output.shape[1:])\n",
    "# mediator_hidden_layer = Dense(10, activation='relu')(mediator_input_layer)\n",
    "# mediator_output_layer = Dense(2, activation='sigmoid')(mediator_hidden_layer)\n",
    "\n",
    "# mediator_network = Model(inputs=mediator_input_layer, outputs=mediator_output_layer)\n",
    "# mediator_network.compile(optimizer='adam', loss='mse')\n",
    "# mediator_network.fit(hidden_layer_model.predict(X_train_resampled_final), y_train_resampled_final, epochs=1, batch_size=32)\n",
    "\n",
    "# # extract hidden layer output from mediator network\n",
    "# agent_hidden_layer_output = mediator_network.layers[1].output\n",
    "\n",
    "# agent_input_layer = Input(shape=agent_hidden_layer_output.shape[1:])\n",
    "# agent_hidden_layer = Dense(5, activation='relu')(agent_input_layer)\n",
    "# agent_output_layer = Dense(2, activation='softmax')(agent_hidden_layer)\n",
    "\n",
    "# agent_network = Model(inputs=agent_input_layer, outputs=agent_output_layer)\n",
    "# agent_network.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# create the autoencoder\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "encoding_dim = 20\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "#MSE is a common choice for reconstruction-based autoencoders.\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=32)\n",
    "\n",
    "# extract hidden layer output from autoencoder\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "hidden_layer_output = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# create the mediator network with the hidden layer output as input\n",
    "mediator_input_layer = Input(shape=(encoding_dim,))\n",
    "mediator_hidden_layer = Dense(10, activation='relu')(mediator_input_layer)\n",
    "mediator_output_layer = Dense(2, activation='sigmoid')(mediator_hidden_layer)\n",
    "\n",
    "mediator_network = Model(inputs=mediator_input_layer, outputs=mediator_output_layer)\n",
    "mediator_network.compile(optimizer='adam', loss='mse')\n",
    "mediator_network.fit(hidden_layer_output, y_train_resampled_final, epochs=1)\n",
    "\n",
    "# extract hidden layer output from mediator network\n",
    "agent_hidden_layer_output = mediator_network.layers[1].output\n",
    "\n",
    "agent_input_layer = Input(shape=agent_hidden_layer_output.shape[1:])\n",
    "agent_hidden_layer = Dense(5, activation='relu')(agent_input_layer)\n",
    "agent_output_layer = Dense(2, activation='softmax')(agent_hidden_layer)\n",
    "\n",
    "agent_network = Model(inputs=agent_input_layer, outputs=agent_output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437faf6",
   "metadata": {},
   "source": [
    "## Check overfitting/underfitting in autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad836b",
   "metadata": {},
   "source": [
    "## Baysian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4740fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from skopt import gp_minimize, space\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the objective function to optimize\n",
    "def objective_function(params):\n",
    "    encoding_dim, learning_rate = params\n",
    "\n",
    "    # create the autoencoder\n",
    "    input_dim = X_train_resampled_final.shape[1]\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "    for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "        X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "        autoencoder.fit(X_train_fold, X_train_fold, epochs=1, batch_size=32, verbose=0)\n",
    "        val_loss = autoencoder.evaluate(X_val_fold, X_val_fold, verbose=0)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    # Return the average validation loss across all folds\n",
    "    return np.mean(val_losses)\n",
    "\n",
    "# Define the search space for the hyperparameters\n",
    "search_space = [\n",
    "    space.Integer(10, 20, name='encoding_dim'),\n",
    "    space.Real(0.001, 0.1, name='learning_rate'),\n",
    "]\n",
    "\n",
    "# Run Bayesian optimization to find the optimal hyperparameters\n",
    "result = gp_minimize(\n",
    "    objective_function,\n",
    "    search_space,\n",
    "    n_calls=10,\n",
    ")\n",
    "\n",
    "# Print the best hyperparameters and validation loss\n",
    "print(f\"Best Hyperparameters: {result.x}\")\n",
    "print(f\"Validation Loss: {result.fun}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91728fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(mediator_network, to_file='mediator_network.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54194c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network, to_file='agent_network.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediator_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b07ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0c23b",
   "metadata": {},
   "source": [
    "## Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab06b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "# from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "# from sklearn.model_selection import KFold\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "# # Define search space for autoencoder hyperparameters\n",
    "# space = {\n",
    "#     'encoding_dim': hp.quniform('encoding_dim', 10, 50, 1),\n",
    "#     'decoding_dim': hp.quniform('decoding_dim', 5, 20, 1),\n",
    "#     'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "#     'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "# }\n",
    "\n",
    "# # Define function to optimize\n",
    "# def optimize(params):\n",
    "#     encoding_dim = int(params['encoding_dim'])\n",
    "#     decoding_dim = int(params['decoding_dim'])\n",
    "#     batch_size = params['batch_size']\n",
    "#     learning_rate = params['learning_rate']\n",
    "    \n",
    "#     # Define autoencoder architecture\n",
    "#     input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "#     input_layer = Input(shape=(input_dim,))\n",
    "#     hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "#     output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "#     autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#     autoencoder.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "\n",
    "#     # Define cross-validation parameters\n",
    "#     kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "#     losses = []\n",
    "\n",
    "#     # Train and evaluate model using cross-validation\n",
    "#     for train_idx, val_idx in kf.split(X_train_resampled_final):\n",
    "#         # Split data into training and validation sets\n",
    "#         train_data, val_data = X_train_resampled_final[train_idx], X_train_resampled_final[val_idx]\n",
    "\n",
    "#         # Train model\n",
    "#         autoencoder.fit(train_data, train_data, epochs=1, batch_size=batch_size, validation_data=(val_data, val_data), verbose=0)\n",
    "\n",
    "#         # Evaluate model\n",
    "#         val_loss = autoencoder.evaluate(val_data, val_data, verbose=0)\n",
    "#         losses.append(val_loss)\n",
    "\n",
    "#     # Calculate mean validation loss across folds\n",
    "#     mean_loss = np.mean(losses)\n",
    "\n",
    "#     return {'loss': mean_loss, 'status': STATUS_OK}\n",
    "\n",
    "# # Run hyperparameter optimization\n",
    "# best = fmin(fn=optimize, space=space, algo=tpe.suggest, max_evals=7)\n",
    "\n",
    "# print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38edc1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: name 'activation' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'activation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3252\\3028204734.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;31m# Run hyperparameter optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best hyperparameters:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             )\n\u001b[1;32m--> 892\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3252\\3028204734.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_resampled_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0minput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mhidden_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity_regularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0moutput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mautoencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'activation' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "space = {\n",
    "    'encoding_dim': hp.quniform('encoding_dim', 10, 50, 1),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "    'patience': hp.quniform('patience', 1, 10, 1),\n",
    "    'l1_reg': hp.uniform('l1_reg', 0, 0.1),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid', 'tanh']),\n",
    "}\n",
    "\n",
    "\n",
    "def optimize(params):\n",
    "    encoding_dim = int(params['encoding_dim'])\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    patience = int(params['patience'])\n",
    "    l1_reg = params['l1_reg']\n",
    "    \n",
    "\n",
    "    # Define autoencoder architecture\n",
    "    input_dim = X_train_resampled_final.shape[1]\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    hidden_layer = Dense(encoding_dim, activation=activation, activity_regularizer=regularizers.l1(l1_reg))(input_layer)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "\n",
    "    # Define cross-validation parameters\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # Train and evaluate model using cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train_resampled_final):\n",
    "        # Split data into training and validation sets\n",
    "        train_data, val_data = X_train_resampled_final[train_idx], X_train_resampled_final[val_idx]\n",
    "\n",
    "        # Train model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, mode='min', restore_best_weights=True)\n",
    "        autoencoder.fit(train_data, train_data, epochs=10, batch_size=batch_size, validation_data=(val_data, val_data), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluate model\n",
    "        val_loss = autoencoder.evaluate(val_data, val_data, verbose=0)\n",
    "        losses.append(val_loss)\n",
    "\n",
    "    # Calculate mean validation loss across folds\n",
    "    mean_loss = np.mean(losses)\n",
    "\n",
    "    return {'loss': mean_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "best = fmin(fn=optimize, space=space, algo=tpe.suggest, max_evals=5)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the autoencoder model\n",
    "def create_autoencoder(encoding_dim, batch_size, learning_rate, patience, l1_reg):\n",
    "    input_layer = Input(shape=(10,))\n",
    "    hidden_layer = Dense(encoding_dim, activation='relu', activity_regularizer='l1', kernel_regularizer='l1')(input_layer)\n",
    "    output_layer = Dense(10, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=int(batch_size), validation_data=(X_test, X_test), callbacks=[EarlyStopping(monitor='val_loss', patience=int(patience), mode='min')])\n",
    "    reconstruction_loss = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    return -reconstruction_loss\n",
    "\n",
    "# Define the search space for the hyperparameters\n",
    "search_space = {\n",
    "    'encoding_dim': (10, 50),\n",
    "    'batch_size': (32, 128),\n",
    "    'learning_rate': (-5, -1),\n",
    "    'patience': (1, 10),\n",
    "    'l1_reg': (0, 0.1),\n",
    "}\n",
    "\n",
    "# Create a BayesianOptimization optimizer and optimize the function defined above\n",
    "optimizer = BayesianOptimization(\n",
    "    f=create_autoencoder,\n",
    "    pbounds=search_space,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "optimizer.maximize(n_iter=10, init_points=2)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_encoding_dim = int(optimizer.max['params']['encoding_dim'])\n",
    "best_batch_size = int(optimizer.max['params']['batch_size'])\n",
    "best_learning_rate = optimizer.max['params']['learning_rate']\n",
    "best_patience = int(optimizer.max['params']['patience'])\n",
    "best_l1_reg = optimizer.max['params']['l1_reg']\n",
    "\n",
    "# Train the autoencoder model using the best hyperparameters\n",
    "best_autoencoder = create_autoencoder(best_encoding_dim, best_batch_size, best_learning_rate, best_patience, best_l1_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683ee28d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_resampled_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3252\\1775055615.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0minput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_resampled_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_resampled_final' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "\n",
    "space = {\n",
    "    'encoding_dim': hp.quniform('encoding_dim', 10, 50, 1),\n",
    "    #'decoding_dim': hp.quniform('decoding_dim', 5, 20, 1),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "}\n",
    "\n",
    "\n",
    "def optimize(params):\n",
    "    encoding_dim = int(params['encoding_dim'])\n",
    "    #decoding_dim = int(params['decoding_dim']) # Change to 10\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    \n",
    "    # Define autoencoder architecture\n",
    "    input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "\n",
    "    # Define cross-validation parameters\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # Train and evaluate model using cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train_resampled_final):\n",
    "        # Split data into training and validation sets\n",
    "        train_data, val_data = X_train_resampled_final[train_idx], X_train_resampled_final[val_idx]\n",
    "\n",
    "        # Train model\n",
    "        autoencoder.fit(train_data, train_data, epochs=1, batch_size=batch_size, validation_data=(val_data, val_data), verbose=0)\n",
    "\n",
    "        # Evaluate model\n",
    "        val_loss = autoencoder.evaluate(val_data, val_data, verbose=0)\n",
    "        losses.append(val_loss)\n",
    "\n",
    "    # Calculate mean validation loss across folds\n",
    "    mean_loss = np.mean(losses)\n",
    "\n",
    "    return {'loss': mean_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "best = fmin(fn=optimize, space=space, algo=tpe.suggest, max_evals=2)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2778b7",
   "metadata": {},
   "source": [
    "## Fit the autoencoder on the chosen hyperparameters and check over/under fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # create the autoencoder\n",
    "# input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "# encoding_dim = 46\n",
    "# decoding_dim = 10\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "# output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# # define the optimizer with the desired learning rate\n",
    "# opt = Adam(lr=0.014369161327797432)\n",
    "\n",
    "# #MSE is a common choice for reconstruction-based autoencoders.\n",
    "# autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=10, batch_size=2)\n",
    "\n",
    "# # extract hidden layer output from autoencoder\n",
    "# hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "# hidden_layer_output = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# # create the mediator network with the hidden layer output as input\n",
    "# mediator_input_layer = Input(shape=(encoding_dim,))\n",
    "# mediator_hidden_layer = Dense(10, activation='relu')(mediator_input_layer)\n",
    "# mediator_output_layer = Dense(2, activation='sigmoid')(mediator_hidden_layer)\n",
    "\n",
    "# mediator_network = Model(inputs=mediator_input_layer, outputs=mediator_output_layer)\n",
    "# mediator_network.compile(optimizer='adam', loss='mse')\n",
    "# mediator_network.fit(hidden_layer_output, y_train_resampled_final, epochs=1)\n",
    "\n",
    "# # extract hidden layer output from mediator network\n",
    "# agent_hidden_layer_output = mediator_network.layers[1].output\n",
    "\n",
    "# agent_input_layer = Input(shape=agent_hidden_layer_output.shape[1:])\n",
    "# agent_hidden_layer = Dense(5, activation='relu')(agent_input_layer)\n",
    "# agent_output_layer = Dense(2, activation='softmax')(agent_hidden_layer)\n",
    "\n",
    "# agent_network = Model(inputs=agent_input_layer, outputs=agent_output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89749a66",
   "metadata": {},
   "source": [
    "## R1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb447f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 40\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.0003))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "#opt = Adam(lr= 0.00874103303583597)\n",
    "opt = Adam(lr= 0.0087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 40\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.025))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.002)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c84a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "# from keras import regularizers\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the autoencoder architecture\n",
    "# input_dim = X_train_resampled_final.shape[1]\n",
    "# encoding_dim = 40\n",
    "# decoding_dim = 10\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# #hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# # Add L1 regularization to the hidden layer\n",
    "# hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.2156))(input_layer)\n",
    "\n",
    "# output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# # Define the optimizer with the desired learning rate\n",
    "# #opt = Adam(lr= 0.00874103303583597)\n",
    "# opt = Adam(lr= 0.0087)\n",
    "# # Define the autoencoder model\n",
    "# autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "# #autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# # Define the number of folds for cross-validation\n",
    "# n_splits = 2\n",
    "# kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# # Define lists to store the MSE of training and validation sets for each fold\n",
    "# train_mse = []\n",
    "# val_mse = []\n",
    "# recon_errors = []\n",
    "\n",
    "\n",
    "# # Loop over each fold\n",
    "# for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "#     # Split the data into training and validation sets for the current fold\n",
    "#     X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "#     # Fit the autoencoder on the training set for the current fold\n",
    "#     history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "#     # Append the MSE of training and validation sets for the current fold to the lists\n",
    "#     train_mse.append(history.history['loss'])\n",
    "#     val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "#     # compute the reconstruction error for the test data\n",
    "#     recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     recon_errors.append(recon_error)\n",
    "\n",
    "# # Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# # mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# # std_train_mse = np.std(train_mse, axis=0)\n",
    "# # mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# # std_val_mse = np.std(val_mse, axis=0)\n",
    "# # Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# #std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "# std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# #std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "# std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# # Plot the MSE of training and validation sets against the number of epochs\n",
    "# epochs = range(1, len(mean_train_mse)+1)\n",
    "# plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "# plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "# plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "# plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Mean Squared Error (MSE)')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ef25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 35\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.025))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.002)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7638d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 32\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.003))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 8\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.003))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ab1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7827262d",
   "metadata": {},
   "source": [
    "## Plot autoencoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b03732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(autoencoder, to_file='autoencoder.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfce604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import Image\n",
    "\n",
    "dot = model_to_dot(autoencoder, show_shapes=True, show_layer_names=True)\n",
    "Image(dot.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89acd793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from sklearn.model_selection import KFold\n",
    "# import numpy as np\n",
    "# from keras import regularizers\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the autoencoder architecture\n",
    "# input_dim = X_train_resampled_final.shape[1]\n",
    "# encoding_dim = 30\n",
    "# decoding_dim = 10\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# #hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# # Add L1 regularization to the hidden layer\n",
    "# hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.15))(input_layer)\n",
    "\n",
    "# output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# # Define the optimizer with the desired learning rate\n",
    "# opt = Adam(lr= 0.00874103303583597)\n",
    "\n",
    "# # Define the autoencoder model\n",
    "# autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "# #autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# # Define the number of folds for cross-validation\n",
    "# n_splits = 2\n",
    "# kf = KFold(n_splits=n_splits, random_state=42,shuffle=True)\n",
    "\n",
    "# # Define lists to store the MSE of training and validation sets for each fold\n",
    "# train_mse = []\n",
    "# val_mse = []\n",
    "# recon_errors = []\n",
    "\n",
    "\n",
    "# # Loop over each fold\n",
    "# for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "#     # Split the data into training and validation sets for the current fold\n",
    "#     X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "#     # Fit the autoencoder on the training set for the current fold\n",
    "#     history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "#     # Append the MSE of training and validation sets for the current fold to the lists\n",
    "#     train_mse.append(history.history['loss'])\n",
    "#     val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "#     # compute the reconstruction error for the test data\n",
    "#     recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     recon_errors.append(recon_error)\n",
    "\n",
    "# # Convert the train_mse and val_mse lists to numpy arrays\n",
    "# train_mse = np.array(train_mse)\n",
    "# val_mse = np.array(val_mse)\n",
    "\n",
    "# # Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "\n",
    "\n",
    "# # Plot the MSE of training and validation sets against the number of epochs\n",
    "# epochs = range(1, len(mean_train_mse)+1)\n",
    "# plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "# plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "# plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "# plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Mean Squared Error (MSE)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f5ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 35\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l2(0.1))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.001)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6344ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 35\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.001)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 40\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.0001)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bb6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 13\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.003))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 40\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l2(0.001))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00071)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b47376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 55\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.003))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51331cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 18\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.003))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits,shuffle=True,random_state=18)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of reconstruction errors\n",
    "plt.hist(recon_errors, bins=5)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf81ce",
   "metadata": {},
   "source": [
    "## Chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 30\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.0095))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 10\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733948cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 35\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.0991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 4\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 10\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=128, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 17\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=64, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 22\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=128, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 50\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=128, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 4\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 4\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=20, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 4\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=15, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef364c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 4\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=15, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(train_mse, axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(val_mse, axis=0)\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# # plot the distribution of reconstruction errors\n",
    "# plt.hist(recon_errors, bins=5)\n",
    "# plt.xlabel('Reconstruction Error')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Reconstruction Errors')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb5b78",
   "metadata": {},
   "source": [
    "## MSE Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 2\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=5, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb740ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Train MSE:\", mean_train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c42489",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_mse_avg = np.mean(mean_train_mse)\n",
    "print(\"Mean Train MSE (Average):\", mean_train_mse_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf68d8",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88486a",
   "metadata": {},
   "source": [
    "The baseline model is simply the mean squared error between each data point in the test set and the mean of the training set, which serves as a simple reference point for comparison to the more complex autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape for the autoencoder\n",
    "input_shape = X_train_resampled_final.shape[1]\n",
    "\n",
    "# Create the input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Create the encoder layer with a linear activation function and no hidden layers\n",
    "encoded_layer = Dense(units=input_shape, activation='linear')(input_layer)\n",
    "\n",
    "# Create the decoder layer with a linear activation function and no hidden layers\n",
    "decoded_layer = Dense(units=input_shape, activation='linear')(encoded_layer)\n",
    "\n",
    "# Create the autoencoder model\n",
    "autoencoder_baseline = Model(input_layer, decoded_layer)\n",
    "\n",
    "# Compile the autoencoder model with a mean squared error loss function and an Adam optimizer\n",
    "autoencoder_baseline.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the autoencoder on the training data\n",
    "autoencoder_baseline.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Evaluate the reconstruction error on the test set\n",
    "test_predictions = autoencoder.predict(X_test)\n",
    "test_mse = np.mean(np.power(X_test - test_predictions, 2), axis=1)\n",
    "baseline_mse = np.mean(np.power(X_test - np.mean(X_train_resampled_final, axis=0), 2), axis=1)\n",
    "\n",
    "# Compare the performance of the autoencoder to the baseline\n",
    "print('Autoencoder MSE:', np.mean(test_mse))\n",
    "print('Baseline MSE:', np.mean(baseline_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "# Define input shape\n",
    "input_shape = X_train_resampled_final.shape[1]\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Define hidden layer with 3 neurons\n",
    "hidden = Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define output layer with 10 neurons\n",
    "outputs = Dense(10, activation='sigmoid')(hidden)\n",
    "\n",
    "# Define autoencoder model\n",
    "autoencoder_baseline = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "autoencoder_baseline.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "autoencoder_baseline.fit(X_train_resampled_final, X_train_resampled_final, epochs=3, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Evaluate model on test data\n",
    "mse = autoencoder_baseline.evaluate(X_test, X_test)\n",
    "print('Mean squared error on test data:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e577f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca527cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(autoencoder_baseline, to_file='autoencoder_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (10,)\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Define encoder and decoder layers\n",
    "encoded = Dense(units=input_shape[0], activation='linear')(inputs)\n",
    "decoded = Dense(units=input_shape[0], activation='linear')(encoded)\n",
    "\n",
    "# Define autoencoder model\n",
    "autoencoder_baseline = Model(inputs=inputs, outputs=decoded)\n",
    "\n",
    "# Compile model\n",
    "autoencoder_baseline.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder on the training data\n",
    "autoencoder_baseline.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=32, validation_data=(X_test, X_test))\n",
    "\n",
    "# Evaluate the reconstruction error on the test set\n",
    "test_predictions = autoencoder_baseline.predict(X_test)\n",
    "test_mse = np.mean(np.power(X_test - test_predictions, 2), axis=1)\n",
    "baseline_mse = np.mean(np.power(X_test - np.mean(X_train_resampled_final, axis=0), 2), axis=1)\n",
    "\n",
    "# Compare the performance of the autoencoder to the baseline\n",
    "print('Autoencoder MSE:', np.mean(test_mse))\n",
    "print('Baseline MSE:', np.mean(baseline_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (10,)\n",
    "\n",
    "# Define input layer\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Define encoder network\n",
    "encoded = Dense(8, activation='relu')(inputs)\n",
    "encoded = Dense(4, activation='relu')(encoded)\n",
    "\n",
    "# Define bottleneck layer\n",
    "bottleneck = Dense(2, activation='relu')(encoded)\n",
    "\n",
    "# Define decoder network\n",
    "decoded = Dense(4, activation='relu')(bottleneck)\n",
    "decoded = Dense(8, activation='relu')(decoded)\n",
    "\n",
    "# Define output layer\n",
    "outputs = Dense(10, activation='sigmoid')(decoded)\n",
    "\n",
    "# Define autoencoder model\n",
    "autoencoder_baseline = Model(inputs, outputs)\n",
    "\n",
    "# Compile autoencoder model\n",
    "autoencoder_baseline.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Print summary of autoencoder model architecture\n",
    "autoencoder_baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd8b5e",
   "metadata": {},
   "source": [
    "## Check if mediator network overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb57cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract hidden layer output from autoencoder\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "hidden_layer_output = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# create the mediator network with the hidden layer output as input\n",
    "mediator_input_layer = Input(shape=(encoding_dim,))\n",
    "mediator_hidden_layer = Dense(10, activation='relu')(mediator_input_layer)\n",
    "mediator_output_layer = Dense(2, activation='sigmoid')(mediator_hidden_layer)\n",
    "\n",
    "mediator_network = Model(inputs=mediator_input_layer, outputs=mediator_output_layer)\n",
    "mediator_network.compile(optimizer='adam', loss='mse')\n",
    "mediator_network.fit(hidden_layer_output, y_train_resampled_final, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f52eac",
   "metadata": {},
   "source": [
    "## Reconstruction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6c641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error between the original and reconstructed data\n",
    "mse = np.mean(np.power(X_test- predictions, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba50527",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histogram of the MSE values\n",
    "plt.hist(mse, bins=50)\n",
    "plt.xlabel(\"Reconstruction Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Reconstruction Errors on Test Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reward function\n",
    "def reward_fn(action, label):\n",
    "    DF = [0,1,2]  # indices of fraud class\n",
    "    DN = [81,    787,   2392,   3121,   3449]  # indices of non-fraud class\n",
    "    terminal = 0  # initialize terminal flag to 0\n",
    "    if label in DF:\n",
    "        if action == label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    else:\n",
    "        if action == label:\n",
    "            reward = 0.5  # set Î» to 0.5\n",
    "        else:\n",
    "            reward = -0.5  # set Î» to -0.5\n",
    "    return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82377946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define the reward function\n",
    "# def reward_fn(action, label):\n",
    "#     # Replace with your own reward function\n",
    "#     if action == label:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 2  # Number of episodes\n",
    "# T = 2  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# # Initialize agent network with same architecture as mediator network\n",
    "# agent_network = keras.models.Sequential([\n",
    "#     keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "#     keras.layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "# agent_network.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "# # Initialize mediator network\n",
    "# mediator_network = None  # Replace with your own mediator network\n",
    "\n",
    "# # Generate dataset D\n",
    "# hidden_layer_output = autoencoder.predict(X_train_resampled_final)\n",
    "# D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ee2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define the reward function\n",
    "# def reward_fn(action, label):\n",
    "#     # Replace with your own reward function\n",
    "#     if action == label:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 8  # Number of episodes\n",
    "# T = 8  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# # # Initialize agent network with same architecture as mediator network\n",
    "# # agent_network = keras.models.Sequential([\n",
    "# #     keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "# #     keras.layers.Dense(2, activation='softmax')\n",
    "# # ])\n",
    "# # agent_network.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "# # #Initialize mediator network\n",
    "# # mediator_network = None  # Replace with your own mediator network\n",
    "\n",
    "# # Generate dataset D\n",
    "# hidden_layer_output = autoencoder.predict(X_train_resampled_final)\n",
    "# D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d5d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import models, layers\n",
    "import random\n",
    "\n",
    "# Initialize replay memory with M capacity\n",
    "M = 10000\n",
    "replay_memory = []\n",
    "\n",
    "# Initialize simulation environment\n",
    "env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define the reward function\n",
    "# def reward_fn(action, label):\n",
    "#     # Replace with your own reward function\n",
    "#     if action == label:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "\n",
    "# Define hyperparameters\n",
    "K = 2  # Number of episodes\n",
    "T = 2  # Number of timesteps per episode\n",
    "gamma = 0.9  # Discount factor\n",
    "batch_size = 32\n",
    "learning_rate_val = 0.001\n",
    "\n",
    "mediator_network = keras.models.Sequential([\n",
    "    keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "mediator_network.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate_val))\n",
    "\n",
    "\n",
    "# Generate dataset D\n",
    "# X_train_resampled_final_20 = np.hstack((X_train_resampled_final, np.zeros((X_train_resampled_final.shape[0], 10))))\n",
    "# hidden_layer_output = [mediator_network.predict(np.array([x]*2).reshape(2, -1))[0] for x in X_train_resampled_final]\n",
    "# D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "hidden_layer_output = mediator_network.predict(X_train_resampled_final)\n",
    "D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# Train agent\n",
    "for k in range(K):\n",
    "    # Shuffle dataset D\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Choose action\n",
    "        action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward = reward_fn(action, D[t][1])\n",
    "        terminal = 1 if t == T - 1 else 0\n",
    "        print(\"Reward:\", reward) # Add this line to print the reward\n",
    "        print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "        # Update state\n",
    "        state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        replay_memory.append((state, action, reward, state_next, terminal))\n",
    "        if len(replay_memory) > M:\n",
    "            replay_memory.pop(0)\n",
    "        \n",
    "        # Sample minibatch from replay memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            minibatch = random.sample(replay_memory, batch_size)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        X = np.zeros((batch_size, 10))\n",
    "        y = np.zeros((batch_size, 10))\n",
    "        for i in range(batch_size):\n",
    "            state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "            X[i] = state_i\n",
    "            y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "            if terminal_i:\n",
    "                y[i][action_i] = reward_i\n",
    "            else:\n",
    "                y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "        # Train agent network on minibatch - CONCEPT OF GRADIENT DESCENT\n",
    "        agent_network.train_on_batch(X, y)\n",
    "        \n",
    "        # Update state\n",
    "        state = state_next\n",
    "        \n",
    "        # Check if episode is over\n",
    "        if terminal:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6610007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import models, layers\n",
    "import random\n",
    "\n",
    "# Initialize replay memory with M capacity\n",
    "M = 10000\n",
    "replay_memory = []\n",
    "\n",
    "# Initialize simulation environment\n",
    "env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define the reward function\n",
    "# def reward_fn(action, label):\n",
    "#     # Replace with your own reward function\n",
    "#     if action == label:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "# Define policy\n",
    "def policy(state, model):\n",
    "    q_values = model.predict(state)\n",
    "    return np.argmax(q_values[0])\n",
    "\n",
    "# Define hyperparameters\n",
    "K = 2  # Number of episodes\n",
    "T = 2  # Number of timesteps per episode\n",
    "gamma = 0.9  # Discount factor\n",
    "batch_size = 32\n",
    "learning_rate_val = 0.001\n",
    "\n",
    "mediator_network = keras.models.Sequential([\n",
    "    keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "mediator_network.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate_val))\n",
    "\n",
    "\n",
    "# Generate dataset D\n",
    "# X_train_resampled_final_20 = np.hstack((X_train_resampled_final, np.zeros((X_train_resampled_final.shape[0], 10))))\n",
    "# hidden_layer_output = [mediator_network.predict(np.array([x]*2).reshape(2, -1))[0] for x in X_train_resampled_final]\n",
    "# D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "hidden_layer_output = mediator_network.predict(X_train_resampled_final)\n",
    "D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# Train agent\n",
    "for k in range(K):\n",
    "    # Shuffle dataset D\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Choose action based on policy\n",
    "        action = policy(state.reshape(1, -1), agent_network)\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward = reward_fn(action, D[t][1])\n",
    "        terminal = 1 if t == T - 1 else 0\n",
    "        print(\"Reward:\", reward) # Add this line to print the reward\n",
    "        print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "        # Update state\n",
    "        state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        replay_memory.append((state, action, reward, state_next, terminal))\n",
    "        if len(replay_memory) > M:\n",
    "            replay_memory.pop(0)\n",
    "        \n",
    "        # Sample minibatch from replay memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            minibatch = random.sample(replay_memory, batch_size)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        X = np.zeros((batch_size, 10))\n",
    "        y = np.zeros((batch_size, 10))\n",
    "        for i in range(batch_size):\n",
    "            state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "            X[i] = state_i\n",
    "            y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "            if terminal_i:\n",
    "                y[i][action_i] = reward_i\n",
    "            else:\n",
    "                y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "                    mediator_network.train_on_batch(X, y)\n",
    "    \n",
    "        # Update state\n",
    "        state = state_next\n",
    "\n",
    "        # Check if episode is over\n",
    "        if terminal:\n",
    "            break\n",
    "\n",
    "# Evaluate policy Ï€Î¸\n",
    "rewards = []\n",
    "for i in range(len(D)):\n",
    "    state = D[i][0]\n",
    "    label = D[i][1]\n",
    "    action_probs = mediator_network.predict(np.array([state]*10).reshape(10, -1))\n",
    "    action = np.argmax(action_probs)\n",
    "    reward = reward_fn(action, label)\n",
    "    rewards.append(reward)\n",
    "print(\"Average reward:\", np.mean(rewards)) # Add this line to print the average reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d637de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mediator_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 40\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.025))(input_layer)\n",
    "\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.002)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=15, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR THE CODE JUST ABOVE\n",
    "\n",
    "test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "test_mse.append(test_error)\n",
    "print(f\"Test MSE: {test_error:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(63453654)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 30\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=32, verbose=0, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "#std_train_mse = np.std(np.array(train_mse), axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "#std_val_mse = np.std(np.array(val_mse), axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
