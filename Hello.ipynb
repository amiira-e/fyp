{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623bc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e04005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b331399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c6cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b2d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666703a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c572aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3595ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032e705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0        403   12378.330000   77000.730000             0.0            0.00   \n",
      "1        138   44423.330000   50090.000000             0.0        82120.22   \n",
      "2        325  143673.480000    4564.000000             0.0        82120.22   \n",
      "3        308  300712.340000   51474.000000             0.0        82120.22   \n",
      "4        349   47243.760000   11262.000000             0.0            0.00   \n",
      "...      ...            ...            ...             ...             ...   \n",
      "406078   278  111168.880136  111168.880136             0.0        82120.22   \n",
      "406079   274  143673.480000   50090.000000             0.0        82120.22   \n",
      "406080    60  143673.480000   50090.000000             0.0            0.00   \n",
      "406081   449   44882.356239   44882.356239             0.0            0.00   \n",
      "406082   220   39953.091459   29059.334627             0.0        82120.22   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0             0.000000               0     3    156679    306900  \n",
      "1        255495.460000               0     0     92881    180374  \n",
      "2        255495.460000               0     1     80756    482539  \n",
      "3        654217.020000               0     1    175711    597630  \n",
      "4             0.000000               0     3    156679     26253  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "406078   255495.460000               0     1     90379    472585  \n",
      "406079   255495.460000               0     1    112071    494845  \n",
      "406080        0.000000               0     1    154830    240268  \n",
      "406081    36237.626509               0     1    122579     88980  \n",
      "406082   255495.460000               0     1     93537    130866  \n",
      "\n",
      "[406083 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "485376  278.0   22928.58            0.0             0.0           0.000   \n",
      "642214   45.0    8606.90         5764.0             0.0           0.000   \n",
      "192982  237.0  220046.83            0.0             0.0      130797.505   \n",
      "99091   328.0   83938.53        13653.5             0.0      130797.505   \n",
      "203398  307.0   74636.86            0.0             0.0      130797.505   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "230877  154.0  195805.05        31725.0             0.0           0.000   \n",
      "315026  301.0   36352.03        13653.5             0.0           0.000   \n",
      "661254  238.5  163969.90        13653.5             0.0      130797.505   \n",
      "688112  280.0    3092.79            0.0             0.0           0.000   \n",
      "642560   35.0   74636.86        30807.0             0.0      130797.505   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "485376           0.000               0     3    291184    424837  \n",
      "642214           0.000               0     3    363649    442961  \n",
      "192982      214326.245               0     1      1853    410946  \n",
      "99091       537297.070               0     0    252825    347652  \n",
      "203398      214326.245               0     1    201182    417173  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "230877      195805.050               0     1    181881    192704  \n",
      "315026           0.000               0     3    458861    630843  \n",
      "661254      706564.020               0     0     37270    676511  \n",
      "688112           0.000               0     3    455345    152073  \n",
      "642560      214326.245               0     1    214954    689599  \n",
      "\n",
      "[70000 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbaee3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19032385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660fad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028208958828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028208958828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "6346/6346 [==============================] - ETA: 0s - loss: 0.4569WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002820BEC8C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002820BEC8C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "6346/6346 [==============================] - 29s 5ms/step - loss: 0.4569 - val_loss: 0.4949\n",
      "Epoch 2/10\n",
      "6346/6346 [==============================] - 27s 4ms/step - loss: 0.4071 - val_loss: 0.4859\n",
      "Epoch 3/10\n",
      "6346/6346 [==============================] - 19s 3ms/step - loss: 0.4029 - val_loss: 0.4838\n",
      "Epoch 4/10\n",
      "6346/6346 [==============================] - 22s 3ms/step - loss: 0.4017 - val_loss: 0.4831\n",
      "Epoch 5/10\n",
      "6346/6346 [==============================] - 46s 7ms/step - loss: 0.4012 - val_loss: 0.4827\n",
      "Epoch 6/10\n",
      "6346/6346 [==============================] - 45s 7ms/step - loss: 0.4010 - val_loss: 0.4825\n",
      "Epoch 7/10\n",
      "6346/6346 [==============================] - 43s 7ms/step - loss: 0.4008 - val_loss: 0.4824\n",
      "Epoch 8/10\n",
      "6346/6346 [==============================] - 39s 6ms/step - loss: 0.4007 - val_loss: 0.4823\n",
      "Epoch 9/10\n",
      "6346/6346 [==============================] - 47s 7ms/step - loss: 0.4007 - val_loss: 0.4822\n",
      "Epoch 10/10\n",
      "6346/6346 [==============================] - 37s 6ms/step - loss: 0.4006 - val_loss: 0.4822\n",
      "Test MSE: 0.43013\n",
      "Epoch 1/10\n",
      "6346/6346 [==============================] - 46s 7ms/step - loss: 0.4821 - val_loss: 0.4007\n",
      "Epoch 2/10\n",
      "6346/6346 [==============================] - 45s 7ms/step - loss: 0.4821 - val_loss: 0.4006\n",
      "Epoch 3/10\n",
      "6346/6346 [==============================] - 43s 7ms/step - loss: 0.4821 - val_loss: 0.4006\n",
      "Epoch 4/10\n",
      "6346/6346 [==============================] - 34s 5ms/step - loss: 0.4820 - val_loss: 0.4005\n",
      "Epoch 5/10\n",
      "6346/6346 [==============================] - 20s 3ms/step - loss: 0.4820 - val_loss: 0.4005\n",
      "Epoch 6/10\n",
      "6346/6346 [==============================] - 20s 3ms/step - loss: 0.4820 - val_loss: 0.4005\n",
      "Epoch 7/10\n",
      "6346/6346 [==============================] - 21s 3ms/step - loss: 0.4820 - val_loss: 0.4006\n",
      "Epoch 8/10\n",
      "6346/6346 [==============================] - 20s 3ms/step - loss: 0.4820 - val_loss: 0.4005\n",
      "Epoch 9/10\n",
      "6346/6346 [==============================] - 21s 3ms/step - loss: 0.4819 - val_loss: 0.4005\n",
      "Epoch 10/10\n",
      "6346/6346 [==============================] - 20s 3ms/step - loss: 0.4819 - val_loss: 0.4004\n",
      "Test MSE: 0.43000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 15\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='tanh', kernel_regularizer=regularizers.l1(0.000391))(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "opt = Adam(lr= 0.00087)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder model with the specified optimizer and loss function\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    # Define early stopping to prevent overfitting and improve efficiency\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10,batch_size=32, verbose=1, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26098e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.models import Model\n",
    "# from keras import regularizers\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.layers import Input, Dense\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Define the autoencoder architecture\n",
    "# input_dim = X_train_resampled_final.shape[1]\n",
    "# encoding_dim = 32\n",
    "# decoding_dim = 10\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# hidden_layer = Dense(encoding_dim, activation='tanh', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "# output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# # Define the optimizer with the desired learning rate\n",
    "# opt = Adam(lr= 0.00087)\n",
    "\n",
    "# # Define the autoencoder model\n",
    "# autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# # Compile the autoencoder model with the specified optimizer and loss function\n",
    "# autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # Define the number of folds for cross-validation\n",
    "# n_splits = 5\n",
    "# kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# # Define lists to store the MSE of training and validation sets for each fold\n",
    "# train_mse = []\n",
    "# val_mse = []\n",
    "# test_mse = []\n",
    "# recon_errors = []\n",
    "\n",
    "# # Loop over each fold\n",
    "# for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "#     # Split the data into training and validation sets for the current fold\n",
    "#     X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "#     # Define early stopping to prevent overfitting and improve efficiency\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#     # Fit the autoencoder on the training set for the current fold\n",
    "#     history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10,batch_size=32, verbose=1, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "#     # Append the MSE of training and validation sets for the current fold to the lists\n",
    "#     train_mse.append(history.history['loss'])\n",
    "#     val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "#     # compute the reconstruction error for the test data\n",
    "#     recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     recon_errors.append(recon_error)\n",
    "    \n",
    "#     # Calculate the MSE for the test set\n",
    "#     test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     test_mse.append(test_error)\n",
    "#     print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# # Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# # Plot the MSE of training and validation sets against the number of epochs\n",
    "# epochs = range(1, len(mean_train_mse)+1)\n",
    "# plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "# plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "# plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "# plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Mean Squared Error (MSE)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b871a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002820BC3D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000002820BC3D288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2188/2188 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate reconstructed outputs for the test set\n",
    "reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the MSE between the input and the reconstructed output for each data point\n",
    "recon_errors = np.mean(np.power(X_test - reconstructed, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98f92d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_fold = np.mean(recon_errors)\n",
    "test_mse.append(test_mse_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "687edc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reconstruction Error: 0.42983\n"
     ]
    }
   ],
   "source": [
    "# Print the final mean and standard deviation of reconstruction error across all folds\n",
    "print(f\"Mean Reconstruction Error: {np.mean(test_mse):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b62211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188/2188 [==============================] - 3s 1ms/step - loss: 0.4300\n",
      "Mean squared error on test data: 0.42999666929244995\n"
     ]
    }
   ],
   "source": [
    "mse = autoencoder.evaluate(X_test, X_test)\n",
    "print('Mean squared error on test data:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18bdc029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train MSE (Average): 0.4446933075785637\n"
     ]
    }
   ],
   "source": [
    "mean_train_mse_avg = np.mean(mean_train_mse)\n",
    "print(\"Mean Train MSE (Average):\", mean_train_mse_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea539278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                165       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 325\n",
      "Trainable params: 325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4301a91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(autoencoder, to_file='autoencoder_last.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa66b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028209C045E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028209C045E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12691/12691 [==============================] - 15s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input = Input(shape=(hidden_layer_output_train.shape[1],))\n",
    "x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.000111))(new_model_input)\n",
    "# x = Dense(32, activation='relu')(x)\n",
    "output = Dense(2, activation='sigmoid')(x)\n",
    "#output = Dense(1, activation='softmax')(x)\n",
    "mediator_network = Model(inputs=new_model_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68b82758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(mediator_network , to_file='mediator_network .png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76dd86ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(hidden_layer_model, to_file='autoencoder_last.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef47ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 15)                165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 165\n",
      "Trainable params: 165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "141d5305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 15)]              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 182\n",
      "Trainable params: 182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mediator_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a28c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert target labels to one-hot encoded format\n",
    "y_train_resampled_final_onehot = to_categorical(y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40ae75db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt_new = Adam(lr= 0.000992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a908a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028246D055E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000028246D055E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "11418/11422 [============================>.] - ETA: 0s - loss: 0.3813WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028209C25558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028209C25558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.3812 - val_loss: 0.7671\n",
      "Epoch 2/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.2674 - val_loss: 0.4712\n",
      "Epoch 3/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.2068 - val_loss: 0.4509\n",
      "Epoch 4/15\n",
      "11422/11422 [==============================] - 19s 2ms/step - loss: 0.1876 - val_loss: 0.3810\n",
      "Epoch 5/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.1802 - val_loss: 0.2973\n",
      "Epoch 6/15\n",
      "11422/11422 [==============================] - 27s 2ms/step - loss: 0.1760 - val_loss: 0.3606\n",
      "Epoch 7/15\n",
      "11422/11422 [==============================] - 24s 2ms/step - loss: 0.1729 - val_loss: 0.3432\n",
      "Epoch 8/15\n",
      "11422/11422 [==============================] - 23s 2ms/step - loss: 0.1705 - val_loss: 0.3617\n",
      "Epoch 9/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.1683 - val_loss: 0.4069\n",
      "Epoch 10/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.1661 - val_loss: 0.3043\n",
      "Epoch 11/15\n",
      "11422/11422 [==============================] - 19s 2ms/step - loss: 0.1642 - val_loss: 0.3139\n",
      "Epoch 12/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.1626 - val_loss: 0.3443\n",
      "Epoch 13/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.1613 - val_loss: 0.2996\n",
      "Epoch 14/15\n",
      "11422/11422 [==============================] - 19s 2ms/step - loss: 0.1602 - val_loss: 0.3651\n",
      "Epoch 15/15\n",
      "11422/11422 [==============================] - 20s 2ms/step - loss: 0.1591 - val_loss: 0.3727\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#Compile the new model\n",
    "mediator_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "history = mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot,\n",
    "                               epochs=15, batch_size=32, validation_split=0.1,\n",
    "                               callbacks=[early_stopping])\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "#history=mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot, epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d72c5466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8355eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss Mean: 0.19227861563364665\n",
      "Validation Loss Mean: 0.38932014306386314\n"
     ]
    }
   ],
   "source": [
    "train_loss_mean = sum(history.history['loss']) / len(history.history['loss'])\n",
    "val_loss_mean = sum(history.history['val_loss']) / len(history.history['val_loss'])\n",
    "\n",
    "print(\"Train Loss Mean:\", train_loss_mean)\n",
    "print(\"Validation Loss Mean:\", val_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2e9f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "406078    1\n",
      "406079    1\n",
      "406080    1\n",
      "406081    1\n",
      "406082    1\n",
      "Name: isFraud, Length: 406083, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0fd12",
   "metadata": {},
   "source": [
    "## Create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fa72b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028209C25CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000028209C25CA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12691/12691 [==============================] - 15s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = Model(inputs=mediator_network .input, outputs=mediator_network .layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.0000611))(new_model_input_med)\n",
    "# x = Dense(32, activation='relu')(x)\n",
    "output_med = Dense(2, activation='sigmoid')(x)\n",
    "#output = Dense(1, activation='softmax')(x)\n",
    "agent_network = Model(inputs=new_model_input_med, outputs=output_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "555b4460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 15)]              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160\n",
      "Trainable params: 160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_model_med.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66634a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the input layer for the agent network\n",
    "# agent_input = Input(shape=(hidden_layer_output_train.shape[1],))\n",
    "\n",
    "# # Define the new Dense layer for the agent network\n",
    "# x = Dense(4, activation='relu')(agent_input)\n",
    "\n",
    "# output_agent= Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# # Define the agent network model\n",
    "# agent_network = Model(inputs=agent_input, outputs=output_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ef889bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_new = Adam(lr= 0.05312)\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "#opt_new = SGD(lr=0.000422, momentum=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fd3b79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000282090D65E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000282090D65E8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028209BFF9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000028209BFF9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "#Compile the new model\n",
    "#agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "# Compile the new model\n",
    "agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n",
    "                               epochs=15, batch_size=32, validation_split=0.2,\n",
    "                               callbacks=[early_stopping],verbose=0)\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "#history=mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot, epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49059cf",
   "metadata": {},
   "source": [
    "## New agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the hidden layer model\n",
    "# hidden_layer_model_med = Model(inputs=mediator_network .input, outputs=mediator_network .layers[1].output)\n",
    "\n",
    "# # Get the activations of the hidden layer for the training data\n",
    "# hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.00611))(new_model_input_med)\n",
    "# x1 = Dense(5, activation='tanh')(x)\n",
    "# x2 = Dense(3, activation='tanh')(x1)\n",
    "# output_med = Dense(2, activation='sigmoid')(x2)\n",
    "# agent_network = Model(inputs=new_model_input_med, outputs=output_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the early stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# # Train the new model on the activations of the hidden layer\n",
    "# history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n",
    "#                                epochs=15, batch_size=32, validation_split=0.25,\n",
    "#                                callbacks=[early_stopping],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09176565",
   "metadata": {},
   "source": [
    "## 20/04/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbfc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 10\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.7\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 2000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e13cc42",
   "metadata": {},
   "source": [
    "# Latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de06028",
   "metadata": {},
   "source": [
    "## 21/04/2023-:Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 7\n",
    "learning_rate=0.7\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    #state = D[0][0]\n",
    "    state= hidden_layer_output_train_med[0, 0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        #next_state = D[step+1][0]\n",
    "        idx = 0  # initialize index counter to 0\n",
    "\n",
    "        # inside a loop or a function\n",
    "        next_state = hidden_layer_output_train_med[idx, 0]\n",
    "        idx += 1  # increment index counter by 1 for the next iteration\n",
    "\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8c024",
   "metadata": {},
   "source": [
    "## N1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# # Define hyperparameters\n",
    "# gamma = 0.85\n",
    "# epsilon = 0.1\n",
    "# batch_size = 128\n",
    "# num_episodes = 10\n",
    "# max_steps = 7\n",
    "# learning_rate=0.7\n",
    "\n",
    "# # Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "# tp = 0\n",
    "# tn = 0\n",
    "# fp = 0\n",
    "# fn = 0\n",
    "\n",
    "# D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# # Initialize experience replay memory\n",
    "# M = 20000\n",
    "# replay_memory = []\n",
    "\n",
    "# def epsilon_greedy_policy(state, epsilon, theta):\n",
    "#     num_actions = agent_network.output_shape[-1]  # Get the number of actions from the output shape of the agent network\n",
    "#     if np.random.uniform() < epsilon:\n",
    "#         # Choose a random action\n",
    "#         action = np.random.randint(num_actions)\n",
    "#     else:\n",
    "#         # Choose the action with the highest Q-value\n",
    "#         # Q_values = agent_network.predict(np.array([state]))[0]\n",
    "#         # Q_values = agent_network.predict(state)[0]\n",
    "#         agent_network.predict([state])\n",
    "#         action = np.argmax(Q_values)\n",
    "#     return action\n",
    "\n",
    "\n",
    "# # Define function for computing loss\n",
    "# def compute_loss(y, Q_values):\n",
    "#     return np.sum(np.square(y - Q_values))\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "# # Define function for computing Q-values\n",
    "# def Q(states, theta):\n",
    "#     # Compute Q-values for all states and actions in the batch\n",
    "#     Q_values = np.dot(states, theta)\n",
    "#     return Q_values\n",
    "\n",
    "# # Initialize simulation environments\n",
    "# environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# # Initialize Q-network parameters\n",
    "# num_features = D[0][0].shape[0]\n",
    "# num_actions = 2\n",
    "# theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# # Start training\n",
    "# for episode in range(num_episodes):\n",
    "#     # Shuffle training data\n",
    "#     random.shuffle(D)\n",
    "#     print(\"Episode \", episode)\n",
    "#     print(\"--------------------------------------------\")\n",
    "    \n",
    "#     # Initialize state\n",
    "#     #state = D[0][0]\n",
    "#     state= hidden_layer_output_train_med[0, 0]\n",
    "    \n",
    "#     # Start episode\n",
    "#     for step in range(max_steps):\n",
    "#         action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "#         true_label = D[step][1]\n",
    "#         predicted_label = action\n",
    "#         #next_state = D[step+1][0]\n",
    "#         idx = 0  # initialize index counter to 0\n",
    "\n",
    "#         # inside a loop or a function\n",
    "#         next_state = hidden_layer_output_train_med[idx, 0]\n",
    "#         idx += 1  # increment index counter by 1 for the next iteration\n",
    "\n",
    "#         reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "#         print(\"Step:\", step)\n",
    "#         print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"\")\n",
    "        \n",
    "#         # Update counters for precision and accuracy\n",
    "#         if true_label == 1:\n",
    "#             if predicted_label == 1:\n",
    "#                 tp += 1\n",
    "#             else:\n",
    "#                 fn += 1\n",
    "#         else:\n",
    "#             if predicted_label == 1:\n",
    "#                 fp += 1\n",
    "#             else:\n",
    "#                 tn += 1\n",
    "        \n",
    "\n",
    "#         # Store experience in memory\n",
    "#         replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "#         # Sample a batch of experiences from memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             # Train the agent network\n",
    "#             states, actions, rewards, next_states, terminals = zip(*random.sample(replay_memory, batch_size))\n",
    "\n",
    "#             # Convert actions tuple into numpy array\n",
    "#             actions = np.array(actions)\n",
    "\n",
    "#             # Compute target Q-values\n",
    "#             target_Q_values = []\n",
    "#             for i in range(batch_size):\n",
    "#                 if terminals[i]:\n",
    "#                     target_Q_values.append(rewards[i])\n",
    "#                 else:\n",
    "#                     next_Q_values = Q(next_states[i], theta)\n",
    "#                     target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "#             # Compute predicted Q-values and loss\n",
    "#             predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "#             loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "#             # Update parameters using gradient descent\n",
    "#             # Compute gradients\n",
    "#             grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "#             # Reshape gradients to match the shape of theta\n",
    "#             grad = grad.reshape(theta.shape)\n",
    "\n",
    "#             # Update parameters using gradient descent\n",
    "#             theta -= grad * learning_rate\n",
    "            \n",
    "#             agent_network.train(batch_size=batch_size)\n",
    "            \n",
    "#         # Update state\n",
    "#         state = next_state\n",
    "        \n",
    "#         # Check if episode is finished\n",
    "#         if terminal==1:\n",
    "#             break\n",
    "            \n",
    "# # Calculate precision and accuracy\n",
    "# precision = tp / (tp + fp)\n",
    "# accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 10\n",
    "max_steps = 7\n",
    "learning_rate = 0.7\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    num_actions = agent_network.output_shape[-1]  # Get the number of actions from the output shape of the agent network\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(np.array([state]))[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "\n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "\n",
    "          # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            # Train the agent network\n",
    "            states, actions, rewards, next_states, terminals = zip(*random.sample(replay_memory, batch_size))\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "            agent_network.train(batch_size=batch_size)\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a8bb44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  10\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  11\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  12\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  13\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  14\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  15\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  16\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  17\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  18\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  19\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  20\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  21\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  22\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  23\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  24\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  25\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  26\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  27\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  28\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  29\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  30\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  31\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  32\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  33\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  34\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  35\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  36\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  37\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  38\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  39\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  40\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.0\n",
      "Accuracy: 0.6293103448275862\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 41\n",
    "max_steps = 7\n",
    "learning_rate=0.5\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    #state = D[0][0]\n",
    "    state= hidden_layer_output_train_med[0, 0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        #next_state = D[step+1][0]\n",
    "        idx = 0  # initialize index counter to 0\n",
    "\n",
    "        # inside a loop or a function\n",
    "        next_state = hidden_layer_output_train_med[idx, 0]\n",
    "        idx += 1  # increment index counter by 1 for the next iteration\n",
    "\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8db04ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.float32'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9036\\2812277520.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;31m# Start episode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_greedy_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mtrue_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mpredicted_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9036\\2812277520.py\u001b[0m in \u001b[0;36mepsilon_greedy_policy\u001b[1;34m(state, epsilon, theta)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Choose the action with the highest Q-value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#Q_values = Q(state, theta)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mQ_values\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0magent_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1081\u001b[0m         raise ValueError(\n\u001b[0;32m   1082\u001b[0m             \"Failed to find data adapter that can handle input: {}, {}\".format(\n\u001b[1;32m-> 1083\u001b[1;33m                 \u001b[0m_type_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_type_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m             )\n\u001b[0;32m   1085\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.float32'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 41\n",
    "max_steps = 7\n",
    "learning_rate=0.5\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "state=np.array(state)\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "import numpy as np\n",
    "\n",
    "D = np.array(D)\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        #Q_values = Q(state, theta)\n",
    "        Q_values= agent_network.predict(state)[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    #state = D[0][0]\n",
    "    state= hidden_layer_output_train_med[0, 0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        #next_state = D[step+1][0]\n",
    "        idx = 0  # initialize index counter to 0\n",
    "\n",
    "        # inside a loop or a function\n",
    "        next_state = hidden_layer_output_train_med[idx, 0]\n",
    "        idx += 1  # increment index counter by 1 for the next iteration\n",
    "\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_mean = sum(history.history['loss']) / len(history.history['loss'])\n",
    "# val_loss_mean = sum(history.history['val_loss']) / len(history.history['val_loss'])\n",
    "\n",
    "# print(\"Train Loss Mean:\", train_loss_mean)\n",
    "# print(\"Validation Loss Mean:\", val_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the reward function\n",
    "# def reward_fn(action, label):\n",
    "#     DF = [0,1,2]  # indices of fraud class\n",
    "#     DN = [81,    787,   2392,   3121,   3449]  # indices of non-fraud class\n",
    "#     terminal = 0  # initialize terminal flag to 0\n",
    "#     if label in DF:\n",
    "#         if action == label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     else:\n",
    "#         if action == label:\n",
    "#             reward = 0.5  # set  to 0.5\n",
    "#         else:\n",
    "#             reward = -0.5  # set  to -0.5\n",
    "#     return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc280510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "# import random\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # # Define the reward function\n",
    "# # def reward_fn(action, label):\n",
    "# #     # Replace with your own reward function\n",
    "# #     if action == label:\n",
    "# #         return 1\n",
    "# #     else:\n",
    "# #         return -1\n",
    "\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 2  # Number of episodes\n",
    "# T = 2  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # mediator_network = keras.models.Sequential([\n",
    "# #     keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "# #     keras.layers.Dense(10, activation='softmax')\n",
    "# # ])\n",
    "# # mediator_network.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate_val))\n",
    "\n",
    "# # Generate dataset D\n",
    "# # X_train_resampled_final_20 = np.hstack((X_train_resampled_final, np.zeros((X_train_resampled_final.shape[0], 10))))\n",
    "# # hidden_layer_output = [mediator_network.predict(np.array([x]*2).reshape(2, -1))[0] for x in X_train_resampled_final]\n",
    "# # D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# hidden_layer_output = mediator_network.predict(X_train_resampled_final)\n",
    "# D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch - CONCEPT OF GRADIENT DESCENT\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf50d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "# import random\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 2  # Number of episodes\n",
    "# T = 2  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # # Define agent network\n",
    "# # agent_network = keras.models.Sequential([\n",
    "# #     keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "# #     keras.layers.Dense(10, activation='softmax')\n",
    "# # ])\n",
    "# # agent_network.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(learning_rate=learning_rate_val))\n",
    "\n",
    "# # Define reward function\n",
    "# def reward_fn(action, label):\n",
    "#     # Replace with your own reward function\n",
    "#     if action == label:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     #np.random.shuffle(D)\n",
    "#     # Shuffle the training data\n",
    "#     D=np.random.shuffle(X_train_resampled_final)\n",
    "\n",
    "    \n",
    "#     # Initialize state\n",
    "#     #state = D[0][0]\n",
    "#     # Initialize state\n",
    "\n",
    "#     #state = X_train_resampled_final[0]\n",
    "#     state = X_train_resampled_final[0][:7]\n",
    "\n",
    "#     #state = X_train_resampled_final[0].reshape(1, -1)\n",
    "\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         #action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e65c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# K=25\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0][:7]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 7))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# KC=10\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(KC):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0][:7]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 7))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdea785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# KC7=2\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(KC7):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0][:7]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 7))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# K1=5\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K1):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, state.shape[0]))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcf98c",
   "metadata": {},
   "source": [
    "## New trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the reward function\n",
    "# def reward_fn(action, label):\n",
    "#     DF = [0,1,2]  # indices of fraud class\n",
    "#     DN = [81,    787,   2392,   3121,   3449]  # indices of non-fraud class\n",
    "#     terminal = 0  # initialize terminal flag to 0\n",
    "#     if label in DF:\n",
    "#         if action == label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     else:\n",
    "#         if action == label:\n",
    "#             reward = 0.5  # set  to 0.5\n",
    "#         else:\n",
    "#             reward = -0.5  # set  to -0.5\n",
    "#     return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551457ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reward function\n",
    "def reward_fn(action, label):\n",
    "    terminal = 0  # initialize terminal flag to 0\n",
    "    if label in [0, 1, 2]:\n",
    "        if action == label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    else:\n",
    "        if action == label:\n",
    "            reward = 0.5  # set  to 0.5\n",
    "        else:\n",
    "            reward = -0.5  # set  to -0.5\n",
    "    return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c3472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "# import random\n",
    "\n",
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K2 = 5  # Number of episodes\n",
    "# T = 3  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K2):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "# #         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "# #         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "#         # Calculate reward and terminal flag\n",
    "#         true_label = D[t][1]\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"True label in dataset is:\", true_label, \", agent has predicted\", action)\n",
    "#         print(\"Terminal:\", terminal)\n",
    "\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch - CONCEPT OF GRADIENT DESCENT\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "# import random\n",
    "\n",
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize an empty list to store the cumulative reward obtained by the agent over time\n",
    "# cumulative_rewards = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K2 = 5  # Number of episodes\n",
    "# T = 5  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K2):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         # print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         # print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "#         # Calculate reward and terminal flag\n",
    "#         true_label = D[t][1]\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"True label in dataset is:\", true_label, \", agent has predicted\", action)\n",
    "#         print(\"Terminal:\", terminal)\n",
    "        \n",
    "#         # Compute the cumulative reward obtained by the agent for this episode\n",
    "#         episode_reward = 0\n",
    "#         if len(replay_memory) >= T:\n",
    "#             for t_ in range(T):\n",
    "#                 episode_reward += replay_memory[-T+t_][2][0]\n",
    "#             cumulative_rewards.append(episode_reward)\n",
    "\n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch - CONCEPT OF GRADIENT DESCENT\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break\n",
    "            \n",
    "# # Plot the cumulative reward obtained by the agent over time\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(cumulative_rewards)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Cumulative Reward')\n",
    "# plt.title('Cumulative Reward over Time')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e09a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    #Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    #Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    \n",
    "    return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 10  # Number of episodes\n",
    "# T = 4  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # Randomly initialize parameters \n",
    "# theta = np.random.rand(10, 10)\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "# for t in range(T):\n",
    "#     # Choose action based on the policy  (s,a)\n",
    "#     logits = np.dot(state, theta)\n",
    "#     action_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "#     action = np.random.choice(range(len(action_probs)), p=action_probs)\n",
    "\n",
    "#     # Calculate reward and terminal flag\n",
    "#     true_label = D[t][1]\n",
    "#     reward, terminal = reward_fn(action, true_label, action, lambda_val=0.1)\n",
    "#     print(\"Step:\", t, \"True Label:\", true_label, \"Action:\", action, \"Reward:\", reward)\n",
    "\n",
    "#     # Update state\n",
    "#     state_next = D[t+1][0] if t < T - 1 else state\n",
    "\n",
    "#     # Store transition in replay memory\n",
    "#     replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#     if len(replay_memory) > M:\n",
    "#         replay_memory.pop(0)\n",
    "\n",
    "#     # Sample minibatch from replay memory\n",
    "#     if len(replay_memory) >= batch_size:\n",
    "#         minibatch = random.sample(replay_memory, batch_size)\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "#     X = np.zeros((batch_size, 10))\n",
    "#     y = np.zeros((batch_size, 2))\n",
    "#     for i in range(batch_size):\n",
    "#         state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#         X[i] = state_i\n",
    "#         y[i] = agent_network.predict(state_i.reshape(1, -1))[0]\n",
    "\n",
    "#         if terminal_i:\n",
    "#             y[i][action_i] = reward_i\n",
    "#         else:\n",
    "#             # Choose action based on the policy  (s,a) for the next state\n",
    "#             logits_next = np.dot(state_next_i, theta)\n",
    "#             action_probs_next = np.exp(logits_next) / np.sum(np.exp(logits_next))\n",
    "#             next_action = np.random.choice(range(2), p=action_probs_next)\n",
    "#             y[i][action_i] = reward_i + gamma * y[i][next_action]\n",
    "\n",
    "#     # Train agent network on minibatch using gradient descent\n",
    "#     agent_network.train_on_batch(X, y)\n",
    "\n",
    "#     # Update state\n",
    "#     state = state_next\n",
    "\n",
    "#     # Check if episode is over\n",
    "#     if terminal:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01187701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# # # Shuffle the training data\n",
    "# # X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "\n",
    "# #zip() function is used to pair up each element of X_train_shuffled (training data) with its \n",
    "# #corresponding element of y_train_resampled_final (training labels), creating a list of tuples\n",
    "# D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# # Initialize replay memory with capacity, M\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 10  # Number of episodes\n",
    "# T = 3  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # Randomly initialize parameters .  denotes the parameters of the policy function used to choose actions. \n",
    "# # The 1st dimension of  corresponds to the number of features in the state representation,and the 2nd\n",
    "# # dimension corresponds to the number of possible actions (The 2 possible actions: Decline transation if 'Fraud' \n",
    "# # or Allow transaction if 'Non-fraud').\n",
    "# theta = np.random.rand(10, 2)\n",
    "\n",
    "# # Agent training\n",
    "# for k in range(K):\n",
    "    \n",
    "#     # Shuffle the dataset D\n",
    "#     np.random.shuffle(D)\n",
    "   \n",
    "#     print(\"Episode \", k)\n",
    "#     print(\"--------------------------------------------\")\n",
    "    \n",
    "#     # Initialize state to D[0][0], which is the 1st element of the list of tuples D, where each tuple\n",
    "#     # contains a shuffled training sample as its first element and its corresponding label as the 2nd element.\n",
    "#     # Thus, variable 'state' stores the feature values of the first training sample.\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "        \n",
    "#         # Choose an action based on the policy  (s,a)\n",
    "#         # Dot product of the state and  is calculated, resulting in a 2-dimensional vector of scores (logits) for each action.\n",
    "#         # The logit is the natural logarithm of the odds that a binary outcome (e.g., 0 or 1) will occur. These scores represent \n",
    "#         # how likely each action is to be chosen given the current state.\n",
    "#         logits = np.dot(state, theta)\n",
    "#         action_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "#         action = 1 if action_probs[1] > 0.5 else 0\n",
    "\n",
    "#         # Calculate reward and terminal flag\n",
    "#         true_label = D[t][1]\n",
    "#         reward, terminal = reward_fn(action, true_label, action, lambda_val=0.1)\n",
    "#         print(\"Step:\", t)\n",
    "#         print(\"True label is\", true_label, \". Agent has predicted:\", action)\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"\")\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "\n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "\n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))[0]\n",
    "\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 # Choose action based on the policy  (s,a) for the next state\n",
    "#                 logits_next = np.dot(state_next_i, theta)\n",
    "#                 action_probs_next = np.exp(logits_next) / np.sum(np.exp(logits_next))\n",
    "#                 next_action = 1 if action_probs_next[1] > 0.5 else 0\n",
    "#                 y[i][action_i] = reward_i + gamma * y[i][next_action]\n",
    "\n",
    "#         # Train agent network on minibatch using gradient descent\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "      \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "\n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27c89d",
   "metadata": {},
   "source": [
    "## NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc044a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.7\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 5\n",
    "max_steps = 5\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.85\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "            if predicted_label == 1:\n",
    "                reward -= 1\n",
    "    return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c6e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 5\n",
    "max_steps = 5\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.99\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "            if predicted_label == 1:\n",
    "                reward -= 1\n",
    "    return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.75\n",
    "epsilon = 0.3\n",
    "batch_size = 64\n",
    "num_episodes = 10\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.2\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.75\n",
    "epsilon = 0.3\n",
    "batch_size = 64\n",
    "num_episodes = 14\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.2\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.55\n",
    "epsilon = 0.3\n",
    "batch_size = 64\n",
    "num_episodes = 14\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.2\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68565a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 7\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.87\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62dc68a",
   "metadata": {},
   "source": [
    "## New agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "batch_size = 32\n",
    "num_episodes = 7\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.275\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.8\n",
    "epsilon = 0.3\n",
    "batch_size = 32\n",
    "num_episodes = 10\n",
    "max_steps = 4\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.0375\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce743a",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.8\n",
    "epsilon = 0.7\n",
    "batch_size = 32\n",
    "num_episodes = 10\n",
    "max_steps = 3\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.75\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 2000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c676d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 10\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.7\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 2000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fac5f",
   "metadata": {},
   "source": [
    "## New agent + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8114e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Iterate over actions in replay_memory and count how many times each action was taken\n",
    "# actions = [memory[1] for memory in replay_memory]\n",
    "# action_counts = np.zeros(10)\n",
    "# for action in actions:\n",
    "#     action_counts[action] += 1\n",
    "\n",
    "# # Plot action distribution\n",
    "# plt.bar(range(10), action_counts)\n",
    "# plt.xlabel('Action')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Action Distribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50528983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to store the success rate obtained by the agent over time\n",
    "# success_rates = []\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # ... (code for training the agent)\n",
    "    \n",
    "#     # Calculate success rate for this episode\n",
    "#     num_successes = 0\n",
    "#     for t in range(T):\n",
    "#         if D[t][1] == 1 and agent_network.predict(D[t][0].reshape(1, -1)).argmax() == 1:\n",
    "#             num_successes += 1\n",
    "#         elif D[t][1] == 0 and agent_network.predict(D[t][0].reshape(1, -1)).argmax() == 0:\n",
    "#             num_successes += 1\n",
    "#     success_rate = num_successes / T\n",
    "#     success_rates.append(success_rate)\n",
    "\n",
    "# # Plot the success rate obtained by the agent over time\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(success_rates)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Success Rate')\n",
    "# plt.title('Success Rate over Time')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
