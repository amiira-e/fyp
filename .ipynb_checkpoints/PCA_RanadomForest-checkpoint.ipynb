{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fe4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77aebe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4a9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dbbf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import random\n",
    "\n",
    "# def reservoir_sampling(iterable, k, header=True):\n",
    "#     reservoir = []\n",
    "#     for i, item in enumerate(iterable):\n",
    "#         if i < k:\n",
    "#             reservoir.append(item)\n",
    "#         else:\n",
    "#             j = random.randint(0, i)\n",
    "#             if j < k:\n",
    "#                 reservoir[j] = item\n",
    "#     return reservoir\n",
    "\n",
    "# # Open the input CSV file\n",
    "# with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\") as f:\n",
    "#     # Check if header line exists\n",
    "#     header = True\n",
    "#     first_line = f.readline()\n",
    "#     if not first_line.startswith('step,type,amount,nameOrig,oldbalanceOrg,newbalanceOrig,nameDest,oldbalanceDest,newbalanceDest,isFraud,isFlaggedFraud'):\n",
    "#         header = False\n",
    "#         f.seek(0)  # Rewind file pointer to beginning\n",
    "\n",
    "#     # Sample from remaining lines\n",
    "#     sampled_lines = reservoir_sampling(f, k=2300000, header=header)\n",
    "\n",
    "# # Open the output CSV file and write the subsample to it\n",
    "# with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample1300000.csv\", mode='w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     if header:\n",
    "#         writer.writerow(first_line.strip().split(','))\n",
    "#     for line in sampled_lines:\n",
    "#         writer.writerow(line.strip().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73182f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78004e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample1300000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a475b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0723ab28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5a09b",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d941b41a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15568\\3758050434.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Calculate the correlation matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcorr_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_sample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pearson'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Resize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# Plot the correlation matrix as a heatmap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mako'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_sample.corr(method='pearson')\n",
    "plt.figure(figsize=(7,5)) # Resize\n",
    "# Plot the correlation matrix as a heatmap\n",
    "sns.heatmap(corr_matrix, cmap='mako', center=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b32d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df_sample.corr(method='spearman')\n",
    "plt.figure(figsize=(7,5)) # Resize\n",
    "# Plot the correlation matrix as a heatmap\n",
    "sns.heatmap(corr_matrix, cmap='mako', center=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e551a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10738b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_sample.corr()\n",
    "\n",
    "# Print correlation matrix\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009be1c",
   "metadata": {},
   "source": [
    "## Distribution shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c026b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for feature in features:\n",
    "    plt.subplot(2,3,features.index(feature)+1)\n",
    "    sns.distplot(df_sample[feature],hist=True,color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change the data type of column 'A' from float64 to float32\n",
    "# df_sample['amount'] = df_sample['amount'].astype('float32')\n",
    "# df_sample['oldbalanceOrg'] = df_sample['oldbalanceOrg'].astype('float32')\n",
    "# df_sample['oldbalanceDest'] = df_sample['oldbalanceDest'].astype('float32')\n",
    "# df_sample['newbalanceOrig'] = df_sample['newbalanceOrig'].astype('float32')\n",
    "# df_sample['newbalanceDest'] = df_sample['newbalanceDest'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3200a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample['step'] = df_sample['step'].astype('int32')\n",
    "# df_sample['isFlaggedFraud'] = df_sample['isFlaggedFraud'].astype('int32') \n",
    "# df_sample['isFraud'] = df_sample['isFraud'].astype('int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90be74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c850827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c998f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4726858",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c2c4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80d76c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9678a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count: 629208\n",
      "Class 1 count: 792\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_train)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d2295ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f4d70",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9956a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8de52ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count: 314602\n",
      "Class 1 count: 125841\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_resampled)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac687784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440443, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37071d3f",
   "metadata": {},
   "source": [
    "## Tomeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7c5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cbe4272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440435, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c22208",
   "metadata": {},
   "source": [
    "## ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "884b8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c99782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count: 311729\n",
      "Class 1 count: 125841\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_train_resampled_new)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd513977",
   "metadata": {},
   "source": [
    "## OSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "540e71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7344156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count: 311434\n",
      "Class 1 count: 125841\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = np.bincount(y_train_resampled_final)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b69f5014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count: 69912\n",
      "Class 1 count: 88\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_test)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8160d1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437275, 10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55a1be8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437275,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b64fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\trainPRIOR.csv\", index=False)\n",
    "#X_test.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086706d5",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3996395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set trimmed means:  {'amount': 579074.7934181612, 'oldbalanceOrg': 391336.54247682774, 'newbalanceOrig': 420198.53580608586, 'oldbalanceDest': 689757.5702524151, 'newbalanceDest': 1030991.099533203}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "random.seed(0)\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.01, 'oldbalanceOrg': 0.07, 'newbalanceOrig': 0.015, 'oldbalanceDest': 0.015, 'newbalanceDest': 0.01}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train_resampled_final.loc[X_train_resampled_final[col_name] > np.percentile(X_train_resampled_final[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "    # Replace the outliers in the test set with the trimmed means obtained from the train set\n",
    "    test_outliers = X_test.loc[X_test[col_name] > np.percentile(X_test[col_name], 100*(1-trim_props[col_name])), col_name]\n",
    "    X_test.loc[test_outliers.index, col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf41283",
   "metadata": {},
   "source": [
    "## New trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6edbbaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "\n",
    "# # Specify columns with outliers\n",
    "# cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# # Specify the number of bootstrapped samples to create per column\n",
    "# num_samples = 50\n",
    "\n",
    "# # Specify the trimming proportions for each column\n",
    "# trim_props = {'amount': 0.01, 'oldbalanceOrg': (0.07, 0.03), 'newbalanceOrig': 0.015, 'oldbalanceDest': 0.015, 'newbalanceDest': 0.01}\n",
    "\n",
    "# # Initialize empty dictionaries to store the trimmed means for each column\n",
    "# train_trimmed_means = {}\n",
    "\n",
    "# # Loop over the specified columns\n",
    "# for col_name in cols_with_outliers:\n",
    "    \n",
    "#     # Check if the trimming proportion for this column is a tuple with two values\n",
    "#     if isinstance(trim_props[col_name], tuple):\n",
    "#         # If so, perform asymmetric trimming for the oldbalanceOrg column\n",
    "#         if col_name == 'oldbalanceOrg':\n",
    "#             # Calculate the median of the bootstrapped sample for the training set\n",
    "#             train_median = np.median(train_sample)\n",
    "#             train_trimmed_means[col_name] = train_median\n",
    "#             # Replace the outliers in the training set with the trimmed means\n",
    "#             X_train_resampled_final.loc[(X_train_resampled_final[col_name] < train_trimmed_means[col_name]) | (X_train_resampled_final[col_name] > train_trimmed_means[col_name]), col_name] = train_median\n",
    "\n",
    "#             continue\n",
    "#         else:\n",
    "#             continue\n",
    "    \n",
    "#     # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "#     train_bootstrapped_samples = []\n",
    "#     train_trimmed_means_list = []\n",
    "    \n",
    "#     # Loop over the number of desired samples\n",
    "#     for i in range(num_samples):\n",
    "#         # Randomly select indices from the column in the training set\n",
    "#         train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "#         # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "#         train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "#         # Calculate the right and left trimmed means of the bootstrapped sample for the training set\n",
    "#         train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "#         train_left_trimmed_mean = np.mean(train_sample[train_sample >= np.percentile(train_sample, 100*trim_props[col_name])])\n",
    "#         train_trimmed_means_list.append((train_left_trimmed_mean, train_right_trimmed_mean))\n",
    "        \n",
    "#     # Calculate the mean of the left and right trimmed means for the training set and add it to the dictionary\n",
    "#     train_left_mean = np.mean([x[0] for x in train_trimmed_means_list])\n",
    "#     train_right_mean = np.mean([x[1] for x in train_trimmed_means_list])\n",
    "#     train_trimmed_means[col_name] = (train_left_mean, train_right_mean)\n",
    "\n",
    "#     # Replace the outliers in the training set with the trimmed means\n",
    "#     X_train_resampled_final.loc[(X_train_resampled_final[col_name] < train_trimmed_means[col_name][0]) | (X_train_resampled_final[col_name] > train_trimmed_means[col_name][1]), col_name] = np.mean(train_sample)\n",
    "\n",
    "# # Print the trimmed means\n",
    "# print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fadbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_resampled_final.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\trainPOST29.csv\", index=False)\n",
    "# #X_test.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ebc1cc",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "48406ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# assuming X_train and X_test are your training and test data matrices\n",
    "# standardize the data using the mean and std from the training set\n",
    "X_train_mean = np.mean(X_train_resampled_final, axis=0)\n",
    "X_train_std = np.std(X_train_resampled_final, axis=0)\n",
    "X_train_std[X_train_std == 0] = 1 # avoid division by zero\n",
    "X_train_std_inv = 1 / X_train_std\n",
    "\n",
    "X_train_stdized = (X_train_resampled_final - X_train_mean) * X_train_std_inv\n",
    "X_test_stdized = (X_test - X_train_mean) * X_train_std_inv\n",
    "\n",
    "# compute the covariance matrix for the training data\n",
    "cov_matrix_train = np.cov(X_train_stdized.T)\n",
    "\n",
    "# compute the eigenvectors and eigenvalues for the training data\n",
    "eig_vals_train, eig_vecs_train = np.linalg.eig(cov_matrix_train)\n",
    "\n",
    "# select the top k eigenvectors for the training data\n",
    "pca_train = PCA(n_components=3)\n",
    "X_train_pca = pca_train.fit_transform(X_train_stdized)\n",
    "\n",
    "# project the test data onto the selected eigenvectors from the training data\n",
    "X_test_pca = pca_train.transform(X_test_stdized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e03563db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = list(df_sample.columns)\n",
    "\n",
    "# # print the selected features\n",
    "# print(\"Selected features:\")\n",
    "# for i in range(pca_train.n_components_):\n",
    "#     # find the index of the maximum absolute value in the ith row of the components array\n",
    "#     idx = np.argmax(np.abs(pca_train.components_[i]))\n",
    "#     # print the name of the feature with the maximum absolute value in the ith row of the components array\n",
    "#     print(f\"PC{i+1}: {feature_names[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e0525f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04702151,  0.22531604,  0.26901231,  0.25768418,  0.48608308,\n",
       "        0.52402073,  0.00502339, -0.37584722, -0.39504699,  0.00546928])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_train.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dfa2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train_pca_df = pd.DataFrame(X_train_pca)\n",
    "X_test_pca_df = pd.DataFrame(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f8f47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_df = pd.DataFrame(X_train_pca)\n",
    "y_train_resampled_final = pd.Series(y_train_resampled_final)\n",
    "\n",
    "X_train_pca_df.reset_index(drop=True, inplace=True)\n",
    "y_train_resampled_final.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b023afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca_df = X_test_pca_df.rename(columns={0: 'PC1', 1: 'PC2', 2: 'PC3'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcdfba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.376125</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.247578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.669300</td>\n",
       "      <td>-0.765709</td>\n",
       "      <td>0.255523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.876609</td>\n",
       "      <td>-0.634862</td>\n",
       "      <td>0.183134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941172</td>\n",
       "      <td>-0.056670</td>\n",
       "      <td>-2.067035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.222762</td>\n",
       "      <td>0.396199</td>\n",
       "      <td>1.241969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>-0.464641</td>\n",
       "      <td>-0.844117</td>\n",
       "      <td>-0.224723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>-1.844180</td>\n",
       "      <td>0.087818</td>\n",
       "      <td>0.312542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>2.281064</td>\n",
       "      <td>0.269918</td>\n",
       "      <td>-3.411714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>-1.899556</td>\n",
       "      <td>-0.153012</td>\n",
       "      <td>0.483744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>-0.228792</td>\n",
       "      <td>-1.146548</td>\n",
       "      <td>-0.043712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PC1       PC2       PC3\n",
       "0     -1.376125  0.005835  0.247578\n",
       "1     -1.669300 -0.765709  0.255523\n",
       "2      0.876609 -0.634862  0.183134\n",
       "3      0.941172 -0.056670 -2.067035\n",
       "4     -0.222762  0.396199  1.241969\n",
       "...         ...       ...       ...\n",
       "69995 -0.464641 -0.844117 -0.224723\n",
       "69996 -1.844180  0.087818  0.312542\n",
       "69997  2.281064  0.269918 -3.411714\n",
       "69998 -1.899556 -0.153012  0.483744\n",
       "69999 -0.228792 -1.146548 -0.043712\n",
       "\n",
       "[70000 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pca_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69a914c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_df = X_train_pca_df.rename(columns={0: 'PC1', 1: 'PC2', 2: 'PC3', 3: 'PC4',4: 'PC5',5: 'PC6'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd002ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PC1', 'PC2', 'PC3'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pca_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17bb1639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.416652</td>\n",
       "      <td>-0.457311</td>\n",
       "      <td>0.124340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.867388</td>\n",
       "      <td>-1.523607</td>\n",
       "      <td>0.422878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.906552</td>\n",
       "      <td>0.481416</td>\n",
       "      <td>3.083051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.131031</td>\n",
       "      <td>-0.252012</td>\n",
       "      <td>-0.090627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.840015</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.514540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437270</th>\n",
       "      <td>0.415895</td>\n",
       "      <td>0.385566</td>\n",
       "      <td>0.044324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437271</th>\n",
       "      <td>1.307946</td>\n",
       "      <td>-0.112801</td>\n",
       "      <td>0.259696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437272</th>\n",
       "      <td>-0.903502</td>\n",
       "      <td>0.231948</td>\n",
       "      <td>-0.115229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437273</th>\n",
       "      <td>-0.256247</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>-0.237099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437274</th>\n",
       "      <td>0.606019</td>\n",
       "      <td>-0.897208</td>\n",
       "      <td>0.348269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PC1       PC2       PC3\n",
       "0       0.416652 -0.457311  0.124340\n",
       "1       1.867388 -1.523607  0.422878\n",
       "2       2.906552  0.481416  3.083051\n",
       "3      -0.131031 -0.252012 -0.090627\n",
       "4      -1.840015  0.058185  0.514540\n",
       "...          ...       ...       ...\n",
       "437270  0.415895  0.385566  0.044324\n",
       "437271  1.307946 -0.112801  0.259696\n",
       "437272 -0.903502  0.231948 -0.115229\n",
       "437273 -0.256247  0.034127 -0.237099\n",
       "437274  0.606019 -0.897208  0.348269\n",
       "\n",
       "[437275 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eedf6f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437275, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f395d",
   "metadata": {},
   "source": [
    "## Scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "433dbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # get the explained variance ratios\n",
    "# variance_ratio = pca_train .explained_variance_ratio_\n",
    "\n",
    "# # create a scree plot\n",
    "# plt.plot(np.arange(1, len(variance_ratio)+1), variance_ratio, 'o-', color='gray', linewidth=2)\n",
    "# plt.title('Scree Plot: Variance Explained')\n",
    "# plt.xlabel('Principal Components')\n",
    "# plt.ylabel('Proportion of Variance Explained')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d66dcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.416652</td>\n",
       "      <td>-0.457311</td>\n",
       "      <td>0.124340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.867388</td>\n",
       "      <td>-1.523607</td>\n",
       "      <td>0.422878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.906552</td>\n",
       "      <td>0.481416</td>\n",
       "      <td>3.083051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.131031</td>\n",
       "      <td>-0.252012</td>\n",
       "      <td>-0.090627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.840015</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.514540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.688454</td>\n",
       "      <td>-0.477339</td>\n",
       "      <td>0.410694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3\n",
       "0  0.416652 -0.457311  0.124340\n",
       "1  1.867388 -1.523607  0.422878\n",
       "2  2.906552  0.481416  3.083051\n",
       "3 -0.131031 -0.252012 -0.090627\n",
       "4 -1.840015  0.058185  0.514540\n",
       "5  0.688454 -0.477339  0.410694"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84d36774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f483b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # create a PCA object\n",
    "# pca = PCA()\n",
    "\n",
    "# # fit the PCA object to your data\n",
    "# pca.fit(X)\n",
    "\n",
    "# # get the eigenvalues\n",
    "# eigenvalues = pca_train.explained_variance_\n",
    "\n",
    "# # create a scree plot\n",
    "# plt.plot(np.arange(1, len(eigenvalues)+1), eigenvalues, 'bo-', linewidth=2)\n",
    "# plt.title('Scree Plot')\n",
    "# plt.xlabel('Principal Component')\n",
    "# plt.ylabel('Eigenvalue')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df6ee46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # create a PCA object\n",
    "# pca = PCA()\n",
    "\n",
    "# # fit the PCA object to your data\n",
    "# pca.fit(X)\n",
    "\n",
    "# # get the eigenvalues\n",
    "# eigenvalues = pca_train.explained_variance_\n",
    "\n",
    "# # create a scree plot\n",
    "# plt.plot(np.arange(1, len(eigenvalues)+1), eigenvalues, 'o-', color='gray', linewidth=1, markersize=5)\n",
    "# plt.axhline(y=1, linestyle='--', color='black', linewidth=1)\n",
    "# plt.title('Scree Plot: PCA Eigenvalues')\n",
    "# plt.xlabel('Principal Components')\n",
    "# plt.ylabel('Eigenvalues')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a04a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc2fec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plot circle\n",
    "# #Create a list of 500 points with equal spacing between -1 and 1\n",
    "# import numpy as np\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# columns=X_train_resampled_final.columns.values #Store the name of the columns for labeling\n",
    "\n",
    "# x=np.linspace(start=-1,stop=1,num=1000)\n",
    "# #Find y1 and y2 for these points\n",
    "# y_positive=lambda x: np.sqrt(1-x**2) \n",
    "# y_negative=lambda x: -np.sqrt(1-x**2)\n",
    "# plt.plot(x,list(map(y_positive, x)), color='maroon')\n",
    "# plt.plot(x,list(map(y_negative, x)),color='maroon')\n",
    "\n",
    "# #Plot smaller circle\n",
    "# x=np.linspace(start=-0.5,stop=0.5,num=500)\n",
    "# y_positive=lambda x: np.sqrt(0.5**2-x**2) \n",
    "# y_negative=lambda x: -np.sqrt(0.5**2-x**2)\n",
    "# plt.plot(x,list(map(y_positive, x)), color='maroon')\n",
    "# plt.plot(x,list(map(y_negative, x)),color='maroon')\n",
    "\n",
    "# #Create broken lines\n",
    "# x=np.linspace(start=-1,stop=1,num=30)\n",
    "# plt.scatter(x,[0]*len(x), marker='_',color='maroon')\n",
    "# plt.scatter([0]*len(x), x, marker='|',color='maroon')\n",
    "\n",
    "# pca_values=pca.components_\n",
    "# #Define color list\n",
    "# colors = ['pink', 'green','purple', 'blue','red','black']\n",
    "# if len(pca_values[0]) > 5:\n",
    "#     colors=colors*(int(len(pca_values[0])/5)+1)\n",
    "    \n",
    "#     add_string=\"\"\n",
    "#     for i in range(6):\n",
    "#         xi=pca_values[0][i]\n",
    "#         yi=pca_values[1][i]\n",
    "#         plt.arrow(0,0, \n",
    "#                   dx=xi, dy=yi, \n",
    "#                   head_width=0.03, head_length=0.03, \n",
    "#                   color=colors[i], length_includes_head=True)\n",
    "#         add_string=f\" ({round(xi,2)} {round(yi,2)})\"\n",
    "# #         plt.text(pca_values[0, i], \n",
    "# #                  pca_values[1, i] , \n",
    "# #                  s=columns[i] + add_string,\n",
    "# #                  fontsize=5)\n",
    "#         plt.text(pca_values[0, i] + 0.0, pca_values[1, i] + 0.07, s=columns[i] + add_string, fontsize=8)\n",
    "        \n",
    "# plt.xlabel(f\"Component 1 ({round(pca_train.explained_variance_ratio_[0]*100,2)}%)\")\n",
    "# plt.ylabel(f\"Component 2 ({round(pca_train.explained_variance_ratio_[1]*100,2)}%)\")\n",
    "# plt.title('Variable factor map (PCA)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f3ce5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = X_train_pca[:,0]\n",
    "# ys = X_train_pca[:,1]\n",
    "# scalex = 1.0/(xs.max() - xs.min())\n",
    "# scaley = 1.0/(ys.max() - ys.min())\n",
    "# fig, ax = plt.subplots(figsize=(14, 9))\n",
    " \n",
    "# for i, feature in enumerate(columns):\n",
    "#     ax.arrow(0, 0, pca_train.components_[0, i], \n",
    "#              pca_train.components_[1, i])\n",
    "#     ax.text(pca_train.components_[0, i] * 1.15, \n",
    "#             pca_train.components_[1, i] * 1.15, \n",
    "#             feature, fontsize=10)\n",
    " \n",
    "#     ax.scatter(xs * scalex,ys * scaley)\n",
    " \n",
    "#     ax.set_xlabel('PC1', fontsize=10)\n",
    "#     ax.set_ylabel('PC2', fontsize=10)\n",
    "#     ax.set_title('Biplot', fontsize=15)\n",
    "#     plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca619079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the PCA components (loadings)\n",
    "# PCs = pca.components_\n",
    "\n",
    "# # Use quiver to generate the basic plot\n",
    "# fig = plt.figure(figsize=(5,5))\n",
    "# plt.quiver(np.zeros(PCs.shape[1]), np.zeros(PCs.shape[1]),\n",
    "#            PCs[0,:], PCs[1,:], \n",
    "#            angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "# # Add labels based on feature names (here just numbers)\n",
    "# feature_names = np.arange(PCs.shape[1])\n",
    "# for i,j,z in zip(PCs[1,:]+0.02, PCs[0,:]+0.02, feature_names):\n",
    "#     plt.text(j, i, z, ha='center', va='center')\n",
    "\n",
    "# # Add unit circle\n",
    "# circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')\n",
    "# plt.gca().add_artist(circle)\n",
    "\n",
    "# # Ensure correct aspect ratio and axis limits\n",
    "# plt.axis('equal')\n",
    "# plt.xlim([-1.0,1.0])\n",
    "# plt.ylim([-1.0,1.0])\n",
    "\n",
    "# # Label axes\n",
    "# plt.xlabel('PC 0')\n",
    "# plt.ylabel('PC 1')\n",
    "\n",
    "# # Done\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf9ba8",
   "metadata": {},
   "source": [
    "## Linear Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b085d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.linear_model import Perceptron\n",
    "\n",
    "# # Create a Perceptron object\n",
    "# clf = Perceptron(random_state=0)\n",
    "\n",
    "# # Train the Perceptron on the data\n",
    "# clf.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "\n",
    "# # Predict the output classes for the data points\n",
    "# y_pred = clf.predict(X_train_resampled_final)\n",
    "\n",
    "# # Check if the Perceptron correctly classified all the data points\n",
    "# if np.all(y_pred == y_train_resampled_final):\n",
    "#     print(\"Data is linearly separable\")\n",
    "# else:\n",
    "#     print(\"Data is not linearly separable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c228282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming your DataFrame is called df and the class column is called 'class'\n",
    "# class0 = df_sample[df_sample['isFraud'] == 0]\n",
    "# class1 = df_sample[df_sample['isFraud'] == 1]\n",
    "\n",
    "# s = 5\n",
    "# plt.scatter(class0['step'], class0['oldbalanceOrg'], color='blue', label='Class 0',marker='.', s=s)\n",
    "# plt.scatter(class1['step'], class1['oldbalanceOrg'], color='red', label='Class 1',marker='.', s=s)\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlabel('step')\n",
    "# plt.ylabel('oldbalanceOrg')\n",
    "# plt.title('Scatter plot of two classes')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd4792f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming your DataFrame is called df and the class column is called 'class'\n",
    "# class0 = df_sample[df_sample['isFraud'] == 0]\n",
    "# class1 = df_sample[df_sample['isFraud'] == 1]\n",
    "\n",
    "# s=4\n",
    "# plt.scatter(class0['step'], class0['oldbalanceOrg'], color='blue', label='Class 0',marker='.', s=s)\n",
    "# plt.scatter(class1['step'], class1['oldbalanceOrg'], color='red', label='Class',marker='.', s=s)\n",
    "\n",
    "# # Fit a linear SVM to the data\n",
    "# from sklearn.svm import SVC\n",
    "# X_new = df_sample[['step', 'oldbalanceOrg']]\n",
    "# y_new = df_sample['isFraud']\n",
    "# svm = SVC(kernel='linear')\n",
    "# svm.fit(X_new, y_new)\n",
    "\n",
    "# # Plot the decision boundary\n",
    "# w = svm.coef_[0]\n",
    "# a = -w[0] / w[1]\n",
    "# xx = np.linspace(np.min(X_new['step']), np.max(X_new['step']))\n",
    "# yy = a * xx - svm.intercept_[0] / w[1]\n",
    "# plt.plot(xx, yy, 'k-', label='Decision boundary')\n",
    "\n",
    "# plt.legend()\n",
    "# plt.xlabel('step')\n",
    "# plt.ylabel('oldbalanceOrg')\n",
    "# plt.title('Scatter plot of two classes with decision boundary')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917bc01",
   "metadata": {},
   "source": [
    "## Choose 3 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5b10bed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.416652</td>\n",
       "      <td>-0.457311</td>\n",
       "      <td>0.124340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.867388</td>\n",
       "      <td>-1.523607</td>\n",
       "      <td>0.422878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.906552</td>\n",
       "      <td>0.481416</td>\n",
       "      <td>3.083051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.131031</td>\n",
       "      <td>-0.252012</td>\n",
       "      <td>-0.090627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.840015</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.514540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437270</th>\n",
       "      <td>0.415895</td>\n",
       "      <td>0.385566</td>\n",
       "      <td>0.044324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437271</th>\n",
       "      <td>1.307946</td>\n",
       "      <td>-0.112801</td>\n",
       "      <td>0.259696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437272</th>\n",
       "      <td>-0.903502</td>\n",
       "      <td>0.231948</td>\n",
       "      <td>-0.115229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437273</th>\n",
       "      <td>-0.256247</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>-0.237099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437274</th>\n",
       "      <td>0.606019</td>\n",
       "      <td>-0.897208</td>\n",
       "      <td>0.348269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PC1       PC2       PC3\n",
       "0       0.416652 -0.457311  0.124340\n",
       "1       1.867388 -1.523607  0.422878\n",
       "2       2.906552  0.481416  3.083051\n",
       "3      -0.131031 -0.252012 -0.090627\n",
       "4      -1.840015  0.058185  0.514540\n",
       "...          ...       ...       ...\n",
       "437270  0.415895  0.385566  0.044324\n",
       "437271  1.307946 -0.112801  0.259696\n",
       "437272 -0.903502  0.231948 -0.115229\n",
       "437273 -0.256247  0.034127 -0.237099\n",
       "437274  0.606019 -0.897208  0.348269\n",
       "\n",
       "[437275 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca_df.iloc[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "193e538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca_df=X_test_pca_df.iloc[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2dd82f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.376125</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.247578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.669300</td>\n",
       "      <td>-0.765709</td>\n",
       "      <td>0.255523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.876609</td>\n",
       "      <td>-0.634862</td>\n",
       "      <td>0.183134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.941172</td>\n",
       "      <td>-0.056670</td>\n",
       "      <td>-2.067035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.222762</td>\n",
       "      <td>0.396199</td>\n",
       "      <td>1.241969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>-0.464641</td>\n",
       "      <td>-0.844117</td>\n",
       "      <td>-0.224723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>-1.844180</td>\n",
       "      <td>0.087818</td>\n",
       "      <td>0.312542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>2.281064</td>\n",
       "      <td>0.269918</td>\n",
       "      <td>-3.411714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>-1.899556</td>\n",
       "      <td>-0.153012</td>\n",
       "      <td>0.483744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>-0.228792</td>\n",
       "      <td>-1.146548</td>\n",
       "      <td>-0.043712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PC1       PC2       PC3\n",
       "0     -1.376125  0.005835  0.247578\n",
       "1     -1.669300 -0.765709  0.255523\n",
       "2      0.876609 -0.634862  0.183134\n",
       "3      0.941172 -0.056670 -2.067035\n",
       "4     -0.222762  0.396199  1.241969\n",
       "...         ...       ...       ...\n",
       "69995 -0.464641 -0.844117 -0.224723\n",
       "69996 -1.844180  0.087818  0.312542\n",
       "69997  2.281064  0.269918 -3.411714\n",
       "69998 -1.899556 -0.153012  0.483744\n",
       "69999 -0.228792 -1.146548 -0.043712\n",
       "\n",
       "[70000 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b3b335c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_df=X_train_pca_df.iloc[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89ff3ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.416652</td>\n",
       "      <td>-0.457311</td>\n",
       "      <td>0.124340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.867388</td>\n",
       "      <td>-1.523607</td>\n",
       "      <td>0.422878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.906552</td>\n",
       "      <td>0.481416</td>\n",
       "      <td>3.083051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.131031</td>\n",
       "      <td>-0.252012</td>\n",
       "      <td>-0.090627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.840015</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.514540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437270</th>\n",
       "      <td>0.415895</td>\n",
       "      <td>0.385566</td>\n",
       "      <td>0.044324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437271</th>\n",
       "      <td>1.307946</td>\n",
       "      <td>-0.112801</td>\n",
       "      <td>0.259696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437272</th>\n",
       "      <td>-0.903502</td>\n",
       "      <td>0.231948</td>\n",
       "      <td>-0.115229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437273</th>\n",
       "      <td>-0.256247</td>\n",
       "      <td>0.034127</td>\n",
       "      <td>-0.237099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437274</th>\n",
       "      <td>0.606019</td>\n",
       "      <td>-0.897208</td>\n",
       "      <td>0.348269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             PC1       PC2       PC3\n",
       "0       0.416652 -0.457311  0.124340\n",
       "1       1.867388 -1.523607  0.422878\n",
       "2       2.906552  0.481416  3.083051\n",
       "3      -0.131031 -0.252012 -0.090627\n",
       "4      -1.840015  0.058185  0.514540\n",
       "...          ...       ...       ...\n",
       "437270  0.415895  0.385566  0.044324\n",
       "437271  1.307946 -0.112801  0.259696\n",
       "437272 -0.903502  0.231948 -0.115229\n",
       "437273 -0.256247  0.034127 -0.237099\n",
       "437274  0.606019 -0.897208  0.348269\n",
       "\n",
       "[437275 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8913f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# n_folds = 2\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "# #class_weights={0:1,0:75}\n",
    "# #rf_model = RandomForestClassifier(criterion='entropy', max_depth= 8, max_features='log2',n_estimators=251,oob_score=True)\n",
    "# #rf_model = RandomForestClassifier(ccp_alpha=0.01,criterion='gini', max_depth= 3, max_features='log2',n_estimators=100,oob_score=True)\n",
    "# rf_model = RandomForestClassifier()\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    \n",
    "#     # Split the data into training and validation sets\n",
    "#     X_fold_train, y_fold_train = X_train_pca_df.iloc[train_idx].values, y_train_resampled_final[train_idx]\n",
    "#     X_val, y_val = X_train_pca_df.iloc[val_idx].values, y_train_resampled_final[val_idx]\n",
    "\n",
    "#     # Train and evaluate the model\n",
    "#     rf_model.fit(X_fold_train, y_fold_train)\n",
    "#     score = rf_model.score(X_val, y_val)\n",
    "    \n",
    "#     print(f\"Fold {fold}: Validation score = {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7218ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# n_folds = 2\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "# rf_model = RandomForestClassifier()\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    \n",
    "#     # Split the data into training and validation sets\n",
    "#     X_fold_train, y_fold_train = X_train_pca_df.iloc[train_idx].values, y_train_resampled_final[train_idx]\n",
    "#     X_val, y_val = X_train_pca_df.iloc[val_idx].values, y_train_resampled_final[val_idx]\n",
    "\n",
    "#     # Train and evaluate the model\n",
    "#     rf_model.fit(X_fold_train, y_fold_train)\n",
    "#     y_pred = rf_model.predict(X_val)\n",
    "#     score = rf_model.score(X_val, y_val)\n",
    "#     print(f\"Fold {fold}: Validation score = {score:.3f}\")\n",
    "    \n",
    "#     # Print classification report\n",
    "#     print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d17904f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# n_folds = 2\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "# rf_model = RandomForestClassifier()\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    \n",
    "#     # Split the data into training and validation sets\n",
    "#     X_fold_train, y_fold_train = X_train_pca_df.iloc[train_idx].values, y_train_resampled_final[train_idx]\n",
    "#     X_val, y_val = X_train_pca_df.iloc[val_idx].values, y_train_resampled_final[val_idx]\n",
    "\n",
    "#     # Train and evaluate the model\n",
    "#     rf_model.fit(X_fold_train, y_fold_train)\n",
    "#     y_pred = rf_model.predict(X_val)\n",
    "#     score = rf_model.score(X_val, y_val)\n",
    "#     print(f\"Fold {fold}: Validation score = {score:.3f}\")\n",
    "    \n",
    "#     # Print confusion matrix\n",
    "#     print(f\"Confusion matrix for fold {fold}:\")\n",
    "#     print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f8c0f",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c967dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# # Define the parameter space to search over\n",
    "# param_dist = {\n",
    "#     'n_estimators': randint(100, 400),\n",
    "#     'max_features': ['sqrt', 'log2','none'],\n",
    "#     'max_depth': [None] + list(range(5, 20, 5)),\n",
    "#     'min_samples_split': randint(2, 15),\n",
    "#     'min_samples_leaf': randint(1, 15),\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# # Initialize the Random Forest model\n",
    "# rf_model = RandomForestClassifier()\n",
    "\n",
    "# # Initialize the RandomizedSearchCV object\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     rf_model, \n",
    "#     param_distributions=param_dist,\n",
    "#     n_iter=10,  # Number of iterations to sample from the parameter space\n",
    "#     cv=3,  # Number of cross-validation folds to use\n",
    "# )\n",
    "\n",
    "# # Fit the RandomizedSearchCV object to the data\n",
    "# random_search.fit(X_train_pca_df, y_train_resampled_final)\n",
    "\n",
    "# # Print the best hyperparameters and corresponding score\n",
    "# print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "# print(\"Best score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eab71c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Skipping weka as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-weka-wrapper3 in c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages (0.2.12)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy in c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages (from python-weka-wrapper3) (1.21.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages (from python-weka-wrapper3) (22.0)\n",
      "Requirement already satisfied: python-javabridge>=4.0.0 in c:\\users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages (from python-weka-wrapper3) (4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall weka\n",
    "!pip install python-weka-wrapper3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b6c46c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'scipy', 'scipy_available']\n"
     ]
    }
   ],
   "source": [
    "import weka.core\n",
    "print(dir(weka.core))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec105ce",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16ba976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.8684852\ttotal: 192ms\tremaining: 9.39s\n",
      "1:\tlearn: 0.8772577\ttotal: 206ms\tremaining: 4.95s\n",
      "2:\tlearn: 0.8662623\ttotal: 220ms\tremaining: 3.45s\n",
      "3:\tlearn: 0.8831625\ttotal: 234ms\tremaining: 2.69s\n",
      "4:\tlearn: 0.8829155\ttotal: 248ms\tremaining: 2.23s\n",
      "5:\tlearn: 0.8848411\ttotal: 261ms\tremaining: 1.92s\n",
      "6:\tlearn: 0.8832997\ttotal: 276ms\tremaining: 1.69s\n",
      "7:\tlearn: 0.8853671\ttotal: 290ms\tremaining: 1.52s\n",
      "8:\tlearn: 0.8867712\ttotal: 305ms\tremaining: 1.39s\n",
      "9:\tlearn: 0.8868627\ttotal: 319ms\tremaining: 1.27s\n",
      "10:\tlearn: 0.8874802\ttotal: 333ms\tremaining: 1.18s\n",
      "11:\tlearn: 0.8876128\ttotal: 351ms\tremaining: 1.11s\n",
      "12:\tlearn: 0.8880107\ttotal: 371ms\tremaining: 1.05s\n",
      "13:\tlearn: 0.8874116\ttotal: 389ms\tremaining: 1s\n",
      "14:\tlearn: 0.8882760\ttotal: 406ms\tremaining: 946ms\n",
      "15:\tlearn: 0.8882440\ttotal: 423ms\tremaining: 898ms\n",
      "16:\tlearn: 0.8886007\ttotal: 440ms\tremaining: 855ms\n",
      "17:\tlearn: 0.8891176\ttotal: 456ms\tremaining: 810ms\n",
      "18:\tlearn: 0.8890398\ttotal: 472ms\tremaining: 769ms\n",
      "19:\tlearn: 0.8892273\ttotal: 488ms\tremaining: 733ms\n",
      "20:\tlearn: 0.8891496\ttotal: 502ms\tremaining: 694ms\n",
      "21:\tlearn: 0.8903251\ttotal: 517ms\tremaining: 658ms\n",
      "22:\tlearn: 0.8908145\ttotal: 532ms\tremaining: 625ms\n",
      "23:\tlearn: 0.8912947\ttotal: 546ms\tremaining: 592ms\n",
      "24:\tlearn: 0.8911849\ttotal: 559ms\tremaining: 559ms\n",
      "25:\tlearn: 0.8920677\ttotal: 573ms\tremaining: 529ms\n",
      "26:\tlearn: 0.8921271\ttotal: 588ms\tremaining: 501ms\n",
      "27:\tlearn: 0.8921866\ttotal: 605ms\tremaining: 475ms\n",
      "28:\tlearn: 0.8926348\ttotal: 621ms\tremaining: 450ms\n",
      "29:\tlearn: 0.8929230\ttotal: 634ms\tremaining: 423ms\n",
      "30:\tlearn: 0.8935633\ttotal: 647ms\tremaining: 397ms\n",
      "31:\tlearn: 0.8943317\ttotal: 660ms\tremaining: 371ms\n",
      "32:\tlearn: 0.8946702\ttotal: 674ms\tremaining: 347ms\n",
      "33:\tlearn: 0.8950086\ttotal: 689ms\tremaining: 324ms\n",
      "34:\tlearn: 0.8951824\ttotal: 701ms\tremaining: 300ms\n",
      "35:\tlearn: 0.8954523\ttotal: 713ms\tremaining: 277ms\n",
      "36:\tlearn: 0.8957633\ttotal: 727ms\tremaining: 255ms\n",
      "37:\tlearn: 0.8961018\ttotal: 739ms\tremaining: 233ms\n",
      "38:\tlearn: 0.8968381\ttotal: 752ms\tremaining: 212ms\n",
      "39:\tlearn: 0.8973778\ttotal: 765ms\tremaining: 191ms\n",
      "40:\tlearn: 0.8978261\ttotal: 782ms\tremaining: 172ms\n",
      "41:\tlearn: 0.8978855\ttotal: 799ms\tremaining: 152ms\n",
      "42:\tlearn: 0.8980273\ttotal: 814ms\tremaining: 132ms\n",
      "43:\tlearn: 0.8982606\ttotal: 833ms\tremaining: 114ms\n",
      "44:\tlearn: 0.8985625\ttotal: 847ms\tremaining: 94.1ms\n",
      "45:\tlearn: 0.8984024\ttotal: 862ms\tremaining: 75ms\n",
      "46:\tlearn: 0.8988232\ttotal: 876ms\tremaining: 55.9ms\n",
      "47:\tlearn: 0.8988872\ttotal: 892ms\tremaining: 37.1ms\n",
      "48:\tlearn: 0.8991342\ttotal: 906ms\tremaining: 18.5ms\n",
      "49:\tlearn: 0.8995504\ttotal: 923ms\tremaining: 0us\n",
      "Fold 1\n",
      "Confusion matrix:\n",
      "[[149696   5896]\n",
      " [ 16232  46814]]\n",
      "Classification report:\n",
      "---------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93    155592\n",
      "           1       0.89      0.74      0.81     63046\n",
      "\n",
      "    accuracy                           0.90    218638\n",
      "   macro avg       0.90      0.85      0.87    218638\n",
      "weighted avg       0.90      0.90      0.90    218638\n",
      "\n",
      "0:\tlearn: 0.8682022\ttotal: 18.5ms\tremaining: 906ms\n",
      "1:\tlearn: 0.8828474\ttotal: 38.3ms\tremaining: 918ms\n",
      "2:\tlearn: 0.8670085\ttotal: 58.8ms\tremaining: 920ms\n",
      "3:\tlearn: 0.8860445\ttotal: 78.9ms\tremaining: 907ms\n",
      "4:\tlearn: 0.8857152\ttotal: 99ms\tremaining: 891ms\n",
      "5:\tlearn: 0.8857884\ttotal: 118ms\tremaining: 863ms\n",
      "6:\tlearn: 0.8873069\ttotal: 136ms\tremaining: 833ms\n",
      "7:\tlearn: 0.8871925\ttotal: 149ms\tremaining: 784ms\n",
      "8:\tlearn: 0.8876453\ttotal: 164ms\tremaining: 745ms\n",
      "9:\tlearn: 0.8865065\ttotal: 177ms\tremaining: 710ms\n",
      "10:\tlearn: 0.8886973\ttotal: 192ms\tremaining: 679ms\n",
      "11:\tlearn: 0.8880112\ttotal: 205ms\tremaining: 650ms\n",
      "12:\tlearn: 0.8881759\ttotal: 220ms\tremaining: 626ms\n",
      "13:\tlearn: 0.8895572\ttotal: 235ms\tremaining: 604ms\n",
      "14:\tlearn: 0.8885098\ttotal: 249ms\tremaining: 581ms\n",
      "15:\tlearn: 0.8888894\ttotal: 262ms\tremaining: 557ms\n",
      "16:\tlearn: 0.8892644\ttotal: 276ms\tremaining: 537ms\n",
      "17:\tlearn: 0.8897264\ttotal: 292ms\tremaining: 519ms\n",
      "18:\tlearn: 0.8911763\ttotal: 307ms\tremaining: 501ms\n",
      "19:\tlearn: 0.8909430\ttotal: 324ms\tremaining: 485ms\n",
      "20:\tlearn: 0.8905405\ttotal: 340ms\tremaining: 470ms\n",
      "21:\tlearn: 0.8903439\ttotal: 353ms\tremaining: 449ms\n",
      "22:\tlearn: 0.8917617\ttotal: 366ms\tremaining: 430ms\n",
      "23:\tlearn: 0.8928686\ttotal: 380ms\tremaining: 411ms\n",
      "24:\tlearn: 0.8929738\ttotal: 392ms\tremaining: 392ms\n",
      "25:\tlearn: 0.8934312\ttotal: 405ms\tremaining: 374ms\n",
      "26:\tlearn: 0.8935867\ttotal: 419ms\tremaining: 357ms\n",
      "27:\tlearn: 0.8939617\ttotal: 431ms\tremaining: 339ms\n",
      "28:\tlearn: 0.8943916\ttotal: 446ms\tremaining: 323ms\n",
      "29:\tlearn: 0.8946569\ttotal: 458ms\tremaining: 305ms\n",
      "30:\tlearn: 0.8947987\ttotal: 471ms\tremaining: 289ms\n",
      "31:\tlearn: 0.8950091\ttotal: 484ms\tremaining: 272ms\n",
      "32:\tlearn: 0.8953842\ttotal: 498ms\tremaining: 257ms\n",
      "33:\tlearn: 0.8956494\ttotal: 513ms\tremaining: 241ms\n",
      "34:\tlearn: 0.8957912\ttotal: 529ms\tremaining: 227ms\n",
      "35:\tlearn: 0.8960153\ttotal: 545ms\tremaining: 212ms\n",
      "36:\tlearn: 0.8961571\ttotal: 558ms\tremaining: 196ms\n",
      "37:\tlearn: 0.8962760\ttotal: 570ms\tremaining: 180ms\n",
      "38:\tlearn: 0.8966602\ttotal: 583ms\tremaining: 165ms\n",
      "39:\tlearn: 0.8968661\ttotal: 597ms\tremaining: 149ms\n",
      "40:\tlearn: 0.8969164\ttotal: 611ms\tremaining: 134ms\n",
      "41:\tlearn: 0.8971725\ttotal: 624ms\tremaining: 119ms\n",
      "42:\tlearn: 0.8973097\ttotal: 639ms\tremaining: 104ms\n",
      "43:\tlearn: 0.8976893\ttotal: 652ms\tremaining: 88.9ms\n",
      "44:\tlearn: 0.8978860\ttotal: 667ms\tremaining: 74.1ms\n",
      "45:\tlearn: 0.8982748\ttotal: 685ms\tremaining: 59.5ms\n",
      "46:\tlearn: 0.8981833\ttotal: 701ms\tremaining: 44.7ms\n",
      "47:\tlearn: 0.8982107\ttotal: 717ms\tremaining: 29.9ms\n",
      "48:\tlearn: 0.8989242\ttotal: 733ms\tremaining: 15ms\n",
      "49:\tlearn: 0.8990660\ttotal: 751ms\tremaining: 0us\n",
      "Fold 2\n",
      "Confusion matrix:\n",
      "[[149822   6020]\n",
      " [ 15961  46834]]\n",
      "Classification report:\n",
      "---------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93    155842\n",
      "           1       0.89      0.75      0.81     62795\n",
      "\n",
      "    accuracy                           0.90    218637\n",
      "   macro avg       0.89      0.85      0.87    218637\n",
      "weighted avg       0.90      0.90      0.90    218637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import catboost as cb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "n_folds = 2\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "clf = cb.CatBoostClassifier(iterations=50, learning_rate=0.1, depth=6, loss_function='Logloss', \n",
    "                             eval_metric='Accuracy', random_seed=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_fold_train, y_fold_train = X_train_pca_df.iloc[train_idx].values, y_train_resampled_final[train_idx]\n",
    "    X_val, y_val = X_train_pca_df.iloc[val_idx].values, y_train_resampled_final[val_idx]\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    clf.fit(X_fold_train, y_fold_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    score = clf.score(X_val, y_val)\n",
    "    #print(f\"Fold {fold}: Validation score = {score:.3f}\")\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    print(f\"Confusion matrix:\")\n",
    "    # Print confusion matrix\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    print(f\"Classification report:\")\n",
    "    print('---------------------')\n",
    "    # Print classification report\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    # Print the OOB score\n",
    "    #print(f\"OOB score: {rf_model.oob_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4413a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd931b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d422ac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x254839f6d48>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(clf, X_test_pca_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410bd4c",
   "metadata": {},
   "source": [
    "## PCA-BASED MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369863b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "n_folds = 2\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_fold_train, y_fold_train = X_train_pca_df.iloc[train_idx].values, y_train_resampled_final[train_idx]\n",
    "    X_val, y_val = X_train_pca_df.iloc[val_idx].values, y_train_resampled_final[val_idx]\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    rf_model.fit(X_fold_train, y_fold_train)\n",
    "    y_pred = rf_model.predict(X_val)\n",
    "    score = rf_model.score(X_val, y_val)\n",
    "    #print(f\"Fold {fold}: Validation score = {score:.3f}\")\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    print(f\"Confusion matrix:\")\n",
    "    # Print confusion matrix\n",
    "    print(confusion_matrix(y_val, y_pred))\n",
    "    print(f\"Classification report:\")\n",
    "    print('---------------------')\n",
    "    # Print classification report\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    # Print the OOB score\n",
    "    #print(f\"OOB score: {rf_model.oob_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685da952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(rf_model, X_test_pca_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888d6ec",
   "metadata": {},
   "source": [
    "## HalfRandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.experimental import enable_halving_search_cv  # Required to enable HalvingRandomSearchCV\n",
    "# from sklearn.model_selection import HalvingRandomSearchCV\n",
    "# import numpy as np\n",
    "\n",
    "# # Create the random forest model\n",
    "# rfc = RandomForestClassifier()\n",
    "\n",
    "# # Set the hyperparameters to tune and their possible values\n",
    "# param_dist = {\n",
    "#     'n_estimators': np.arange(100, 400),\n",
    "#     'max_features': ['sqrt', 'log2','auto']\n",
    "#     'max_depth': [5, 10, 15, 20, None],\n",
    "#     'min_samples_split': [2, 5, 15],\n",
    "#     'min_samples_leaf': [2, 5, 15],\n",
    "#     'bootstrap': [True, False],\n",
    "# }\n",
    "\n",
    "# # Set up the HalvingRandomSearchCV with aggressive early stopping\n",
    "# search = HalvingRandomSearchCV(rfc, param_dist, cv=5,verbose=1, \n",
    "#                                factor=2, resource='n_samples', max_resources=100, \n",
    "#                                aggressive_elimination=True, random_state=18, \n",
    "#                                scoring='accuracy', refit=True)\n",
    "\n",
    "# # Fit the HalvingRandomSearchCV object to the data\n",
    "# search.fit(X_train_pca_df, y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27fb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and evaluate on the test set\n",
    "best_params = search.best_params_\n",
    "best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bad5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e86b11",
   "metadata": {},
   "source": [
    "## RF-PCA Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec09298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "n_folds = 2\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "class_weights={0:15,1:70}\n",
    "rf_model = RandomForestClassifier(class_weight=class_weights)\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_fold_train, y_fold_train = X_train_pca_df.iloc[train_idx].values, y_train_resampled_final[train_idx]\n",
    "    X_val, y_val = X_train_pca_df.iloc[val_idx].values, y_train_resampled_final[val_idx]\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    rf_model.fit(X_fold_train, y_fold_train)\n",
    "#     y_pred = rf_model.predict(X_val)\n",
    "#     score = rf_model.score(X_val, y_val)\n",
    "    \n",
    "#     # Print confusion matrix and classification report\n",
    "#     print(f\"Fold {fold+1}\")\n",
    "#     print(f\"Confusion matrix:\")\n",
    "#     print(confusion_matrix(y_val, y_pred))\n",
    "#     print(f\"Classification report:\")\n",
    "#     print('---------------------')\n",
    "#     print(classification_report(y_val, y_pred))\n",
    "    \n",
    "#     # Get precision, recall, and f1 score for this fold\n",
    "#     report = classification_report(y_val, y_pred, output_dict=True)\n",
    "#     precision_list.append(report['weighted avg']['precision'])\n",
    "#     recall_list.append(report['weighted avg']['recall'])\n",
    "#     f1_list.append(report['weighted avg']['f1-score'])\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Total runtime: {total_time:.2f} seconds\")\n",
    "\n",
    "# print(f\"Total runtime: {total_time:.2f} seconds\")\n",
    "# # Calculate average precision, recall, and f1 score across all folds\n",
    "# avg_precision = sum(precision_list) / n_folds\n",
    "# avg_recall = sum(recall_list) / n_folds\n",
    "# avg_f1 = sum(f1_list) / n_folds\n",
    "\n",
    "# print(f\"Average precision: {avg_precision}\")\n",
    "# print(f\"Average recall: {avg_recall}\")\n",
    "# print(f\"Average F1 score: {avg_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# # Load iris dataset\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# # Train a random forest classifier with 3 decision trees\n",
    "rf_model = RandomForestClassifier(n_estimators=2)\n",
    "rf_model.fit(X_train_pca_df, y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(15, 15))\n",
    "# plot_tree(rf_model.estimators_[1], filled=True, ax=ax, max_depth=2, fontsize=14)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd19a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# n_folds = 2\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "# rf_model = RandomForestClassifier('n_estimators': 130, 'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 20, 'bootstrap': False)\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    \n",
    "#     # Split the data into training and validation sets\n",
    "#     X_fold_train, y_fold_train = X_train_pca_df.iloc[train_idx].values, y_train_resampled_final[train_idx]\n",
    "#     X_val, y_val = X_train_pca_df.iloc[val_idx].values, y_train_resampled_final[val_idx]\n",
    "\n",
    "#     # Train and evaluate the model\n",
    "#     rf_model.fit(X_fold_train, y_fold_train)\n",
    "#     y_pred = rf_model.predict(X_val)\n",
    "#     y_prob = rf_model.predict_proba(X_val)[:,1] # get probability estimates for positive class\n",
    "#     score = rf_model.score(X_val, y_val)\n",
    "#     print(f\"Fold {fold}: Validation score = {score:.3f}\")\n",
    "    \n",
    "#     # Print confusion matrix\n",
    "#     print(f\"Confusion matrix for fold {fold}:\")\n",
    "#     print(confusion_matrix(y_val, y_pred))\n",
    "    \n",
    "#     # Plot ROC curve\n",
    "#     fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "#     plt.plot(fpr, tpr, label=f\"Fold {fold} (AUC = {roc_auc:.2f})\")\n",
    "    \n",
    "# plt.plot([0, 1], [0, 1], 'k--', label='Random guessing')\n",
    "# plt.xlabel('False positive rate')\n",
    "# plt.ylabel('True positive rate')\n",
    "# plt.title('ROC curve for Random Forest classifier')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c1abe",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2023d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### from sklearn.model_selection import StratifiedKFold\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score, f1_score\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Define the number of splits for stratified cross-validation\n",
    "# n_splits = 2\n",
    "\n",
    "# # Initialize StratifiedKFold\n",
    "# skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "# # Create lists to store evaluation metrics for each fold\n",
    "# f1_scores = []\n",
    "# recall_scores = []\n",
    "# precision_scores = []\n",
    "# accuracy_scores = []\n",
    "\n",
    "# # Create lists to store ROC curve data for each fold\n",
    "# fprs = []\n",
    "# tprs = []\n",
    "# aucs = []\n",
    "\n",
    "# # Initialize the OOB error list\n",
    "# oob_error = []\n",
    "\n",
    "# # Iterate over each fold\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_pca_df, y_train_resampled_final)):\n",
    "#     print(f'Fold: {fold+1}')\n",
    "    \n",
    "#     # Split the data into training and validation sets\n",
    "#     X_fold_train, y_fold_train = X_train_pca_df[train_idx], y_train_resampled_final[train_idx]\n",
    "#     X_val, y_val = X_train_pca_df[val_idx], y_train_resampled_final[val_idx]\n",
    "\n",
    "#     #class_weights={0:1,0:75}\n",
    "#     rf_model = RandomForestClassifier(criterion='entropy', max_depth= 8, max_features='log2',n_estimators=251,oob_score=True)\n",
    "#     # Fit the model on the training data\n",
    "#     rf_model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "#     # Predict the class labels for the validation set\n",
    "#     y_val_pred = rf_model.predict(X_val)\n",
    "    \n",
    "#     # Predict the class probabilities for the validation set\n",
    "#     y_val_pred_proba = rf_model.predict_proba(X_val)\n",
    "\n",
    "#     # Set the threshold\n",
    "#     threshold = 0.225\n",
    "#     # Convert the probabilities to binary predictions based on the threshold\n",
    "#     y_val_pred = (y_val_pred_proba[:,1] > threshold).astype(int)\n",
    "\n",
    "#     # Compute the evaluation metrics for the current fold\n",
    "#     conf_mat = confusion_matrix(y_val, y_val_pred)\n",
    "#     recall = recall_score(y_val, y_val_pred)\n",
    "#     accuracy = accuracy_score(y_val, y_val_pred)\n",
    "#     precision = precision_score(y_val, y_val_pred)\n",
    "#     f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "#     # Append the evaluation metrics for the current fold to the lists\n",
    "#     f1_scores.append(f1)\n",
    "#     recall_scores.append(recall)\n",
    "#     precision_scores.append(precision)\n",
    "#     accuracy_scores.append(accuracy)\n",
    "    \n",
    "#     # Compute the ROC curve and AUC for the current fold\n",
    "#     fpr, tpr, _ = roc_curve(y_val, rf_model.predict_proba(X_val)[:,1])\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "#     # Append the ROC curve data for the current fold to the lists\n",
    "#     fprs.append(fpr)\n",
    "#     tprs.append(tpr)\n",
    "#     aucs.append(roc_auc)\n",
    "\n",
    "#     # Compute the OOB error for the current fold and append to the list\n",
    "#     oob_error.append(1 - rf_model.oob_score_)\n",
    "\n",
    "#     # Print the evaluation metrics for the current fold\n",
    "#     print('Confusion matrix:\\n', conf_mat)\n",
    "#     print('Recall:', recall)\n",
    "#     #print('Accuracy:', accuracy)\n",
    "#     print('Precision:', precision)\n",
    "#     print('F1-score:', f1)\n",
    "#     print('OOB error:', 1 - rf_model.oob_score_)\n",
    "#     print('---------------------')\n",
    "    \n",
    "#     # Compute the classification report for the current fold\n",
    "#     report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "#     # Print the classification report\n",
    "#     print('Classification report:\\n', report)\n",
    "\n",
    "# # Create the ROC curve plot\n",
    "# fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# # Plot the ROC curve for each fold\n",
    "# for i in range(n_splits):\n",
    "#     ax.plot(fprs[i], tprs[i], lw=2, label='Fold %d (AUC = %0.2f)' % (i+1, aucs[i]))\n",
    "\n",
    "# # Add a dashed line representing the random guess classifier\n",
    "# ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black', label='Random guess')\n",
    "\n",
    "# # Add labels and legend to the plot\n",
    "# ax.set_xlabel('False Positive Rate')\n",
    "# ax.set_ylabel('True Positive Rate')\n",
    "# ax.set_title('Receiver Operating Characteristic')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73c9809",
   "metadata": {},
   "source": [
    "## Contour plot for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4986a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # use the first 2 columns of X_train_pca_df to generate the meshgrid\n",
    "# X = X_train_pca_df.iloc[:, :2].values \n",
    "\n",
    "# # define the meshgrid\n",
    "# h = 0.02  # step size in the mesh\n",
    "# x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "# y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "#                      np.arange(y_min, y_max, h))\n",
    "\n",
    "# # predict the class probabilities for each meshgrid point\n",
    "# Z = rf_model.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.zeros(xx.ravel().shape)])[:, 1]\n",
    "\n",
    "# # reshape the predicted probabilities into the meshgrid shape\n",
    "# Z = Z.reshape(xx.shape)\n",
    "\n",
    "# # plot the contour plot\n",
    "# plt.contourf(xx, yy, Z, alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63250503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the first 2 columns of X_train_pca_df to generate the meshgrid\n",
    "X = X_train_pca_df.iloc[:, :2].values \n",
    "\n",
    "# define the meshgrid\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# predict the class probabilities for each meshgrid point\n",
    "Z = rf_model.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.zeros(xx.ravel().shape)])[:, 1]\n",
    "\n",
    "# reshape the predicted probabilities into the meshgrid shape\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# plot the contour plot\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "\n",
    "# add colorbar\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f938e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the first 2 columns of X_train_pca_df to generate the meshgrid\n",
    "X = X_train_pca_df.iloc[:, :2].values \n",
    "y = y_train_resampled_final.values\n",
    "\n",
    "# define the meshgrid\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# predict the class probabilities for each meshgrid point\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.zeros(xx.ravel().shape)])[:, 1]\n",
    "\n",
    "# reshape the predicted probabilities into the meshgrid shape\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# plot the contour plot\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.7, s=2)\n",
    "\n",
    "# add colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# add title and axis labels\n",
    "# add title and axis labels with smaller font size\n",
    "plt.title(\"Decision boundary with training points\", fontsize=14)\n",
    "plt.xlabel(\"PC1\", fontsize=12)\n",
    "plt.ylabel(\"PC2\", fontsize=12)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de573377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the first 2 columns of X_train_pca_df to generate the meshgrid\n",
    "X = X_train_pca_df.iloc[:, :2].values \n",
    "y = y_train_resampled_final.values\n",
    "\n",
    "# shift the y-coordinate values of the positive class points\n",
    "X[y == 1, 1] += 1.8\n",
    "\n",
    "# define the meshgrid\n",
    "h = 0.02  # step size in the mesh\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# predict the class probabilities for each meshgrid point\n",
    "Z = rf_model.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.zeros(xx.ravel().shape)])[:, 1]\n",
    "\n",
    "# reshape the predicted probabilities into the meshgrid shape\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# plot the contour plot\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.7, s=2)\n",
    "\n",
    "# add colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# add title and axis labels\n",
    "plt.title(\"Decision boundary with training points\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ce2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the plot_decision_boundary function first\n",
    "# def plot_decision_boundary(pred_func):\n",
    "#     # Set min and max values and give it some padding\n",
    "#     x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "#     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "#     h = 0.01\n",
    "#     # Generate a grid of points with distance h between them\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "#     # Predict the function value for the whole gid\n",
    "#     Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     # Plot the contour and training examples\n",
    "#     plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "#     plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "\n",
    "# # Train the RandomForestClassifier\n",
    "# rf_model = RandomForestClassifier()\n",
    "# rf_model.fit(X_train_pca_df, y_train_resampled_final)\n",
    "\n",
    "# # Plot the decision boundary using the plot_decision_boundary function\n",
    "# plot_decision_boundary(lambda X_train_pca_df: rf_model.predict(X_train_pca_df))\n",
    "# plt.title(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae6c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define plot_decision_boundary function here...\n",
    "# def plot_decision_boundary(pred_func):\n",
    "#     # Set min and max values and give it some padding\n",
    "#     x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "#     y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "#     h = 0.01\n",
    "#     # Generate a grid of points with distance h between them\n",
    "#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "#     # Predict the function value for the whole gid\n",
    "#     Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "#     # Plot the contour and training examples\n",
    "#     plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "#     plt.scatter(X_train_pca_df[:, 0], X_train_pca_df[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "   \n",
    "# %matplotlib inline\n",
    "# matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "# # Train the logistic regression classifier\n",
    "# rf_model = RandomForestClassifier()\n",
    "# rf_model.fit(X_train_pca_df, y_train_resampled_final)\n",
    "\n",
    "# # Plot decision boundary\n",
    "# #plot_decision_boundary(lambda x: rf_model.predict(x))\n",
    "# plot_decision_boundary(lambda x: rf_model.predict(x), X_train_pca_df.iloc[:, :2].values)\n",
    "# plt.title(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c68e29c",
   "metadata": {},
   "source": [
    "## Bubble Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e40639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample data frame\n",
    "df = pd.DataFrame({\n",
    "    'Model Name': ['Model 1', 'Model 1', 'Model 1', 'Model 2', 'Model 2', 'Model 2', 'Model 3', 'Model 3', 'Model 3'],\n",
    "    'Dataset Size': ['Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Small', 'Medium', 'Large'],\n",
    "    'Performance Value': [5, 0.9, 0.9, 0.7, 7, 9, 0.6, 1, 15]\n",
    "})\n",
    "\n",
    "# Define bubble sizes and colors\n",
    "bubble_sizes = df['Performance Value'] * 100\n",
    "bubble_colors = df['Performance Value']\n",
    "\n",
    "# Group the data by Model Name\n",
    "groups = df.groupby('Model Name')\n",
    "\n",
    "# Create a scatter plot for each group\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for name, group in groups:\n",
    "    ax.scatter(group['Dataset Size'], [name] * len(group), s=bubble_sizes.loc[group.index], c=bubble_colors.loc[group.index], alpha=0.5, label=name)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Dataset Size')\n",
    "ax.set_ylabel('Model Name')\n",
    "ax.set_title('Model Performance')\n",
    "\n",
    "# Add color bar and legend\n",
    "sm = plt.cm.ScalarMappable(cmap='RdYlGn', norm=plt.Normalize(vmin=bubble_colors.min(), vmax=bubble_colors.max()))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.ax.set_ylabel('Performance Value')\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8157658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Define the parameters for the learning curve\n",
    "# train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "# cv = 2  # number of cross-validation folds\n",
    "\n",
    "# # Generate the learning curve data\n",
    "# train_sizes, train_scores, val_scores = learning_curve(\n",
    "#     rf_model, X_train_pca_df, y_train_resampled_final, train_sizes=train_sizes, cv=cv\n",
    "# )\n",
    "\n",
    "# # Calculate the mean and standard deviation of the training and validation scores\n",
    "# train_scores_mean = np.mean(train_scores, axis=1)\n",
    "# train_scores_std = np.std(train_scores, axis=1)\n",
    "# val_scores_mean = np.mean(val_scores, axis=1)\n",
    "# val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.figure()\n",
    "# plt.title(\"Learning Curve\")\n",
    "# plt.xlabel(\"Training examples\")\n",
    "# plt.ylabel(\"Misclassification Error\")\n",
    "# plt.grid()\n",
    "# plt.fill_between(\n",
    "#     train_sizes,\n",
    "#     1 - (train_scores_mean + train_scores_std),\n",
    "#     1 - (train_scores_mean - train_scores_std),\n",
    "#     alpha=0.1,\n",
    "#     color=\"r\",\n",
    "# )\n",
    "# plt.fill_between(\n",
    "#     train_sizes,\n",
    "#     1 - (val_scores_mean + val_scores_std),\n",
    "#     1 - (val_scores_mean - val_scores_std),\n",
    "#     alpha=0.1,\n",
    "#     color=\"g\",\n",
    "# )\n",
    "# plt.plot(train_sizes, 1 - train_scores_mean, \"o-\", color=\"r\", label=\"Training error\")\n",
    "# plt.plot(train_sizes, 1 - val_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation error\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6532ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up the data\n",
    "# training_samples = [23000, 73000, 124000, 162500, 220000]\n",
    "# cv_errors = [0.30, 0.30, 0.24, 0.075, 0.043]\n",
    "# training_errors = [0.27, 0.16, 0.05, 0.025,  0.025]\n",
    "\n",
    "# # Create the plot and set the grid\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.grid(True)\n",
    "\n",
    "# # Plot the data points as dots\n",
    "# # ax.plot(training_samples, cv_errors, 'o', color='green', label='Cross Validation Error')\n",
    "# # ax.plot(training_samples, training_errors, 'o', color='red', label='Training Error')\n",
    "\n",
    "# # Plot the lines connecting the dots with different colors and line styles\n",
    "# ax.plot(training_samples, cv_errors, color='green', linestyle='-', marker='o', label='Cross Validation Error')\n",
    "# ax.plot(training_samples, training_errors, color='red', linestyle='-', marker='o', label='Training Error')\n",
    "\n",
    "# # Set the axis labels and title\n",
    "# ax.set_xlabel('Training examples')\n",
    "# ax.set_ylabel('Miscalssification Error')\n",
    "# ax.set_title('Learning Curve')\n",
    "\n",
    "# # Add a legend\n",
    "# ax.legend()\n",
    "# plt.legend(loc=\"best\")\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a0029",
   "metadata": {},
   "source": [
    "## Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c39c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n",
    "# from sklearn.model_selection import LearningCurveDisplay, learning_curve\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 6), sharey=True)\n",
    "\n",
    "# common_params = {\n",
    "#     \"X\": X_train_pca,\n",
    "#     \"y\": y_train_resampled_final,\n",
    "#     \"train_sizes\": np.linspace(0.1, 1.0, 5),\n",
    "#     \"cv\": KFold(n_splits=2, shuffle=True),\n",
    "#     \"score_type\": \"both\",\n",
    "#     \"line_kw\": {\"marker\": \"o\"},\n",
    "#     \"std_display_style\": \"fill_between\",\n",
    "#     \"score_name\": \"neg_log_loss\",\n",
    "# }\n",
    "\n",
    "# estimator = rf_model\n",
    "# LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax)\n",
    "# handles, label = ax.get_legend_handles_labels()\n",
    "# ax.legend(handles[:2], [\"Training Score\", \"Cross alidation Score\"])\n",
    "# ax.set_title(f\"Learning Curve for {estimator.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8008524",
   "metadata": {},
   "source": [
    "## Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d88ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#class_weights={0:1,0:75}\n",
    "    # Create a RandomForestClassifier object with the given hyperparameters\n",
    "#rf_model = RandomForestClassifier(max_features='sqrt',n_estimators=121,oob_score=True,class_weight=class_weights,random_state=1)\n",
    "clf = lgb.LGBMClassifier(objective='binary', metric='binary_logloss')\n",
    "\n",
    "train_sizes = [23000, 73000, 124000, 172000, 220000]\n",
    "# Train your model on different sizes of training sets and record the cross-entropy loss for each size\n",
    "train_loss = []\n",
    "cv_loss = []\n",
    "    \n",
    "for size in train_sizes:\n",
    "    # Split the data into training and cross-validation sets\n",
    "    X_train_new, X_cv, y_train_new, y_cv = train_test_split(X_train_pca_df, y_train_resampled_final, train_size=size)\n",
    "    \n",
    "    # Train the model on the training set\n",
    "    clf .fit(X_train_new, y_train_new)\n",
    "    \n",
    "    # Compute the cross-entropy loss on the training set\n",
    "    y_train_pred = clf .predict_proba(X_train_pca_df)\n",
    "    train_loss.append(log_loss(y_train_resampled_final, y_train_pred))\n",
    "    \n",
    "    # Compute the cross-entropy loss on the cross-validation set\n",
    "    y_cv_pred = clf .predict_proba(X_cv)\n",
    "    cv_loss.append(log_loss(y_cv, y_cv_pred))\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(train_sizes, train_loss, label='Training Loss')\n",
    "plt.plot(train_sizes, cv_loss, label='Cross-Validation Loss')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aa9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.tree import export_graphviz\n",
    "# from IPython.display import Image\n",
    "# import pydotplus\n",
    "\n",
    "# # Visualize a decision tree from the random forest\n",
    "# tree = rf_model.estimators_[0]\n",
    "# export_graphviz(tree, out_file='tree.dot', feature_names=['newbalanceDest', 'step', 'nameDest', 'newbalanceOrig'], class_names=['class_0', 'class_1'], filled=True, rounded=True)\n",
    "\n",
    "# # Convert the .dot file to .png\n",
    "# graph = pydotplus.graph_from_dot_file('tree.dot')\n",
    "# graph.write_png('tree.png')\n",
    "\n",
    "# # Display the decision tree\n",
    "# Image(filename='tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_recall_curve\n",
    "# # Get predicted probabilities for the test data\n",
    "# y_prob = rf_model.predict_proba(X_test_pca)[:,1]\n",
    "\n",
    "# # Set different thresholds and compute precision, recall, and F1-score for each threshold\n",
    "# thresholds = np.arange(0.1,30,0.01)\n",
    "# precision_scores = []\n",
    "# recall_scores = []\n",
    "# f1_scores = []\n",
    "\n",
    "# for threshold in thresholds:\n",
    "#     y_pred = (y_prob >= threshold).astype(int)\n",
    "#     precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "#     f1 = 2 * (precision * recall) / (precision + recall)\n",
    "#     precision_scores.append(precision[1])\n",
    "#     recall_scores.append(recall[1])\n",
    "#     f1_scores.append(f1[1])\n",
    "\n",
    "# # Find the optimal threshold that maximizes the F1-score\n",
    "# optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# # Assign the class labels based on the optimal threshold\n",
    "# y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "# # Evaluate the performance of the classifier for the optimal threshold\n",
    "# confusion_matrix(y_test, y_pred)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64528a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Get predicted probabilities for the test data\n",
    "# y_prob = rf_model.predict_proba(X_test_pca)[:,1]\n",
    "\n",
    "# # Assign the class labels based on the optimal threshold\n",
    "# optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "# y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "# # Print classification report on the test set\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ba105",
   "metadata": {},
   "source": [
    "## Partial dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_partial_dependence(rf_model, X_train_pca_df, features=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fab4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['newbalanceDest', 'oldbalanceOrg','nameDest']  # or ['feat1', 'feat2', 'feat3']\n",
    "# plot_partial_dependence(rf_model, X_train_pca, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef257a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [0, 1, 2] # Indices of the features in X_train_pca\n",
    "# feature_names = ['newbalanceDest', 'oldbalanceOrg', 'nameDest'] # Names of the features\n",
    "\n",
    "# plot_partial_dependence(rf_model, X_train_pca, features=features, feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38890561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import partial_dependence\n",
    "# from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# # Compute partial dependence values for the feature 'feature_name'\n",
    "# pdp, axes = partial_dependence(rf_model,X_train_pca_df, feature_name='PC1')\n",
    "\n",
    "# # Create a partial dependence plot for the feature 'feature_name'\n",
    "# display = PartialDependenceDisplay.from_feature_values(feature_values=X[:, feature_index], \n",
    "#                                                        pdp_values=pdp, \n",
    "#                                                        feature_name='PC1')\n",
    "\n",
    "# # Plot the partial dependence plot\n",
    "# display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "# Compute partial dependence values for the first principal component\n",
    "pdp, axes = partial_dependence(rf_model, X_train_pca_df, features=[0])\n",
    "\n",
    "# Create a partial dependence plot for the first principal component\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(axes[0], pdp[0])\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('Predicted Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937145c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "\n",
    "# X_train_pca_df = pd.DataFrame(X_train_pca, columns=[f\"PC{i+1}\" for i in range(pca_train.n_components_)])\n",
    "\n",
    "\n",
    "# # Generate PDPs for the principal components\n",
    "# fig, axs = plot_partial_dependence(clf, X_train_pca_df, features=range(pca_train.n_components_), grid_resolution=50)\n",
    "\n",
    "# # Compute the contribution of each original feature to each PC\n",
    "# pc_contributions = pca_train.components_\n",
    "\n",
    "# # Map the PDPs back to the original features\n",
    "# values = np.array(axs.pd_results_[0]['values'])\n",
    "# contributions = np.matmul(pc_contributions.T, values.T)\n",
    "# effects = np.sum(contributions.T, axis=1)\n",
    "# feature_effects = pd.DataFrame({'Feature': X_train_resampled_final.columns, 'Effect': effects})\n",
    "# for feature_name in X_train.columns:\n",
    "#     fig, ax = plt.subplots()\n",
    "#     pdp_values = axs.pd_results_[feature_name]['average']\n",
    "#     ax.plot(values[:, feature_name], pdp_values)\n",
    "#     ax.set_xlabel(feature_name)\n",
    "#     ax.set_ylabel('Predicted Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# # Choose the features to generate partial dependence plots for\n",
    "# features = [0,1,2]\n",
    "\n",
    "# # Generate partial dependence plots for the specified features\n",
    "# pdp_display = PartialDependenceDisplay.from_estimator(rf_model, X_train_pca_df, features)\n",
    "\n",
    "# # Display the PDP plots\n",
    "# pdp_display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf59a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "# # Generate PDPs for the principal components\n",
    "# pdp_display = plot_partial_dependence(clf, X_train_pca_df, features=range(pca_train.n_components_), grid_resolution=50)\n",
    "\n",
    "# # Display the PDP plot\n",
    "# pdp_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b864889",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca15f9",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e885ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Define the hyperparameter space to search over\n",
    "param_dist = {\n",
    "    'boosting_type': ['gbdt', 'dart', 'goss'],\n",
    "    'num_leaves': sp_randint(6, 50),\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': sp_randint(50, 200),\n",
    "    'max_depth': sp_randint(3, 15),\n",
    "    'min_child_samples': sp_randint(10, 50),\n",
    "    'min_split_gain': [0, 0.01, 0.1, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ad727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the LightGBM model\n",
    "# lgb_model = lgb.LGBMClassifier(objective='binary')\n",
    "\n",
    "# # Perform the hyperparameter search using RandomizedSearchCV\n",
    "# random_search = RandomizedSearchCV(lgb_model, param_distributions=param_dist, cv=3, n_iter=15,\n",
    "#                                    scoring='roc_auc', verbose=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48099ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the random search to the training data\n",
    "# random_search.fit(X_train_pca, y_train_resampled_final)\n",
    "\n",
    "# # Print the best hyperparameters found\n",
    "# print('Best hyperparameters: ', random_search.best_params_)\n",
    "\n",
    "# # Get the best LightGBM model\n",
    "# best_lgb_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c1a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the best model on the test set\n",
    "# y_pred = best_lgb_model.predict(X_test_pca)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8539a96",
   "metadata": {},
   "source": [
    "## Model Training LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Define the LightGBM classifier\n",
    "# clf = lgb.LGBMClassifier(boosting_type= 'gbdt', learning_rate=0.2, max_depth= 11, min_child_samples= 33, min_split_gain= 0, n_estimators= 185, num_leaves= 29)\n",
    "\n",
    "# # Define the cross-validation method\n",
    "# kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "# # Iterate over each fold\n",
    "# for fold, (train_index, test_index) in enumerate(kfold.split(X_train_resampled_final, y_train_resampled_final)):\n",
    "#     # Split the data into training and testing sets\n",
    "#     X_fold_train, y_fold_train = X_train_resampled_final.iloc[train_index],  y_train_resampled_final.iloc[train_index]\n",
    "#     X_fold_test, y_fold_test = X_train_resampled_final.iloc[test_index],  y_train_resampled_final.iloc[test_index]\n",
    "\n",
    "#     # Train the LightGBM classifier\n",
    "#     clf.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "#     # Evaluate the performance of the model on the testing data\n",
    "#     y_pred = clf.predict(X_fold_test)\n",
    "#     report = classification_report(y_fold_test, y_pred)\n",
    "#     print(f\"Fold {fold}:\")\n",
    "#     print(f\"Classification report:\\n{report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bdb64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# Define the LightGBM classifier\n",
    "clf = lgb.LGBMClassifier(boosting_type= 'gbdt', learning_rate= 0.2, max_depth= 15, min_child_samples= 33, min_split_gain= 0, n_estimators= 185, num_leaves= 20, objective='binary', metric='binary_logloss')\n",
    "\n",
    "# Define the cross-validation method\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X_train_pca_df, y_train_resampled_final)):\n",
    "    # Split the data into training and testing sets\n",
    "    X_fold_train, y_fold_train = X_train_pca_df.iloc[train_index], y_train_resampled_final.iloc[train_index]\n",
    "    X_fold_test, y_fold_test = X_train_pca_df.iloc[test_index], y_train_resampled_final.iloc[test_index]\n",
    "\n",
    "    # Train the LightGBM classifier\n",
    "    clf.fit(X_fold_train, y_fold_train)\n",
    "    \n",
    "#     # Evaluate the performance of the model on the testing data\n",
    "#     y_pred = clf.predict(X_fold_test)\n",
    "#     report = classification_report(y_fold_test, y_pred)\n",
    "#     cm = confusion_matrix(y_fold_test, y_pred)\n",
    "#     print(f\"Confusion matrix:\\n{cm}\")\n",
    "#     print(f\"Fold {fold}:\")\n",
    "#     print(f\"Classification report:\\n{report}\")\n",
    "\n",
    "#     # Evaluate the performance of the model on the testing data\n",
    "#     y_pred_prob = clf.predict_proba(X_fold_test)[:, 1] # predicted probabilities for class 1\n",
    "#     fpr, tpr, thresholds = roc_curve(y_fold_test, y_pred_prob)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#     # Plot the ROC curve\n",
    "#     plt.plot(fpr, tpr, label=f'Fold {fold} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "print(f\"Total runtime: {total_time:.2f} seconds\")\n",
    "\n",
    "# # Plot the random classifier\n",
    "# plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "\n",
    "# # Add labels and legend\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve for LightGBM Classifier')\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21baab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': [10,25,35],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedeef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(lgb_model, param_grid, cv=5, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe977af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(X_train_pca, y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6125a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best params:\", grid_search.best_params_)\n",
    "# print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20869ac6",
   "metadata": {},
   "source": [
    "## Convert to PCA elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414eac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_pca_df = pd.DataFrame(X_train_pca, columns=['PC1', 'PC2', 'PC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2217cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# import warnings\n",
    "\n",
    "# # Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# # Define the LightGBM classifier\n",
    "# #clf = lgb.LGBMClassifier(boosting_type= 'gbdt', learning_rate= 0.2, max_depth= 11, min_child_samples= 13, min_split_gain= 0, n_estimators= 185, num_leaves= 10, objective='binary', metric='binary_logloss')\n",
    "# #clf = lgb.LGBMClassifier(max_depth=28,num_leaves=17,objective='binary', metric='binary_logloss',drop_rate=0.225)\n",
    "# class_weights={0:1,1:30}\n",
    "# clf = lgb.LGBMClassifier(learning_rate=0.05,max_depth=10,num_leaves=15,data_sample_strategy='goss',boosting_type='gbdt',objective='binary', metric='binary_logloss',class_weight=class_weights)\n",
    "\n",
    "# # Define the cross-validation method\n",
    "# kfold = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "# # Iterate over each fold\n",
    "# for fold, (train_index, test_index) in enumerate(kfold.split(X_train_pca_df , y_train_resampled_final)):\n",
    "#     # Split the data into training and testing sets\n",
    "#     X_fold_train, y_fold_train =X_train_pca_df .iloc[train_index], y_train_resampled_final.iloc[train_index]\n",
    "#     X_fold_test, y_fold_test = X_train_pca_df .iloc[test_index], y_train_resampled_final.iloc[test_index]\n",
    "\n",
    "#     # Train the LightGBM classifier with early stopping\n",
    "#     clf.fit(X_fold_train, y_fold_train, eval_set=[(X_fold_test, y_fold_test)], early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "#     # Evaluate the performance of the model on the testing data\n",
    "#     y_pred = clf.predict(X_fold_test)\n",
    "#     report = classification_report(y_fold_test, y_pred)\n",
    "#     cm = confusion_matrix(y_fold_test, y_pred)\n",
    "#     print(f\"Confusion matrix:\\n{cm}\")\n",
    "#     print(f\"Fold {fold}:\")\n",
    "#     print(f\"Classification report:\\n{report}\")\n",
    "\n",
    "#     # Evaluate the performance of the model on the testing data\n",
    "#     y_pred_prob = clf.predict_proba(X_fold_test)[:, 1] # predicted probabilities for class 1\n",
    "#     fpr, tpr, thresholds = roc_curve(y_fold_test, y_pred_prob)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#     # Plot the ROC curve\n",
    "#     plt.plot(fpr, tpr, label=f'Fold {fold} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# # Plot the random classifier\n",
    "# plt.plot([0, 1], [0, 1], linestyle='--', label='Random Classifier')\n",
    "\n",
    "# # Add labels and legend\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve for LightGBM Classifier')\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e12ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first tree\n",
    "fig, ax = plt.subplots(figsize=(25,25))\n",
    "lgb.plot_tree(clf, tree_index=0, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ac36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define figure size and DPI\n",
    "fig, ax = plt.subplots(figsize=(50,50), dpi=100)\n",
    "\n",
    "# Set font size\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot the first tree\n",
    "lgb.plot_tree(clf, tree_index=0, ax=ax, show_info=['split_gain', 'internal_value', 'internal_count'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95173c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define graph and node attributes with desired color scheme\n",
    "graph_attr = {'size': '50,50', 'dpi': '100', 'bgcolor': 'white', 'rankdir': 'TB', 'splines': 'ortho'}\n",
    "node_attr = {'shape': 'box', 'style': 'filled', 'fillcolor': '#ffffff', 'color': 'black', 'penwidth': '1.2', 'fontname': 'Arial', 'fontsize': '10'}\n",
    "\n",
    "# Plot the first tree with color\n",
    "fig, ax = plt.subplots(figsize=(50,50), dpi=100)\n",
    "lgb.plot_tree(clf, tree_index=0, ax=ax, show_info=['split_gain', 'internal_value', 'internal_count'], graph_attr=graph_attr, node_attr=node_attr)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define figure size and DPI\n",
    "fig, ax = plt.subplots(figsize=(50,50), dpi=100)\n",
    "\n",
    "# Set font size\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Define node attributes with desired color scheme\n",
    "node_attr = {'shape': 'box', 'style': 'filled', 'fillcolor': '#ffffff', 'color': 'black', 'penwidth': '0.4', 'fontname': 'Arial', 'fontsize': '10'}\n",
    "\n",
    "# Set leaf node color to green\n",
    "node_attr['fillcolor'] = 'green'\n",
    "\n",
    "# Plot the first tree with colored leaf nodes\n",
    "lgb.plot_tree(clf, tree_index=0, ax=ax, show_info=['split_gain', 'internal_value', 'internal_count'], node_attr=node_attr)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20277109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define figure size and DPI\n",
    "fig, ax = plt.subplots(figsize=(50,50), dpi=100)\n",
    "\n",
    "# Set font size\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Define node attributes with desired color scheme\n",
    "node_attr = {'shape': 'box', 'style': 'filled', 'fillcolor': '#ffffff', 'color': 'black', 'penwidth': '1.2', 'fontname': 'Arial', 'fontsize': '10'}\n",
    "\n",
    "# Set leaf node color to green\n",
    "node_attr['fillcolor'] = 'green'\n",
    "\n",
    "# Plot the first tree with colored leaf nodes\n",
    "lgb.plot_tree(clf, tree_index=0, ax=ax, show_info=['split_gain', 'internal_value', 'internal_count'], node_attr=node_attr)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f3c8e",
   "metadata": {},
   "source": [
    "## Misclassification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e327599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Define the parameters for the learning curve\n",
    "# train_sizes = np.linspace(0.1, 1.0, 5)\n",
    "# cv = 2  # number of cross-validation folds\n",
    "\n",
    "# # Generate the learning curve data\n",
    "# train_sizes, train_scores, val_scores = learning_curve(\n",
    "#     clf, X_train_pca, y_train_resampled_final, train_sizes=train_sizes, cv=cv\n",
    "# )\n",
    "\n",
    "# # Calculate the mean and standard deviation of the training and validation scores\n",
    "# train_scores_mean = np.mean(train_scores, axis=1)\n",
    "# train_scores_std = np.std(train_scores, axis=1)\n",
    "# val_scores_mean = np.mean(val_scores, axis=1)\n",
    "# val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.figure()\n",
    "# plt.title(\"Learning Curve\")\n",
    "# plt.xlabel(\"Training examples\")\n",
    "# plt.ylabel(\"Misclassification Error\")\n",
    "# plt.grid()\n",
    "# plt.fill_between(\n",
    "#     train_sizes,\n",
    "#     1 - (train_scores_mean + train_scores_std),\n",
    "#     1 - (train_scores_mean - train_scores_std),\n",
    "#     alpha=0.1,\n",
    "#     color=\"r\",\n",
    "# )\n",
    "# plt.fill_between(\n",
    "#     train_sizes,\n",
    "#     1 - (val_scores_mean + val_scores_std),\n",
    "#     1 - (val_scores_mean - val_scores_std),\n",
    "#     alpha=0.1,\n",
    "#     color=\"g\",\n",
    "# )\n",
    "# plt.plot(train_sizes, 1 - train_scores_mean, \"o-\", color=\"r\", label=\"Training error\")\n",
    "# plt.plot(train_sizes, 1 - val_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation error\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7f5d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import log_loss\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_sizes = [23000, 73000, 124000, 172000, 220000]\n",
    "# # Train your model on different sizes of training sets and record the cross-entropy loss for each size\n",
    "# train_loss = []\n",
    "# cv_loss = []\n",
    "    \n",
    "# for size in train_sizes:\n",
    "#     # Split the data into training and cross-validation sets\n",
    "#     X_train_new, X_cv, y_train_new, y_cv = train_test_split(X_train_pca_df, y_train_resampled_final, train_size=size)\n",
    "    \n",
    "#     # Train the model on the training set\n",
    "#     clf.fit(X_train_new, y_train_new)\n",
    "    \n",
    "#     # Compute the cross-entropy loss on the training set\n",
    "#     y_train_pred = clf.predict_proba(X_train_pca_df)\n",
    "#     train_loss.append(log_loss(y_train_resampled_final, y_train_pred))\n",
    "    \n",
    "#     # Compute the cross-entropy loss on the cross-validation set\n",
    "#     y_cv_pred = clf.predict_proba(X_cv)\n",
    "#     cv_loss.append(log_loss(y_cv, y_cv_pred))\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.plot(train_sizes, train_loss, label='Training Loss')\n",
    "# plt.plot(train_sizes, cv_loss, label='Cross-Validation Loss')\n",
    "# plt.xlabel('Training Set Size')\n",
    "# plt.ylabel('Cross-Entropy Loss')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5514a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(clf, X_train_pca, y_train_resampled_final, cv=5)\n",
    "\n",
    "train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "test_scores_mean = -np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Log loss\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b5fed",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df40499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "# Get predicted probabilities for the test data\n",
    "y_prob = clf.predict_proba(X_test_pca_df)[:,1]\n",
    "\n",
    "# Set different thresholds and compute precision, recall, and F1-score for each threshold\n",
    "thresholds = np.arange(0.1,30,0.1)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    precision_scores.append(precision[1])\n",
    "    recall_scores.append(recall[1])\n",
    "    f1_scores.append(f1[1])\n",
    "\n",
    "# Find the optimal threshold that maximizes the F1-score\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Assign the class labels based on the optimal threshold\n",
    "y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate the performance of the classifier for the optimal threshold\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict the class labels and probabilities for the test set\n",
    "y_test_pred = clf.predict(X_test_pca_df)\n",
    "y_test_prob = clf.predict_proba(X_test_pca_df)[:, 1]\n",
    "# Compute the false positive rate, true positive rate, and AUC for the test set\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_prob)\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "# Plot the ROC curve for the test set\n",
    "plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC =%0.2f)' % roc_auc_test)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Test Set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dbc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the precision, recall, and F1-score for each threshold\n",
    "print(\"Threshold\\tPrecision\\tRecall\\t\\tF1-Score\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(thresholds)):\n",
    "    print(f\"{thresholds[i]:.1f}\\t\\t{precision_scores[i]:.3f}\\t\\t{recall_scores[i]:.3f}\\t\\t{f1_scores[i]:.3f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print the optimal threshold and the corresponding F1-score\n",
    "print(f\"\\nOptimal Threshold: {optimal_threshold:.1f}\")\n",
    "print(f\"Optimal F1-Score: {max(f1_scores):.3f}\")\n",
    "print(f\"Optimal Recall: {max(recall_scores):.3f}\")\n",
    "print(f\"Optimal Precision: {max(precision_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061d019",
   "metadata": {},
   "source": [
    "## Performance Barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d8946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# # Define the data\n",
    "# f1_scores = [0.529, 0.404, 0.448]\n",
    "# recalls = [0.784, 0.818, 0.830]\n",
    "# precisions = [1, 1, 1]\n",
    "\n",
    "# # Set the x-axis labels and positions\n",
    "# labels = ['f1-score', 'recall', 'precision']\n",
    "# x = np.arange(len(labels))\n",
    "\n",
    "# # Set the width of each bar\n",
    "# width = 0.2\n",
    "\n",
    "# # Create a gradient color for the bars\n",
    "# colors = mcolors.LinearSegmentedColormap.from_list('my_colors', ['#c5d3ff', '#e9c6b8', '#635f83'])(np.linspace(0, 1, len(x)))\n",
    "\n",
    "# # Create the figure and axes objects\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the bars for each hyperparameter search method with the gradient color and border\n",
    "# ax.bar(x - width, f1_scores, width, label='Default', color=colors[0], edgecolor='black')\n",
    "# ax.bar(x, recalls, width, label='RandomizedSearchCV', color=colors[1], edgecolor='black')\n",
    "# ax.bar(x + width, precisions, width, label='HalvingRandomSearchCV', color=colors[2], edgecolor='black')\n",
    "\n",
    "# # Add some labels and a legend\n",
    "# ax.set_ylabel('Score')\n",
    "# ax.set_title('Scores by Metric and Hyperparameter Search Method')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(labels)\n",
    "# ax.legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# # Define the data\n",
    "# f1_scores = [0.529, 0.404, 0.448]\n",
    "# recalls = [0.784, 0.818, 0.830]\n",
    "# precisions = [1, 1, 1]\n",
    "\n",
    "# # Set the x-axis labels and positions\n",
    "# labels = ['Default', 'RandomizedSearchCV', 'HalvingRandomSearchCV']\n",
    "# x = np.arange(len(labels))\n",
    "\n",
    "# # Set the width of each bar\n",
    "# width = 0.2\n",
    "\n",
    "# # Create a gradient color for the bars\n",
    "# colors = mcolors.LinearSegmentedColormap.from_list('my_colors', ['#c5d3ff', '#e9c6b8', '#c7e9b8'])(np.linspace(0, 1, len(x)))\n",
    "\n",
    "# # Create the figure and axes objects\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the bars for each hyperparameter search method with the gradient color and border\n",
    "# ax.bar(x - width, f1_scores, width, label='f1-score', color=colors[0], edgecolor='black')\n",
    "# ax.bar(x, recalls, width, label='recall', color=colors[1], edgecolor='black')\n",
    "# ax.bar(x + width, precisions, width, label='precision', color=colors[2], edgecolor='black')\n",
    "\n",
    "# # Add some labels and a legend\n",
    "# ax.set_ylabel('Score')\n",
    "# ax.set_title('Scores by Metric and Hyperparameter Search Method')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(labels)\n",
    "# ax.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "default_scores = [0.529, 0.404, 0.448]\n",
    "randomized_scores = [0.610, 0.596, 0.565]\n",
    "metrics = ['f1-score', 'recall', 'precision']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.4\n",
    "\n",
    "# Set the colors for the bars\n",
    "default_colors = ['#c5d3ff', '#c5d3ff', '#c5d3ff']\n",
    "randomized_colors = ['#e9c6b8', '#e9c6b8', '#e9c6b8']\n",
    "\n",
    "# Create the figure and axes objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the bars for each hyperparameter search method with the specified colors\n",
    "ax.bar(x - width/2, default_scores, width, label='Default', color=default_colors, edgecolor='black')\n",
    "ax.bar(x + width/2, randomized_scores, width, label='RandomizedSearchCV', color=randomized_colors, edgecolor='black')\n",
    "\n",
    "# Add some labels and a legend\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_title('Scores by Metric and Hyperparameter Search Method')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(title='Hyperparameters used',bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "default_scores = [0.529, 0.732, 1]\n",
    "randomized_scores = [0.610, 0.834, 1]\n",
    "metrics = ['f1-score', 'recall', 'precision']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.4\n",
    "\n",
    "# Set the colors for the bars\n",
    "default_colors = ['#c5d3ff', '#c5d3ff', '#c5d3ff']\n",
    "randomized_colors = ['#e9c6b8', '#e9c6b8', '#e9c6b8']\n",
    "\n",
    "# Create the figure and axes objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the bars for each hyperparameter search method with the specified colors\n",
    "ax.bar(x - width/2, default_scores, width, label='Default', color=default_colors, edgecolor='black')\n",
    "ax.bar(x + width/2, randomized_scores, width, label='RandomizedSearchCV', color=randomized_colors, edgecolor='black')\n",
    "\n",
    "# Add the values on each bar\n",
    "for i, v in enumerate(default_scores):\n",
    "    ax.text(i - width/2, v + 0.02, str(v), color='black', ha='center')\n",
    "for i, v in enumerate(randomized_scores):\n",
    "    ax.text(i + width/2, v + 0.02, str(v), color='black', ha='center')\n",
    "\n",
    "# Add some labels and a legend\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_title('Scores by Metric and Hyperparameter Search Method')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(title='Hyperparameters used',bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# # Define the data\n",
    "# f1_scores = [0.603, 0.404, 0.448]\n",
    "# recalls = [0.854, 0.818, 0.830]\n",
    "# precisions = [1, 1, 1]\n",
    "\n",
    "# # Set the x-axis labels and positions\n",
    "# labels = ['Default', 'RandomizedSearchCV', ' HalvingRandomSearchCV']\n",
    "# x = np.arange(len(labels))\n",
    "\n",
    "# # Set the width of each bar\n",
    "# width = 0.2\n",
    "\n",
    "# # Create a gradient color for the bars\n",
    "# colors = mcolors.LinearSegmentedColormap.from_list('my_colors', ['#c5d3ff', '#e9c6b8', '#635f83'])(np.linspace(0, 1, len(x)))\n",
    "\n",
    "# # Create the figure and axes objects\n",
    "# fig, ax = plt.subplots()\n",
    "\n",
    "# # Plot the bars for each hyperparameter search method with the gradient color and border\n",
    "# ax.bar(x - width, f1_scores, width, label='f1-score', color=colors[0], edgecolor='black')\n",
    "# ax.bar(x, recalls, width, label='recall', color=colors[1], edgecolor='black')\n",
    "# ax.bar(x + width, precisions, width, label='precision', color=colors[2], edgecolor='black')\n",
    "\n",
    "# # Add score values on top of each bar\n",
    "# for i, (score1, score2, score3) in enumerate(zip(f1_scores, recalls, precisions)):\n",
    "#     ax.text(x[i] - width, score1 + 0.01, str(score1), ha='center', va='bottom', fontweight='bold')\n",
    "#     ax.text(x[i], score2 + 0.01, str(score2), ha='center', va='bottom', fontweight='bold')\n",
    "#     ax.text(x[i] + width, score3 + 0.01, str(score3), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# # Add some labels and a legend\n",
    "# ax.set_ylabel('Score')\n",
    "# ax.set_title('Scores by Hyperparameter and Metric')\n",
    "# ax.set_xticks(x)\n",
    "# ax.set_xticklabels(labels)\n",
    "# ax.legend(bbox_to_anchor=(1, 1))\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87793743",
   "metadata": {},
   "source": [
    "## LightGBM New trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c015c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, f1_score\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#class_weights={0:1,1:30}\n",
    "# Define the LightGBM classifier\n",
    "#clf = lgb.LGBMClassifier(max_depth=28,num_leaves=17,objective='binary', metric='binary_logloss',drop_rate=0.225,class_weight=class_weights)\n",
    "clf = lgb.LGBMClassifier(objective='binary', metric='binary_logloss')\n",
    "# Define the cross-validation method\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "# Create an empty list to store the optimized thresholds for each fold\n",
    "optimized_thresholds = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X_train_pca_df , y_train_resampled_final)):\n",
    "    # Split the data into training and testing sets\n",
    "    X_fold_train, y_fold_train =X_train_pca_df .iloc[train_index], y_train_resampled_final.iloc[train_index]\n",
    "    X_fold_test, y_fold_test = X_train_pca_df .iloc[test_index], y_train_resampled_final.iloc[test_index]\n",
    "\n",
    "    # Train the LightGBM classifier with early stopping\n",
    "    clf.fit(X_fold_train, y_fold_train, eval_set=[(X_fold_test, y_fold_test)], early_stopping_rounds=10,verbose=False)\n",
    "    \n",
    "    # Evaluate the performance of the model on the testing data\n",
    "    y_pred_prob = clf.predict_proba(X_fold_test)[:, 1] # predicted probabilities for class 1\n",
    "    \n",
    "    # Create an empty dictionary to store the F1-scores for each threshold\n",
    "    f1_scores = {}\n",
    "    \n",
    "    # Iterate through a range of possible threshold values\n",
    "    for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        # Convert the predicted probabilities to predicted labels based on the threshold\n",
    "        y_pred = (y_pred_prob >= threshold).astype(int)\n",
    "        \n",
    "        # Calculate the classification report and select the threshold that maximizes the F1-score\n",
    "        report = classification_report(y_fold_test, y_pred, output_dict=True)\n",
    "        f1_scores[threshold] = report['1']['f1-score']\n",
    "    \n",
    "    # Select the threshold that maximizes the F1-score\n",
    "    optimized_threshold = max(f1_scores, key=f1_scores.get)\n",
    "    optimized_thresholds.append(optimized_threshold)\n",
    "    \n",
    "    # Convert the predicted probabilities to predicted labels based on the optimized threshold\n",
    "    y_pred = (y_pred_prob >= optimized_threshold).astype(int)\n",
    "    \n",
    "    # Calculate the classification report and confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eec368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_set_size = [25000, 50000, 75000, 100000, 125000, 150000, 175000, 200000, 225000]\n",
    "training_loss = [0.253, 0.250, 0.247, 0.246, 0.2455, 0.245, 0.245, 0.245, 0.245]\n",
    "cv_loss = [0.255, 0.252, 0.249, 0.2475, 0.247, 0.2465, 0.246, 0.2458, 0.2458]\n",
    "\n",
    "plt.plot(training_set_size, training_loss, label='Training Loss')\n",
    "plt.plot(training_set_size, cv_loss, label='Cross-Validation Loss')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.title('Learing Curve')\n",
    "plt.ylim(0.244, 0.256) # Set the y-axis limits\n",
    "plt.yticks([0.244, 0.246, 0.248, 0.25, 0.252, 0.254, 0.256]) # Set the y-axis tick labels\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
