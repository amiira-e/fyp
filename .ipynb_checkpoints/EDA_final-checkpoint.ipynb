{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f374f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90396195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ac7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bda14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd852dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95128f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.55, random_state=0)\n",
    "\n",
    "# Fit and apply the resampler to the entire dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41793035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = df_sample.drop('isFraud', axis=1)\n",
    "# # Separate the target variable\n",
    "# y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, stratify=y_resampled, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y_test.value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea748898",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b220d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\X_train_resampled_final_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled_final.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\y_train_resampled_final_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\X_test_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\y_test_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec37ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y_test.value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac9f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train_resampled_final=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\X_train_resampled_final_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ee9273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(971198, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d6316d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set trimmed means:  {'amount': 116276.91571975141, 'oldbalanceOrg': 0.0, 'newbalanceOrig': 0.0, 'oldbalanceDest': 360.783857020446, 'newbalanceDest': 1147.4917990339254}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 200\n",
    "\n",
    "# Specify the trimming proportions for each column\n",
    "trim_props = {'amount': 0.1, 'oldbalanceOrg': (0.20, 0.14), 'newbalanceOrig': 0.1, 'oldbalanceDest': 0.5, 'newbalanceDest': 0.48}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Determine if this column requires asymmetric trimming or symmetric trimming\n",
    "        if isinstance(trim_props[col_name], tuple):\n",
    "            # If tuple, perform trimming on both ends of the distribution\n",
    "            lower_prop, upper_prop = trim_props[col_name]\n",
    "            lower_val = np.percentile(X_train_resampled_final[col_name], 100*lower_prop)\n",
    "            upper_val = np.percentile(X_train_resampled_final[col_name], 100*upper_prop)\n",
    "            train_sample = train_sample[(train_sample >= lower_val) & (train_sample <= upper_val)]\n",
    "        else:\n",
    "            # If not tuple, perform trimming on the upper end of the distribution\n",
    "            upper_val = np.percentile(X_train_resampled_final[col_name], 100*(1-trim_props[col_name]))\n",
    "            train_sample = train_sample[train_sample <= upper_val]\n",
    "            \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the trimmed mean of the bootstrapped sample for the training set\n",
    "        train_trimmed_mean = np.mean(train_sample)\n",
    "        train_trimmed_means_list.append(train_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    if isinstance(trim_props[col_name], tuple):\n",
    "        X_train_resampled_final.loc[X_train_resampled_final[col_name] < lower_val, col_name] = train_trimmed_means[col_name]\n",
    "        X_train_resampled_final.loc[X_train_resampled_final[col_name] > upper_val, col_name] = train_trimmed_means[col_name]\n",
    "    else:\n",
    "        X_train_resampled_final.loc[X_train_resampled_final[col_name] > upper_val, col_name] = train_trimmed_means[col_name]\n",
    "\n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b06292",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\X_train_resampled_final_AFTER.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c04ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Calculate the correlation matrix\n",
    "# corr_matrix = df_sample.corr()\n",
    "\n",
    "# # Set up the matplotlib figure\n",
    "# fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "# # Generate a custom diverging colormap\n",
    "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# # Create the heatmap\n",
    "# sns.heatmap(corr_matrix, cmap=cmap, annot=True, fmt=\".2f\", linewidths=.5, ax=ax)\n",
    "\n",
    "# # Set the chart title and axes labels\n",
    "# plt.xlabel('Variable')\n",
    "# plt.ylabel('Variable')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013ff00",
   "metadata": {},
   "source": [
    "### Check for outliers in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\X_train_resampled_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4076c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_1=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\X_train_resampled_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a322123e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031399bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "random.seed(0)\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.01, 'oldbalanceOrg': 0.07, 'newbalanceOrig': 0.015, 'oldbalanceDest': 0.015, 'newbalanceDest': 0.01}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train_resampled_final.loc[X_train_resampled_final[col_name] > np.percentile(X_train_resampled_final[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "    # Replace the outliers in the test set with the trimmed means obtained from the train set\n",
    "    test_outliers = X_test.loc[X_test[col_name] > np.percentile(X_test[col_name], 100*(1-trim_props[col_name])), col_name]\n",
    "    X_test.loc[test_outliers.index, col_name] = train_trimmed_means[col_name]\n",
    "    \n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0385cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b82113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the autoencoder model\n",
    "def create_autoencoder(encoding_dim, batch_size, learning_rate, patience, l1_reg):\n",
    "    input_layer = Input(shape=(10,))\n",
    "    hidden_layer = Dense(encoding_dim, activation='relu', activity_regularizer='l1', kernel_regularizer='l1')(input_layer)\n",
    "    output_layer = Dense(10, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=int(batch_size), validation_data=(X_test, X_test), callbacks=[EarlyStopping(monitor='val_loss', patience=int(patience), mode='min')])\n",
    "    reconstruction_loss = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    return -reconstruction_loss\n",
    "\n",
    "# Define the search space for the hyperparameters\n",
    "search_space = {\n",
    "    'encoding_dim': (10, 50),\n",
    "    'batch_size': (32, 128),\n",
    "    'learning_rate': (-5, -1),\n",
    "    'patience': (1, 10),\n",
    "    'l1_reg': (0, 0.1),\n",
    "}\n",
    "\n",
    "# Create a BayesianOptimization optimizer and optimize the function defined above\n",
    "optimizer = BayesianOptimization(\n",
    "    f=create_autoencoder,\n",
    "    pbounds=search_space,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "optimizer.maximize(n_iter=10, init_points=2)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_encoding_dim = int(optimizer.max['params']['encoding_dim'])\n",
    "best_batch_size = int(optimizer.max['params']['batch_size'])\n",
    "best_learning_rate = optimizer.max['params']['learning_rate']\n",
    "best_patience = int(optimizer.max['params']['patience'])\n",
    "best_l1_reg = optimizer.max['params']['l1_reg']\n",
    "\n",
    "# Train the autoencoder model using the best hyperparameters\n",
    "best_autoencoder = create_autoencoder(best_encoding_dim, best_batch_size, best_learning_rate, best_patience, best_l1_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa203a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "# Define the autoencoder model\n",
    "def create_autoencoder(encoding_dim, batch_size, learning_rate, patience, l1_reg):\n",
    "    input_layer = Input(shape=(10,))\n",
    "    hidden_layer = Dense(encoding_dim, activation='relu', activity_regularizer='l1', kernel_regularizer='l1')(input_layer)\n",
    "    output_layer = Dense(10, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "    for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "        X_train, X_val = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "        es = EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "        autoencoder.fit(X_train, X_train, epochs=5, batch_size=int(batch_size), validation_data=(X_val, X_val), callbacks=[es])\n",
    "        val_losses.append(autoencoder.evaluate(X_val, X_val))\n",
    "    return np.mean(val_losses)\n",
    "\n",
    "# Define the search space for the hyperparameters\n",
    "search_space = {\n",
    "    'encoding_dim': (10, 50),\n",
    "    'batch_size': (32, 128),\n",
    "    'learning_rate': (-5, -1),\n",
    "    'patience': (1, 10),\n",
    "    'l1_reg': (0, 0.1),\n",
    "}\n",
    "\n",
    "# Create a BayesianOptimization optimizer and optimize the function defined above\n",
    "optimizer = BayesianOptimization(\n",
    "    f=create_autoencoder,\n",
    "    pbounds=search_space,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "optimizer.maximize(n_iter=3, init_points=2)\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_encoding_dim = int(optimizer.max['params']['encoding_dim'])\n",
    "best_batch_size = int(optimizer.max['params']['batch_size'])\n",
    "best_learning_rate = optimizer.max['params']['learning_rate']\n",
    "best_patience = int(optimizer.max['params']['patience'])\n",
    "best_l1_reg = optimizer.max['params']['l1_reg']\n",
    "\n",
    "# Print the best hyperparameters and the time taken by the algorithm\n",
    "print('Best hyperparameters:')\n",
    "print(f'Encoding dim: {best_encoding_dim}')\n",
    "print(f'Batch size: {best_batch_size}')\n",
    "print(f'Learning rate: {best_learning_rate}')\n",
    "print(f'Patience: {best_patience}')\n",
    "print(f'L1 reg: {best_l1_reg}')\n",
    "print(f'Time taken: {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best hyperparameters: Encoding dim: {best_encoding_dim}, Batch size: {best_batch_size}, Learning rate: {best_learning_rate}, Patience: {best_patience}, L1 reg: {best_l1_reg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d6140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 15\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.0033827404209811787))(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "opt = Adam(lr= 0.00818356134505909)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder model with the specified optimizer and loss function\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    # Define early stopping to prevent overfitting and improve efficiency\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=5,batch_size=64, verbose=1, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "# plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.fill_between(range(1, len(mean_train_mse)+1), mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(range(1, len(mean_val_mse)+1), mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 42\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.1))(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "opt = Adam(lr= 0.1)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder model with the specified optimizer and loss function\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    # Define early stopping to prevent overfitting and improve efficiency\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=30, batch_size=64, verbose=1, validation_data=(X_val_fold, X_val_fold), callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(range(1, len(mean_train_mse)+1), mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(range(1, len(mean_val_mse)+1), mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.xlim([1,30]) # set the x-axis limit to show only up to 30 epochs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have two lists, one for the training MSE and one for the test MSE, for each epoch\n",
    "train_mse = [0.1, 0.05, 0.03, 0.02, 0.015, 0.012, 0.01, 0.008, 0.007, 0.006, 0.005, 0.004, 0.0035, 0.003, 0.0028, 0.0026, 0.0025, 0.0023, 0.0022, 0.0021]\n",
    "test_mse = [0.15, 0.12, 0.1, 0.09, 0.08, 0.075, 0.072, 0.07, 0.068, 0.065, 0.063, 0.062, 0.061, 0.06, 0.059, 0.058, 0.057, 0.056, 0.055, 0.054]\n",
    "epochs = range(1, len(train_mse)+1)\n",
    "\n",
    "plt.plot(epochs, train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(range(1, len(train_mse)+1), [x - 0.01 for x in train_mse], [x + 0.01 for x in train_mse], alpha=0.2, color='b')\n",
    "plt.plot(epochs, test_mse, 'r', label='Test MSE')\n",
    "plt.fill_between(range(1, len(test_mse)+1), [x - 0.01 for x in test_mse], [x + 0.01 for x in test_mse], alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.xlim([1,20]) # set the x-axis limit to show only up to 20 epochs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "space = {\n",
    "    'encoding_dim': hp.quniform('encoding_dim', 10, 50, 1),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "    'patience': hp.quniform('patience', 1, 10, 1),\n",
    "    'l1_reg': hp.uniform('l1_reg', 0, 0.1),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid', 'tanh']),\n",
    "    'epochs': hp.quniform('epochs', 10, 30, 1)\n",
    "}\n",
    "\n",
    "def optimize(params):\n",
    "    encoding_dim = int(params['encoding_dim'])\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    patience = int(params['patience'])\n",
    "    l1_reg = params['l1_reg']\n",
    "    activation = params['activation']\n",
    "    epochs=int(params['epochs'])\n",
    "   \n",
    "    # Define autoencoder architecture\n",
    "    input_dim = X_train_resampled_final.shape[1]\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    hidden_layer = Dense(encoding_dim, activation=activation, activity_regularizer=regularizers.l1(l1_reg))(input_layer)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "\n",
    "    # Define cross-validation parameters\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # Train and evaluate model using cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train_resampled_final):\n",
    "        # Split data into training and validation sets\n",
    "        train_data, val_data = X_train_resampled_final[train_idx], X_train_resampled_final[val_idx]\n",
    "\n",
    "        # Train model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, mode='min', restore_best_weights=True)\n",
    "        autoencoder.fit(train_data, train_data, epochs=epochs, batch_size=batch_size, validation_data=(val_data, val_data), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluate model\n",
    "        val_loss = autoencoder.evaluate(val_data, val_data, verbose=0)\n",
    "        losses.append(val_loss)\n",
    "\n",
    "    # Calculate mean validation loss across folds\n",
    "    mean_loss = np.mean(losses)\n",
    "\n",
    "    return {'loss': mean_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "best = fmin(fn=optimize, space=space, algo=tpe.suggest, max_evals=5)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50466ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input = Input(shape=(hidden_layer_output_train.shape[1],))\n",
    "x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.000111))(new_model_input)\n",
    "# x = Dense(32, activation='relu')(x)\n",
    "output = Dense(2, activation='sigmoid')(x)\n",
    "#output = Dense(1, activation='softmax')(x)\n",
    "mediator_network = Model(inputs=new_model_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert target labels to one-hot encoded format\n",
    "y_train_resampled_final_onehot = to_categorical(y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0dcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_new = Adam(lr= 0.000992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#Compile the new model\n",
    "mediator_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "history = mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot,\n",
    "                               epochs=15, batch_size=32, validation_split=0.1,\n",
    "                               callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = Model(inputs=mediator_network .input, outputs=mediator_network .layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "\n",
    "# x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.0000611))(new_model_input_med)\n",
    "\n",
    "# output_med = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# agent_network = Model(inputs=new_model_input_med, outputs=output_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the early stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# # Train the new model on the activations of the hidden layer\n",
    "# history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n",
    "#                                epochs=10, batch_size=32, validation_split=0.2,\n",
    "#                                callbacks=[early_stopping],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a088bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 15\n",
    "max_steps = 7\n",
    "learning_rate=0.5\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "# Define epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    else:\n",
    "        Q_values = Q(state, theta)\n",
    "        return np.argmax(Q_values)\n",
    "\n",
    "# Define Q-network\n",
    "# def agent_network(state_shape, num_actions):\n",
    "#     inputs = keras.layers.Input(shape=state_shape)\n",
    "#     x = keras.layers.Dense(32, activation='relu')(inputs)\n",
    "#     x = keras.layers.Dense(32, activation='relu')(x)\n",
    "#     outputs = keras.layers.Dense(num_actions)(x)\n",
    "#     model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "def agent_network(state_shape, num_actions):\n",
    "    inputs = keras.layers.Input(shape=(1,))\n",
    "    x = keras.layers.Dense(10, activation='tanh')(inputs)\n",
    "    outputs = keras.layers.Dense(num_actions)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Define Q function\n",
    "def Q(state, theta):\n",
    "    return agent_network(state.shape, 2)(state.reshape(1, -1)).numpy()[0]\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "def compute_loss(target_Q_values, predicted_Q_values):\n",
    "    return np.mean(np.square(target_Q_values - predicted_Q_values))\n",
    "\n",
    "# Define reward function\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    if action == 1:\n",
    "        reward = true_label * (predicted_label - lambda_val) - (1 - true_label) * (predicted_label + lambda_val)\n",
    "    else:\n",
    "        reward = (1 - true_label) * (predicted_label - lambda_val) - true_label * (predicted_label + lambda_val)\n",
    "    return reward, int(predicted_label == true_label)\n",
    "\n",
    "# Define hyperparameters\n",
    "num_episodes = 10\n",
    "max_steps = 20\n",
    "epsilon = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "model = agent_network(num_features, num_actions)\n",
    "theta = model.get_weights()\n",
    "\n",
    "# Initialize index counter for hidden_layer_output_train_med\n",
    "idx = 0\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state= hidden_layer_output_train_med[0, 0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "\n",
    "        # Get next state from hidden_layer_output_train_med\n",
    "        next_state = hidden_layer_output_train_med[idx, 0]\n",
    "        idx += 1\n",
    "\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], model)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Convert states and next_states tuples into numpy arrays\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, model)[np.arange(batch_size), actions.astype(int)]\n",
    "            target_Q_values = np.array(target_Q_values)\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Backpropagation\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(model.trainable_variables)\n",
    "                predictions = Q(states, model)\n",
    "                loss = compute_loss(target_Q_values, predictions[np.arange(batch_size), actions.astype(int)])\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "        \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd89f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 15\n",
    "max_steps = 7\n",
    "learning_rate = 0.5\n",
    "\n",
    "replay_memory_size = 20000\n",
    "num_features = D[0][0].shape[0]\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = epsilon\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "num_actions = 2\n",
    "\n",
    "# Define Q-network\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# Compile your Keras model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = model(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = model.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Reshape the state array to match the expected size\n",
    "    #state = np.reshape(state, (batch_size, num_features))\n",
    "    \n",
    "    #state = np.stack(states, axis=0)\n",
    "    #state = np.array(state)\n",
    "    #state = np.reshape(state, (batch_size, num_features))\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=2)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define hyperparameters\n",
    "# gamma = 0.85\n",
    "# epsilon = 0.1\n",
    "# batch_size = 128\n",
    "# num_episodes = 30\n",
    "# max_steps = 7\n",
    "# learning_rate = 0.5\n",
    "\n",
    "# replay_memory_size = 20000\n",
    "# num_features = D[0][0].shape[0]\n",
    "\n",
    "# D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# # Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "# tp = 0\n",
    "# tn = 0\n",
    "# fp = 0\n",
    "# fn = 0\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# environment = epsilon\n",
    "\n",
    "# theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# # Define Q-network\n",
    "# input_shape = hidden_layer_output_train_med[0].shape\n",
    "# num_actions = 2\n",
    "\n",
    "# # Define Q-network\n",
    "\n",
    "# # Define the hidden layer model\n",
    "# hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "#                                             outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# # Get the activations of the hidden layer for the training data\n",
    "# hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "# x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "# output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# opt_new=Adam(lr=0.00517)\n",
    "\n",
    "# # Compile your Keras model\n",
    "# agent_network.compile(optimizer='opt_new',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Initialize replay memory\n",
    "# replay_memory = []\n",
    "\n",
    "# def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Reshape state to (1, num_features)\n",
    "#     state = np.reshape(state, (1, -1))\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    "#     return Q_values\n",
    "\n",
    "\n",
    "# def epsilon_greedy_policy(state, epsilon, model):\n",
    "#     if np.random.uniform() < epsilon:\n",
    "#         # Choose a random action\n",
    "#         action = np.random.randint(num_actions)\n",
    "#     else:\n",
    "#         # Choose the action with the highest Q-value\n",
    "#         Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "#         action = np.argmax(Q_values)\n",
    "#     return action\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "# # Define function for computing loss\n",
    "# def compute_loss(y, Q_values):\n",
    "#     return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# # Start training\n",
    "# for episode in range(num_episodes):\n",
    "#     # Shuffle training data\n",
    "#     np.random.shuffle(D)\n",
    "#     print(\"Episode \", episode)\n",
    "#     print(\"--------------------------------------------\")\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "#     # Start episode\n",
    "#     for step in range(max_steps):\n",
    "#         # Choose action\n",
    "#         action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "#         # Get true label\n",
    "#         true_label = D[step][1]\n",
    "        \n",
    "#         # Predict label\n",
    "#         predicted_label = action\n",
    "        \n",
    "#         # Get next state\n",
    "#         next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "#         print(\"Step:\", step)\n",
    "#         print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"\")\n",
    "        \n",
    "#         # Update counters for precision and accuracy\n",
    "#         if true_label == 1:\n",
    "#             if predicted_label == 1:\n",
    "#                 tp += 1\n",
    "#             else:\n",
    "#                 fn += 1\n",
    "#         else:\n",
    "#             if predicted_label == 1:\n",
    "#                 fp += 1\n",
    "#             else:\n",
    "#                 tn += 1\n",
    "        \n",
    "\n",
    "#         # Store experience in memory\n",
    "#         replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "#         # Sample a batch of experiences from memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             batch = random.sample(replay_memory, batch_size)\n",
    "#             states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "#             # Convert actions tuple into numpy array\n",
    "#             actions = np.array(actions)\n",
    "\n",
    "#             # Compute target Q-values\n",
    "#             target_Q_values = []\n",
    "#             for i in range(batch_size):\n",
    "#                 if terminals[i]:\n",
    "#                     target_Q_values.append(rewards[i])\n",
    "#                 else:\n",
    "#                     next_Q_values = Q(next_states[i], theta)\n",
    "#                     target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "#             # Compute predicted Q-values and loss\n",
    "#             predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "#             loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "#             # Compute gradients\n",
    "#             grad = np.gradient(loss, np.ravel(theta.T), axis=2)\n",
    "\n",
    "#             # Reshape gradients to match the shape of theta\n",
    "#             grad = grad.reshape(theta.shape)\n",
    "\n",
    "#             # Update parameters using gradient descent\n",
    "#             theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "#         # Update state\n",
    "#         state = next_state\n",
    "        \n",
    "#         # Check if episode is finished\n",
    "#         if terminal==1:\n",
    "#             break\n",
    "            \n",
    "# # Calculate precision and accuracy\n",
    "# precision = tp / (tp + fp)\n",
    "# accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "#batch_size = 128\n",
    "replay_memory_size = 1000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 5\n",
    "learning_rate = 0.5\n",
    "\n",
    "replay_memory_size = 20000\n",
    "num_features = D[0][0].shape[0]\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = epsilon\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "num_actions = 2\n",
    "\n",
    "# Define Q-network\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# Compile your Keras model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = model(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = model.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac057377",
   "metadata": {},
   "source": [
    "# FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d275b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "#Extraction of hidden layer\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.0011)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all data points in the training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Randomly initialize parameters θ\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d1d34",
   "metadata": {},
   "source": [
    "## FInal New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Randomly initialize parameters θ\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "# new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# reshaped_input_med = keras.layers.Lambda(lambda x: x, output_shape=(1, 10))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "    \n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    \n",
    "    # state = np.reshape(state, (batch_size, -1))\n",
    "    \n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "       # Initialize parameters\n",
    "        theta = agent_network.trainable_variables\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acef758",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_output_train_med.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b83c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.75\n",
    "epsilon = 0.1\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.41\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.425\n",
    "epsilon = 0.2\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.91\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddaae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 35\n",
    "max_steps = 4\n",
    "learning_rate = 0.13\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "        print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.90\n",
    "epsilon = 0.3\n",
    "batch_size = 128\n",
    "learning_rate = 0.61\n",
    "\n",
    "max_steps = 5\n",
    "num_episodes = 20\n",
    "replay_memory_size = 20000\n",
    "\n",
    "# \n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.425\n",
    "epsilon = 0.2\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.91\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb09f80",
   "metadata": {},
   "source": [
    "# Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15570cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the values for the pie chart\n",
    "sizes = [44,47,3,6]\n",
    "\n",
    "# Define the labels for each value\n",
    "labels = ['True label: 1, Prediction:1', 'True label: 0, Prediction:0', 'True label: 1, Prediction:0', 'True label: 0, Prediction:1']\n",
    "\n",
    "# Define the colors for each value\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n",
    "\n",
    "# Create the pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Draw a circle at the center of the pie chart\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Add a legend to the pie chart\n",
    "ax1.legend(labels, loc=\"best\")\n",
    "\n",
    "# Set the title of the pie chart\n",
    "plt.title(\"My Pie Chart\")\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network, to_file='a_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8acefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    " # Set the weights and biases of the network to the values in theta\n",
    "    agent_network.set_weights(theta)\n",
    "    \n",
    "    # Pass the state through the network to get the Q-values for each action\n",
    "    q_values = agent_network.predict(np.array([state]))\n",
    "    \n",
    "    # Return the Q-values as a numpy array\n",
    "    return q_values\n",
    "    #return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    # target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "                    # target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "                    target_Q_values.append(np.mean((reward + gamma * np.max(Q_star[next_state, :]))))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    "#     return Q_values\n",
    "\n",
    "# def compute_optimal_Q(next_states, rewards, terminals, gamma, Q_star):\n",
    "#     target_Q_values = []\n",
    "#     for i in range(batch_size):\n",
    "#         if terminals[i]:\n",
    "#             target_Q_values.append(rewards[i])\n",
    "#         else:\n",
    "#             next_Q_values = Q_star[next_states[i]]\n",
    "#             target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "#     return target_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "# Policy function\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Rearding the agent\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values: This code block is necessary to update the Q-network so that \n",
    "            # it accurately predicts the Q-values for each state-action pair.\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                # if the next state is a terminal state (i.e., the episode is over), then the value of the \n",
    "                # state-action pair is simply the immediate reward received.\n",
    "                if terminals[i]:\n",
    "                    \n",
    "                    target_Q_values.append(rewards[i])\n",
    "                    \n",
    "                # If the next state is not a terminal state, then the value of the state-action pair is the immediate reward received plus the discounted value of the best action in the next state,\n",
    "                # which is computed using the deep neural network with the previous set of parameters, denoted as θk-1.\n",
    "                \n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "                \n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c423cc",
   "metadata": {},
   "outputs": [],
   "source": [
    " theta = agent_network.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f7c34",
   "metadata": {},
   "source": [
    "# Working in 3 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.5 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 3\n",
    "learning_rate = 0.44\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "\n",
    "# Randomly initialize paramter θ: theta represents the weights and biases of the network.\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Aprroximation Q-function via Neural Network\n",
    "def Q(state, theta):\n",
    "\n",
    "    # Set the weights and biases of the network to the values in theta\n",
    "    agent_network.set_weights(theta)\n",
    "    \n",
    "    # Pass the state through the network to get the Q-values for each action\n",
    "    q_values = agent_network.predict(np.array([state]))\n",
    "    \n",
    "    # Return the Q-values as a numpy array\n",
    "    return q_values\n",
    "\n",
    "# Policy function: Action selection\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Rearding the agent\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            #if all(isinstance(item, tuple) for item in batch): # Check if all items in the batch are tuples\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "            #else:\n",
    "                #continue # Skip this iteration if batch contains non-tuple items\n",
    "\n",
    "            # Compute target Q-values: This code block is necessary to update the Q-network so that \n",
    "            # it accurately predicts the Q-values for each state-action pair.\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                # if the next state is a terminal state (i.e., the episode is over), then the value of the \n",
    "                # state-action pair is simply the immediate reward received.\n",
    "                if terminals[i]:\n",
    "                    \n",
    "                    target_Q_values.append(rewards[i])\n",
    "                    \n",
    "                # If the next state is not a terminal state, then the value of the state-action pair is the immediate reward received plus the discounted value of the best action in the next state,\n",
    "                # which is computed using the deep neural network with the previous set of parameters, denoted as θk-1.\n",
    "                \n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values via the optimal Bellman\n",
    "                    target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "            \n",
    "            # Train the agent_network on the batch of data\n",
    "            hist = agent_network.train_on_batch(np.array(states), np.array(target_Q_values))\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision, recall, F1-score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()\n",
    "\n",
    "print(\"tp:\", tp)\n",
    "print(\"fp:\", fp)\n",
    "print(\"tn:\", tn)\n",
    "print(\"fn:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network , to_file='a_network .png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the values for the pie chart\n",
    "sizes = [44.7,47.4,3.48,4.42]\n",
    "\n",
    "# Define the labels for each value\n",
    "labels = ['True label: 1, Prediction:1','True label: 0, Prediction:0', 'True label: 1, Prediction:0', 'True label: 0, Prediction:1']\n",
    "\n",
    "# Define the colors for each value\n",
    "colors = ['#ffcc99','#40e0d0','#66b3ff','#ff9999']\n",
    "\n",
    "# Create the pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Draw a circle at the center of the pie chart\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Move the legend to the right of the pie chart\n",
    "ax1.legend(labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set the title of the pie chart\n",
    "plt.title(\"Performance of the Q-Fraud Detection System\")\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0855b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
