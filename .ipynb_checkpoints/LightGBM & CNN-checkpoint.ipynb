{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f374f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90396195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bda14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41793035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f83c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5f220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a3c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ced06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5184944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0        371   33602.910000   42421.000000             0.0         22079.8   \n",
      "1        138   44423.330000   42421.000000             0.0         22079.8   \n",
      "2        325  124507.730000    4564.000000             0.0         22079.8   \n",
      "3        308  300712.340000   51474.000000             0.0         22079.8   \n",
      "4        349   47243.760000   11262.000000             0.0             0.0   \n",
      "...      ...            ...            ...             ...             ...   \n",
      "426744   276  111168.880136  111168.880136             0.0         22079.8   \n",
      "426745   274  124507.730000   42421.000000             0.0         22079.8   \n",
      "426746    60  124507.730000   42421.000000             0.0             0.0   \n",
      "426747   449   44882.356239   44882.356239             0.0             0.0   \n",
      "426748   220   39953.091459   29059.334627             0.0         22079.8   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0        257366.420000               0     1     82314    488591  \n",
      "1        184168.130000               0     0     92881    180374  \n",
      "2        184168.130000               0     1     80756    482539  \n",
      "3        184168.130000               0     1    175711    597630  \n",
      "4             0.000000               0     3    167405     26253  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "426744   184168.130000               0     1     90379    472585  \n",
      "426745   184168.130000               0     1    112071    494845  \n",
      "426746        0.000000               0     1    154830    240268  \n",
      "426747    36237.626509               0     1    122579     88980  \n",
      "426748   184168.130000               0     1     93537    130866  \n",
      "\n",
      "[426749 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "485376  278.0   22928.58            0.0             0.0           0.000   \n",
      "642214   45.0    8606.90         5764.0             0.0           0.000   \n",
      "192982  237.0  220046.83            0.0             0.0      130797.505   \n",
      "99091   328.0   83938.53        13653.5             0.0      130797.505   \n",
      "203398  307.0   74636.86            0.0             0.0      130797.505   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "230877  154.0  195805.05        31725.0             0.0           0.000   \n",
      "315026  301.0   36352.03        13653.5             0.0           0.000   \n",
      "661254  238.5  163969.90        13653.5             0.0      130797.505   \n",
      "688112  280.0    3092.79            0.0             0.0           0.000   \n",
      "642560   35.0   74636.86        30807.0             0.0      130797.505   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "485376           0.000               0     3    291184    424837  \n",
      "642214           0.000               0     3    363649    442961  \n",
      "192982      214326.245               0     1      1853    410946  \n",
      "99091       537297.070               0     0    252825    347652  \n",
      "203398      214326.245               0     1    201182    417173  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "230877      195805.050               0     1    181881    192704  \n",
      "315026           0.000               0     3    458861    630843  \n",
      "661254      706564.020               0     0     37270    676511  \n",
      "688112           0.000               0     3    455345    152073  \n",
      "642560      214326.245               0     1    214954    689599  \n",
      "\n",
      "[70000 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0385cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc22192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba3d6140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000295A4AA90D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000295A4AA90D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "6641/6668 [============================>.] - ETA: 0s - loss: 0.4590WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000295AB34F8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000295AB34F8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "6668/6668 [==============================] - 28s 4ms/step - loss: 0.4589 - val_loss: 0.4846\n",
      "Epoch 2/30\n",
      "6668/6668 [==============================] - 27s 4ms/step - loss: 0.4094 - val_loss: 0.4767\n",
      "Epoch 3/30\n",
      "6668/6668 [==============================] - 22s 3ms/step - loss: 0.4057 - val_loss: 0.4749\n",
      "Epoch 4/30\n",
      "6668/6668 [==============================] - 27s 4ms/step - loss: 0.4046 - val_loss: 0.4743\n",
      "Epoch 5/30\n",
      "6668/6668 [==============================] - 24s 4ms/step - loss: 0.4042 - val_loss: 0.4740\n",
      "Epoch 6/30\n",
      "6668/6668 [==============================] - 22s 3ms/step - loss: 0.4040 - val_loss: 0.4738\n",
      "Epoch 7/30\n",
      "6668/6668 [==============================] - 22s 3ms/step - loss: 0.4038 - val_loss: 0.4737\n",
      "Epoch 8/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4037 - val_loss: 0.4736\n",
      "Epoch 9/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4037 - val_loss: 0.4736\n",
      "Epoch 10/30\n",
      "6668/6668 [==============================] - 21s 3ms/step - loss: 0.4036 - val_loss: 0.4735\n",
      "Epoch 11/30\n",
      "6668/6668 [==============================] - 22s 3ms/step - loss: 0.4036 - val_loss: 0.4735\n",
      "Epoch 12/30\n",
      "6668/6668 [==============================] - 22s 3ms/step - loss: 0.4035 - val_loss: 0.4734\n",
      "Epoch 13/30\n",
      "6668/6668 [==============================] - 23s 3ms/step - loss: 0.4035 - val_loss: 0.4735\n",
      "Epoch 14/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4035 - val_loss: 0.4734\n",
      "Epoch 15/30\n",
      "6668/6668 [==============================] - 21s 3ms/step - loss: 0.4035 - val_loss: 0.4734\n",
      "Epoch 16/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4035 - val_loss: 0.4734\n",
      "Epoch 17/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4034 - val_loss: 0.4734\n",
      "Epoch 18/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 19/30\n",
      "6668/6668 [==============================] - 23s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 20/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 21/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4034 - val_loss: 0.4734\n",
      "Epoch 22/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 23/30\n",
      "6668/6668 [==============================] - 17s 3ms/step - loss: 0.4034 - val_loss: 0.4734\n",
      "Epoch 24/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 25/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 26/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 27/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4034 - val_loss: 0.4733\n",
      "Epoch 28/30\n",
      "6668/6668 [==============================] - 21s 3ms/step - loss: 0.4033 - val_loss: 0.4733\n",
      "Epoch 29/30\n",
      "6668/6668 [==============================] - 23s 3ms/step - loss: 0.4033 - val_loss: 0.4732\n",
      "Epoch 30/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4033 - val_loss: 0.4733\n",
      "Test MSE: 0.43057\n",
      "Epoch 1/30\n",
      "6668/6668 [==============================] - 21s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 2/30\n",
      "6668/6668 [==============================] - 21s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 3/30\n",
      "6668/6668 [==============================] - 22s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 4/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 5/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 6/30\n",
      "6668/6668 [==============================] - 21s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 7/30\n",
      "6668/6668 [==============================] - 21s 3ms/step - loss: 0.4732 - val_loss: 0.4035\n",
      "Epoch 8/30\n",
      "6668/6668 [==============================] - 19s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 9/30\n",
      "6668/6668 [==============================] - 20s 3ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 10/30\n",
      "6668/6668 [==============================] - 24s 4ms/step - loss: 0.4732 - val_loss: 0.4035\n",
      "Epoch 11/30\n",
      "6668/6668 [==============================] - 31s 5ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 12/30\n",
      "6668/6668 [==============================] - 29s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 13/30\n",
      "6668/6668 [==============================] - 29s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 14/30\n",
      "6668/6668 [==============================] - 31s 5ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 15/30\n",
      "6668/6668 [==============================] - 28s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 16/30\n",
      "6668/6668 [==============================] - 26s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 17/30\n",
      "6668/6668 [==============================] - 23s 3ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 18/30\n",
      "6668/6668 [==============================] - 26s 4ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 19/30\n",
      "6668/6668 [==============================] - 27s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 20/30\n",
      "6668/6668 [==============================] - 26s 4ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 21/30\n",
      "6668/6668 [==============================] - 27s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 22/30\n",
      "6668/6668 [==============================] - 22s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 23/30\n",
      "6668/6668 [==============================] - 28s 4ms/step - loss: 0.4732 - val_loss: 0.4035\n",
      "Epoch 24/30\n",
      "6668/6668 [==============================] - 25s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 25/30\n",
      "6668/6668 [==============================] - 24s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 26/30\n",
      "6668/6668 [==============================] - 23s 3ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 27/30\n",
      "6668/6668 [==============================] - 27s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 28/30\n",
      "6668/6668 [==============================] - 23s 3ms/step - loss: 0.4732 - val_loss: 0.4034\n",
      "Epoch 29/30\n",
      "6668/6668 [==============================] - 32s 5ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Epoch 30/30\n",
      "6668/6668 [==============================] - 28s 4ms/step - loss: 0.4732 - val_loss: 0.4033\n",
      "Test MSE: 0.43060\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 15\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='tanh', kernel_regularizer=regularizers.l1(0.000391))(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "opt = Adam(lr= 0.00087)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder model with the specified optimizer and loss function\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    # Define early stopping to prevent overfitting and improve efficiency\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=30,batch_size=32, verbose=1, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50466ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001EEB824DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001EEB824DAF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12792/12792 [==============================] - 13s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input = Input(shape=(hidden_layer_output_train.shape[1],))\n",
    "x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.000111))(new_model_input)\n",
    "# x = Dense(32, activation='relu')(x)\n",
    "output = Dense(2, activation='sigmoid')(x)\n",
    "#output = Dense(1, activation='softmax')(x)\n",
    "mediator_network = Model(inputs=new_model_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db0c2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert target labels to one-hot encoded format\n",
    "y_train_resampled_final_onehot = to_categorical(y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f0dcd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt_new = Adam(lr= 0.000992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34b3c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001EEB59F0288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001EEB59F0288> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "11513/11513 [==============================] - ETA: 0s - loss: 0.4006WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001EEB5B8BD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001EEB5B8BD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.4006 - val_loss: 0.7881\n",
      "Epoch 2/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.2365 - val_loss: 0.3959\n",
      "Epoch 3/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.1830 - val_loss: 0.3886\n",
      "Epoch 4/15\n",
      "11513/11513 [==============================] - 17s 2ms/step - loss: 0.1734 - val_loss: 0.4432\n",
      "Epoch 5/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.1684 - val_loss: 0.3044\n",
      "Epoch 6/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.1648 - val_loss: 0.3222\n",
      "Epoch 7/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.1621 - val_loss: 0.3215\n",
      "Epoch 8/15\n",
      "11513/11513 [==============================] - 20s 2ms/step - loss: 0.1595 - val_loss: 0.3349\n",
      "Epoch 9/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.1574 - val_loss: 0.3135\n",
      "Epoch 10/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.1557 - val_loss: 0.3110\n",
      "Epoch 11/15\n",
      "11513/11513 [==============================] - 18s 2ms/step - loss: 0.1543 - val_loss: 0.3276\n",
      "Epoch 12/15\n",
      "11513/11513 [==============================] - 20s 2ms/step - loss: 0.1531 - val_loss: 0.3260\n",
      "Epoch 13/15\n",
      "11513/11513 [==============================] - 21s 2ms/step - loss: 0.1521 - val_loss: 0.2898\n",
      "Epoch 14/15\n",
      "11513/11513 [==============================] - 23s 2ms/step - loss: 0.1511 - val_loss: 0.3096\n",
      "Epoch 15/15\n",
      "11513/11513 [==============================] - 20s 2ms/step - loss: 0.1503 - val_loss: 0.3088\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#Compile the new model\n",
    "mediator_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "history = mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot,\n",
    "                               epochs=15, batch_size=32, validation_split=0.1,\n",
    "                               callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = Model(inputs=mediator_network .input, outputs=mediator_network .layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "\n",
    "# x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.0000611))(new_model_input_med)\n",
    "\n",
    "# output_med = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# agent_network = Model(inputs=new_model_input_med, outputs=output_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the early stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# # Train the new model on the activations of the hidden layer\n",
    "# history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n",
    "#                                epochs=10, batch_size=32, validation_split=0.2,\n",
    "#                                callbacks=[early_stopping],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a088bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 15\n",
    "max_steps = 7\n",
    "learning_rate=0.5\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "# Define epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    else:\n",
    "        Q_values = Q(state, theta)\n",
    "        return np.argmax(Q_values)\n",
    "\n",
    "# Define Q-network\n",
    "# def agent_network(state_shape, num_actions):\n",
    "#     inputs = keras.layers.Input(shape=state_shape)\n",
    "#     x = keras.layers.Dense(32, activation='relu')(inputs)\n",
    "#     x = keras.layers.Dense(32, activation='relu')(x)\n",
    "#     outputs = keras.layers.Dense(num_actions)(x)\n",
    "#     model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "def agent_network(state_shape, num_actions):\n",
    "    inputs = keras.layers.Input(shape=(1,))\n",
    "    x = keras.layers.Dense(10, activation='tanh')(inputs)\n",
    "    outputs = keras.layers.Dense(num_actions)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Define Q function\n",
    "def Q(state, theta):\n",
    "    return agent_network(state.shape, 2)(state.reshape(1, -1)).numpy()[0]\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "def compute_loss(target_Q_values, predicted_Q_values):\n",
    "    return np.mean(np.square(target_Q_values - predicted_Q_values))\n",
    "\n",
    "# Define reward function\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    if action == 1:\n",
    "        reward = true_label * (predicted_label - lambda_val) - (1 - true_label) * (predicted_label + lambda_val)\n",
    "    else:\n",
    "        reward = (1 - true_label) * (predicted_label - lambda_val) - true_label * (predicted_label + lambda_val)\n",
    "    return reward, int(predicted_label == true_label)\n",
    "\n",
    "# Define hyperparameters\n",
    "num_episodes = 10\n",
    "max_steps = 20\n",
    "epsilon = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "model = agent_network(num_features, num_actions)\n",
    "theta = model.get_weights()\n",
    "\n",
    "# Initialize index counter for hidden_layer_output_train_med\n",
    "idx = 0\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state= hidden_layer_output_train_med[0, 0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "\n",
    "        # Get next state from hidden_layer_output_train_med\n",
    "        next_state = hidden_layer_output_train_med[idx, 0]\n",
    "        idx += 1\n",
    "\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], model)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Convert states and next_states tuples into numpy arrays\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, model)[np.arange(batch_size), actions.astype(int)]\n",
    "            target_Q_values = np.array(target_Q_values)\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Backpropagation\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(model.trainable_variables)\n",
    "                predictions = Q(states, model)\n",
    "                loss = compute_loss(target_Q_values, predictions[np.arange(batch_size), actions.astype(int)])\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "        \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd89f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 15\n",
    "max_steps = 7\n",
    "learning_rate = 0.5\n",
    "\n",
    "replay_memory_size = 20000\n",
    "num_features = D[0][0].shape[0]\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = epsilon\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "num_actions = 2\n",
    "\n",
    "# Define Q-network\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# Compile your Keras model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = model(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = model.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Reshape the state array to match the expected size\n",
    "    #state = np.reshape(state, (batch_size, num_features))\n",
    "    \n",
    "    #state = np.stack(states, axis=0)\n",
    "    #state = np.array(state)\n",
    "    #state = np.reshape(state, (batch_size, num_features))\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=2)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define hyperparameters\n",
    "# gamma = 0.85\n",
    "# epsilon = 0.1\n",
    "# batch_size = 128\n",
    "# num_episodes = 30\n",
    "# max_steps = 7\n",
    "# learning_rate = 0.5\n",
    "\n",
    "# replay_memory_size = 20000\n",
    "# num_features = D[0][0].shape[0]\n",
    "\n",
    "# D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# # Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "# tp = 0\n",
    "# tn = 0\n",
    "# fp = 0\n",
    "# fn = 0\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# environment = epsilon\n",
    "\n",
    "# theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# # Define Q-network\n",
    "# input_shape = hidden_layer_output_train_med[0].shape\n",
    "# num_actions = 2\n",
    "\n",
    "# # Define Q-network\n",
    "\n",
    "# # Define the hidden layer model\n",
    "# hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "#                                             outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# # Get the activations of the hidden layer for the training data\n",
    "# hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "# x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "# output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# opt_new=Adam(lr=0.00517)\n",
    "\n",
    "# # Compile your Keras model\n",
    "# agent_network.compile(optimizer='opt_new',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Initialize replay memory\n",
    "# replay_memory = []\n",
    "\n",
    "# def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Reshape state to (1, num_features)\n",
    "#     state = np.reshape(state, (1, -1))\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    "#     return Q_values\n",
    "\n",
    "\n",
    "# def epsilon_greedy_policy(state, epsilon, model):\n",
    "#     if np.random.uniform() < epsilon:\n",
    "#         # Choose a random action\n",
    "#         action = np.random.randint(num_actions)\n",
    "#     else:\n",
    "#         # Choose the action with the highest Q-value\n",
    "#         Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "#         action = np.argmax(Q_values)\n",
    "#     return action\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "# # Define function for computing loss\n",
    "# def compute_loss(y, Q_values):\n",
    "#     return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# # Start training\n",
    "# for episode in range(num_episodes):\n",
    "#     # Shuffle training data\n",
    "#     np.random.shuffle(D)\n",
    "#     print(\"Episode \", episode)\n",
    "#     print(\"--------------------------------------------\")\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "#     # Start episode\n",
    "#     for step in range(max_steps):\n",
    "#         # Choose action\n",
    "#         action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "#         # Get true label\n",
    "#         true_label = D[step][1]\n",
    "        \n",
    "#         # Predict label\n",
    "#         predicted_label = action\n",
    "        \n",
    "#         # Get next state\n",
    "#         next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "#         print(\"Step:\", step)\n",
    "#         print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"\")\n",
    "        \n",
    "#         # Update counters for precision and accuracy\n",
    "#         if true_label == 1:\n",
    "#             if predicted_label == 1:\n",
    "#                 tp += 1\n",
    "#             else:\n",
    "#                 fn += 1\n",
    "#         else:\n",
    "#             if predicted_label == 1:\n",
    "#                 fp += 1\n",
    "#             else:\n",
    "#                 tn += 1\n",
    "        \n",
    "\n",
    "#         # Store experience in memory\n",
    "#         replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "#         # Sample a batch of experiences from memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             batch = random.sample(replay_memory, batch_size)\n",
    "#             states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "#             # Convert actions tuple into numpy array\n",
    "#             actions = np.array(actions)\n",
    "\n",
    "#             # Compute target Q-values\n",
    "#             target_Q_values = []\n",
    "#             for i in range(batch_size):\n",
    "#                 if terminals[i]:\n",
    "#                     target_Q_values.append(rewards[i])\n",
    "#                 else:\n",
    "#                     next_Q_values = Q(next_states[i], theta)\n",
    "#                     target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "#             # Compute predicted Q-values and loss\n",
    "#             predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "#             loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "#             # Compute gradients\n",
    "#             grad = np.gradient(loss, np.ravel(theta.T), axis=2)\n",
    "\n",
    "#             # Reshape gradients to match the shape of theta\n",
    "#             grad = grad.reshape(theta.shape)\n",
    "\n",
    "#             # Update parameters using gradient descent\n",
    "#             theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "#         # Update state\n",
    "#         state = next_state\n",
    "        \n",
    "#         # Check if episode is finished\n",
    "#         if terminal==1:\n",
    "#             break\n",
    "            \n",
    "# # Calculate precision and accuracy\n",
    "# precision = tp / (tp + fp)\n",
    "# accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "#batch_size = 128\n",
    "replay_memory_size = 1000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 5\n",
    "learning_rate = 0.5\n",
    "\n",
    "replay_memory_size = 20000\n",
    "num_features = D[0][0].shape[0]\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = epsilon\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "num_actions = 2\n",
    "\n",
    "# Define Q-network\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# Compile your Keras model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = model(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = model.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac057377",
   "metadata": {},
   "source": [
    "# FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d275b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "#Extraction of hidden layer\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.0011)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all data points in the training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Randomly initialize parameters θ\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d1d34",
   "metadata": {},
   "source": [
    "## FInal New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Randomly initialize parameters θ\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "# new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# reshaped_input_med = keras.layers.Lambda(lambda x: x, output_shape=(1, 10))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "    \n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    \n",
    "    # state = np.reshape(state, (batch_size, -1))\n",
    "    \n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "       # Initialize parameters\n",
    "        theta = agent_network.trainable_variables\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acef758",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_output_train_med.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b83c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.75\n",
    "epsilon = 0.1\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.41\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.425\n",
    "epsilon = 0.2\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.91\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddaae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 35\n",
    "max_steps = 4\n",
    "learning_rate = 0.13\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "        print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.90\n",
    "epsilon = 0.3\n",
    "batch_size = 128\n",
    "learning_rate = 0.61\n",
    "\n",
    "max_steps = 5\n",
    "num_episodes = 20\n",
    "replay_memory_size = 20000\n",
    "\n",
    "# \n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.425\n",
    "epsilon = 0.2\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.91\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb09f80",
   "metadata": {},
   "source": [
    "# Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a15570cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the values for the pie chart\n",
    "sizes = [44,47,3,6]\n",
    "\n",
    "# Define the labels for each value\n",
    "labels = ['True label: 1, Prediction:1', 'True label: 0, Prediction:0', 'True label: 1, Prediction:0', 'True label: 0, Prediction:1']\n",
    "\n",
    "# Define the colors for each value\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n",
    "\n",
    "# Create the pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Draw a circle at the center of the pie chart\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Add a legend to the pie chart\n",
    "ax1.legend(labels, loc=\"best\")\n",
    "\n",
    "# Set the title of the pie chart\n",
    "plt.title(\"My Pie Chart\")\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network, to_file='a_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8acefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    " # Set the weights and biases of the network to the values in theta\n",
    "    agent_network.set_weights(theta)\n",
    "    \n",
    "    # Pass the state through the network to get the Q-values for each action\n",
    "    q_values = agent_network.predict(np.array([state]))\n",
    "    \n",
    "    # Return the Q-values as a numpy array\n",
    "    return q_values\n",
    "    #return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    # target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "                    # target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "                    target_Q_values.append(np.mean((reward + gamma * np.max(Q_star[next_state, :]))))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    "#     return Q_values\n",
    "\n",
    "# def compute_optimal_Q(next_states, rewards, terminals, gamma, Q_star):\n",
    "#     target_Q_values = []\n",
    "#     for i in range(batch_size):\n",
    "#         if terminals[i]:\n",
    "#             target_Q_values.append(rewards[i])\n",
    "#         else:\n",
    "#             next_Q_values = Q_star[next_states[i]]\n",
    "#             target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "#     return target_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "# Policy function\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Rearding the agent\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values: This code block is necessary to update the Q-network so that \n",
    "            # it accurately predicts the Q-values for each state-action pair.\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                # if the next state is a terminal state (i.e., the episode is over), then the value of the \n",
    "                # state-action pair is simply the immediate reward received.\n",
    "                if terminals[i]:\n",
    "                    \n",
    "                    target_Q_values.append(rewards[i])\n",
    "                    \n",
    "                # If the next state is not a terminal state, then the value of the state-action pair is the immediate reward received plus the discounted value of the best action in the next state,\n",
    "                # which is computed using the deep neural network with the previous set of parameters, denoted as θk-1.\n",
    "                \n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "                \n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c423cc",
   "metadata": {},
   "outputs": [],
   "source": [
    " theta = agent_network.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f7c34",
   "metadata": {},
   "source": [
    "# Working in 3 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf46f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001EEB3FE5AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001EEB3FE5AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12792/12792 [==============================] - 12s 969us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc3bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fd6057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  10\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  11\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  12\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  13\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  14\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  15\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  16\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  17\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  18\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  19\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  20\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  21\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  22\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  23\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  24\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  25\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  26\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  27\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  28\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  29\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Precision: 0.34782608695652173\n",
      "Recall: 0.3076923076923077\n",
      "F1-score: 0.32653061224489793\n",
      "Accuracy: 0.5285714285714286\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 8\n",
      "fp: 15\n",
      "tn: 29\n",
      "fn: 18\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.51 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 3\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "\n",
    "# Randomly initialize paramter θ: theta represents the weights and biases of the network.\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Aprroximation Q-function via Neural Network\n",
    "def Q(state, theta):\n",
    "\n",
    "    # Set the weights and biases of the network to the values in theta\n",
    "    agent_network.set_weights(theta)\n",
    "    \n",
    "    # Pass the state through the network to get the Q-values for each action\n",
    "    q_values = agent_network.predict(np.array([state]))\n",
    "    \n",
    "    # Return the Q-values as a numpy array\n",
    "    return q_values\n",
    "\n",
    "# Policy function: Action selection\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Rearding the agent\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            #if all(isinstance(item, tuple) for item in batch): # Check if all items in the batch are tuples\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "            #else:\n",
    "                #continue # Skip this iteration if batch contains non-tuple items\n",
    "\n",
    "            # Compute target Q-values: This code block is necessary to update the Q-network so that \n",
    "            # it accurately predicts the Q-values for each state-action pair.\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                # if the next state is a terminal state (i.e., the episode is over), then the value of the \n",
    "                # state-action pair is simply the immediate reward received.\n",
    "                if terminals[i]:\n",
    "                    \n",
    "                    target_Q_values.append(rewards[i])\n",
    "                    \n",
    "                # If the next state is not a terminal state, then the value of the state-action pair is the immediate reward received plus the discounted value of the best action in the next state,\n",
    "                # which is computed using the deep neural network with the previous set of parameters, denoted as θk-1.\n",
    "                \n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values via the optimal Bellman\n",
    "                    target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "            \n",
    "            # Train the agent_network on the batch of data\n",
    "            hist = agent_network.train_on_batch(np.array(states), np.array(target_Q_values))\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()\n",
    "\n",
    "print(\"tp:\", tp)\n",
    "print(\"fp:\", fp)\n",
    "print(\"tn:\", tn)\n",
    "print(\"fn:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network , to_file='a_network .png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec37a8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAw0AAAGZCAYAAADLid2cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeXxMZ/v48c9k3xPZVyRCUGsstbQIWkVQqkqt1YfqitLqo621raWqaPtVtVQFpT9UiaXSqqX2IqIEtQtJCIktiyRz//6YJ1Mjk0hIMlmu9+s1L3Lmnvu+zpmTyVzn3ItGKaUQQgghhBBCiDyYmToAIYQQQgghROkmSYMQQgghhBAiX5I0CCGEEEIIIfIlSYMQQgghhBAiX5I0CCGEEEIIIfIlSYMQQgghhBAiX5I0CCGEEEIIIfIlSYMQQgghhBAiX5I0CCGEEEIIIfIlSYMwicWLF6PRaPQPCwsL/P39eeWVV7h8+XKRt3f48GFat26Ns7MzGo2GWbNmFXkbwrivvvqK4OBgrKys0Gg0pKSkGC23e/duJkyYYPT5qlWrEh4eXtyhApCZmcncuXNp3rw5zs7O2NraUrt2bcaOHUtycnKB6xk0aJDBOX7/IzIyshj3oPD69etHcHDwQ8s99dRT+n0wMzPD0dGR6tWr06tXL9asWYNS6pFjWLp0KXPmzHnk1z9uO1lZWWg0Gj755JNij8GYPXv20K1bNwICArC2tsbLy4vmzZvz/vvvF0t7cXFxTJgwgZiYmGKpXwhR/liYOgBRsX3//ffUrFmTtLQ0duzYwZQpU9i+fTtHjx7F3t6+yNoZPHgwd+/eZcWKFVSqVImqVasWWd0ib9HR0bzzzjv85z//YeDAgVhYWODo6Gi07O7du5k4cSKDBg3CxcWlhCPVSU1NpVOnTvz5558MHTqUjz/+GFtbW/bs2cOMGTP48ccf+e2336hWrVqB6rO1tWXr1q25ttesWbOoQy8x1atXZ8mSJQDcvXuXs2fP8vPPP/PCCy/QunVr1q9fn+d7nJ+lS5dy+vRp3nnnnaIOuUDtWFhYsGfPHgICAoq1fWPWrVtH9+7dadu2LZ9//jk+Pj5cuXKFv/76i5UrVzJ9+vQibzMuLo6JEycSHBxMvXr1irx+IUT5I0mDMKk6derQuHFjAMLCwsjOzmby5MmsXbuWvn37Plbd2dnZZGVlYW1tzd9//82QIUPo2LFjUYRNZmam/g6JyNuxY8cAGDJkCE2bNjVxNA83cuRItm/fzooVK3jppZf028PCwujZsydNmzalZ8+eHDx4EDOzh9+oNTMzo1mzZoWK4d69e5iZmZXac8vOzs5gn9q1a8eQIUNYsGABQ4YMYdiwYSxbtsyEET66wr5XRWXatGkEBwezefNmzM3N9dv79OnD559/bpKYhBDiQdI9SZQqOX+0L1y4oN+WkJDAa6+9hr+/P1ZWVgQGBjJx4kSysrL0Zc6fP49Go2H69Ol88sknBAYGYm1tzffff49GoyErK4u5c+fqu1bk+Pvvv+nWrRuVKlXCxsaGBg0a8MMPPxjEtG3bNjQaDREREYwaNQo/Pz+sra05ffq0vpvV1q1bGTJkCG5ubjg5OTFgwADu3r1LQkICvXr1wsXFBR8fH0aPHk1mZqZB/RMnTuTJJ5/E1dUVJycnQkNDWbhwYa6uHjlddDZv3kxoaCi2trbUrFmTRYsW5TqOly9fZujQoQQEBGBlZYWvry89e/YkMTFRX+bWrVuMHj2awMBArKys8PPzY8SIEdy9e7dA79WiRYuoX78+NjY2uLq60r17d2JjY/XPt2nThn79+gHw5JNPotFoGDRokNG6JkyYwHvvvQdAYGCg/n3atm2bQbmC7HtBzhdjEhISWLRoER06dDBIGHLUqFGDMWPGEB0dXWTdi3777Tc0Gg3Lly9n5MiR+Pr6YmNjw/nz50lMTOT111+nVq1aODg44OXlRbt27di1a5fROv7880+D7adPn0aj0bB06VKD7QsXLqRGjRpYW1tTu3btXM8/qv/85z88++yzrFixgri4OP12pRRff/21/lypVKkSL774IufOndOXeeqpp/j11185c+aMQZfFHBkZGUyaNImQkBCsra3x9PTk1VdfJSkpKVccS5cupVmzZtjb2+Po6EjDhg1ZvHjxQ9vJq3vS0aNH6dq1Ky4uLtjY2NCwYUMiIiIMyuS8Bz/99BP//e9/8fHxwdnZmWeffZZ//vnnocfu+vXreHh4GCQMOe5PTgcOHIi7uzvp6em5yrVq1Yr69evrf165ciVNmzbF2dkZe3t7qlWrxpAhQ/TxNm/eHID+/fvrj8X9+75//37Cw8P1n42hoaGsXr3aoM0FCxbof09fffVVXF1dcXZ2ZtCgQaSmpnLlyhV69uyJs7Mzvr6+jBkz5qG/h0KIUkwJYQLff/+9AtSBAwcMts+ePVsB6rvvvlNKKRUfH68CAgJUlSpV1Lx589Rvv/2mJk+erKytrdWgQYP0rzt37pwClJ+fnwoLC1OrVq1SW7ZsUfv371d79uxRgOrZs6fas2eP2rNnj1JKqRMnTihHR0dVrVo1tWTJErVhwwbVp08fBahp06bp6/7jjz/0dffs2VOtW7dORUZGquvXr+v3IzAwUI0aNUpt2bJFTZs2TZmbm6s+ffqo0NBQ9cknn6ioqCg1ZswYBagvvvjCYJ8HDRqkFi5cqKKiolRUVJSaPHmysrW1VRMnTjQoV6VKFeXv769q166tlixZon799Vf14osvKkBt375dXy4uLk75+Pgod3d3NXPmTPXbb7+plStXqsGDB6vY2FillFJ3795VDRo0MCgze/Zs5ezsrNq2bau0Wm2+799nn32mANWnTx+1YcMGtWTJEhUUFKScnZ3VqVOnlFJKHTt2TH300UcKUN9//73as2ePOn36tNH6Ll26pN5++20FqDVr1ujfp5s3bxZq3wt6vhizfPlyBai5c+fmWeb48eMKUG+88Ua+dSml1MCBA5W9vb3KzMw0eGRlZenLREVF6c+tXr16qfXr16v169erGzduqGPHjqk333xTrVy5Um3btk2tX79eDRo0SJmZmakdO3bkqmPnzp0G7f/zzz8KUBEREfpt8+fPV4Dq3r27ioyMVBEREapatWoqICBAVatW7aH71LJlS1W/fv08n//6668VoH788Uf9tldeeUVZWlqq9957T23evFktW7ZMhYSEKB8fH3X16lWllO5cadasmfL399e/93v37lVKKZWVlaXat2+vHBwc1OTJk1VUVJSaP3++8vHxUXXr1lVpaWn6tv773//qf9dzPgO++OILNX78+Ie2k5mZqQA1efJkfX3Hjh1TDg4OKjg4WEVERKjIyEjVq1evXL/HOe9B1apV1YABA9TGjRvV8uXLlb+/v6pVq5bKzs7O97gOGjRIAWrEiBFq37596t69e0bLHTx4UP/7dL8jR44oQM2bN08ppdSOHTuURqNRffv2VRs3blRbt25VixYtUgMHDlRKKXXz5k21YMECBagJEyboj0VcXJx+f6ysrFSbNm3UTz/9pDZt2qQGDBiQ5/kUFBSk3nvvPbVlyxY1ZcoUZW5urvr166caNGigPvvsMxUVFaVGjx6tADV79ux8j4UQovSSpEGYRM6X7b1796rMzEx1+/ZtFRkZqTw8PJSjo6NKSEhQSin12muvKQcHB3XhwgWD18+YMUMB6tixY0qpf5OGatWqGf2DC6g333zTYFvv3r2VtbW1unjxosH2jh07Kjs7O5WSkqKU+jdpaNWqVZ778fbbbxtsf/755xWgZs6cabC9QYMGKjQ0NM/jkp2drTIzM9WkSZOUm5ubwZf3KlWqKBsbG4NjkZaWplxdXdVrr72m3zZ48GBlaWmpjh8/nmc7U6ZMUWZmZrmStlWrVilAbdy4Mc/XJicnK1tbW9WpUyeD7RcvXlTW1tbq5Zdf1m/LKzk05vPPP1eAOnfuXK7nCrrvBT1fjJk6daoC1ObNm/Msk5aWpgDVuXPnh+7PwIEDFZDr0bJlS32ZnC+bbdu2fWh9WVlZKjMzU7Vu3Vq9+OKLuep4WNKQlZWlvLy8VNOmTQ3OqzNnzigLC4siSRrWr19v8IV6586dRr8onj9/XllbW6uxY8fqt3Xo0MFoDBEREQpQv/zyi8H2vXv3Glxg+Oeff5SZmZn+i3Fe8mrHWNLQs2dPZWNjo/8ynePZZ59VDg4O6tatW0qpf9+Drl27GpTLSUQfdv5fvXpVtWjRQn+OWFlZqZYtW6qpU6eqO3fuGJRt2bKlaty4scG2IUOGKBcXF3X37l2l1L/n8oOvvV/OxZT7k4AcwcHBqkmTJgYJrlJKPffcc8rPz09//uQkDSNHjjQoFx4ergA1Z84cg+116tRRTZs2zfdYCCFKL+meJEyqWbNmWFpa4ujoSHh4ON7e3mzatAkvLy8AIiMjCQsLw9fXl6ysLP0jZ2zC9u3bDerr2rUrlpaWBWp769attGvXLtfAx5xb63v27DHY/sILL+RZ14Mz+9SqVQuAzp0759p+f9ernDjat2+Ps7Mz5ubmWFpaMm7cOK5fv87Vq1cNyjZo0IDKlSvrf7axsaFGjRoGdW7atImwsDB9DMZERkZSp04dGjRoYHBcO3ToYLRb0P327NlDWlparq5GAQEBtG3blt9//z3P1z6Ogux7Yc+XR5XTxU2r1Rq0k52dbVDO1taWAwcOGDwWLlyYq768zq1vvvmGhg0bYmNjg4WFBZaWlmzfvt2gG1hBHT9+nMTERF5++WWDLnpBQUE8+eSTha7PGPVAl7rIyEjMzMzo27evwXHy8/Ojbt26+Z5n99fh5uZGp06dDOpo1KgR7u7u+jq2bNmCVqvlzTffLJJ9Ad3v5rPPPoufn5/B9oEDB3Lnzh327dtnsL1r164GP+cMMH7wd/5BHh4e7Nq1i/379zNlyhS6dOnCiRMn+OCDD6hXrx43btzQlx0+fDh//fWXvu2UlBSWLVvGK6+8gp2dHYB+/FDPnj356aefuHLlSoH3+cSJE5w+fZq+ffuilDI45p06deLy5cucPn3a4DWP8/knhCg7JGkQJrVkyRIOHDjA4cOHuXLlCjExMbRs2VL/fGJiIuvXr8fS0tLg8cQTTwDk6tPs4+NT4LavX79utLyvr6/++YLW7erqavCzlZVVntvv74+8f/9+nn32WQDmz5/Prl27OHDgAB9++CEAaWlpBq93c3PL1ba1tbVBuWvXruHv759nrKA7rjExMbmOq6OjI0opo33Fc+Qcl7yO3YPHragUZN8Le77cLychub+v/YNynstJNCdNmmTQzoOzKpmZmdG4cWODR0hISK56jR3L6dOn89Zbb9GiRQtWr17N3r17OXDgAM8880yu86Igct4Xb2/vXM8Z2/Yocr4Q5vwOJSYmotVqcXd3z/We/PXXX/m+HzkSExO5fv16rtdbWlqSlJSkr+PatWsADz33CyM5OblQnxEPnqPW1tZA7t/jvDRp0oQPPviAVatWER8fzzvvvMPZs2eZMWOGvkz37t0JCAjgm2++AXRji9LS0njjjTf0ZcLCwlizZg0ZGRn0799fn6T99NNPD40hZ9zTiBEjch3vnBmnHnzfHvXzTwhRtpTO6TlEhVGrVi397EnGuLu7U69ePT799FOjz+f88c5x/xXUh3FzcyM+Pj7X9pyrcu7u7o9cd0GtWLECS0tLIiMjsbGx0W9fu3btI9fp4eFhMBDVGHd3d2xtbY0OJM55Pi85X4zyOnb5vba4FfZ8uV9YWBgWFhasXbuWYcOGGS2T8760bdsWgKFDhxpcZc35klhYxs6tpUuX0r59e/2Xwxy3bt0y+DnnvMnIyDDY/uAXu5z3LSEhIVdbxrY9inXr1mFmZkarVq0A3fthZmbGn3/+afQO4P3nfF7c3d3x8vLKc/C5k5MToDvvQTeVaGEuHuSnUqVKhfqMKEo5dxznzJnD33//rd9uYWHBG2+8wYQJE/j888+ZO3cuHTp0yLXORvfu3enevTsZGRns3r2bKVOm0Lt3bwIDA2nSpEme7ebs08cff5zrzkmOsjxlsBDi0UnSIEq18PBwNm7cSLVq1ahUqVKR1t2uXTt+/vlnrly5YvBlcsmSJbmmlSwuObO33D9rSlpaWq7ZWQqjY8eOREREcPLkSaNXtUF3XD/77DPc3NwIDAwsVP3NmzfH1taWpUuX8uKLL+q3x8XFsXXrVnr27PlIcRf2qqwxj3O+eHt78+qrrzJv3jxWrlyZawalU6dOMW3aNAIDA+nWrRugS0LyS0Qeh0ajyZWEHD58mAMHDhi8ZzlrjsTExNCuXTv99nXr1hm8tnbt2nh6erJ8+XKGDx+u33727Fn27dtHlSpVHiveBQsWEBUVxYABA/THJDw8nBkzZhAfH0+PHj3yff2Dd41yhIeHs2rVKjQaDY0aNcrz9R06dMDMzIy5c+fm+6U4r3aMadeuHRs2bCAxMVHfZRJ0nxEODg5FNo1wfHy80UQnpxvag+fY0KFDmTRpEn369OH06dP5LlZpbW1NWFgYTk5OREVFcfjwYZo0aZLn71vt2rUJDAwkOjqaSZMmPe6uCSHKEUkaRKk2adIkoqKiaNGiBe+88w4hISGkp6dz/vx5Nm7cyLfffvvI3RHGjx+v7wM/btw4XF1dWbZsGRs2bGD69Ok4OzsX8d7k1rlzZ2bOnMnLL7/M0KFDuX79OjNmzHjkK9agO2abNm2iVatWjB07lrp165KSksLmzZt59913qVmzJiNGjGD16tW0atWKkSNHUq9ePbRaLRcvXmTLli2MGjUqz37uLi4ufPzxx4wdO5YBAwbQp08frl+/zsSJE7GxsWH8+PGPFHfdunUBmD17NgMHDsTS0pKQkJBCLRT2uOfLzJkzOXHiBP369WPHjh106dIFa2tr9u7dq+8isnbt2gKPm3kc4eHhTJkyhUmTJvHUU09x4sQJJk2alGthQn9/f9q0acMnn3yCk5MTlStXJioqKtfdKnNzcyZNmsSwYcN44YUXePXVV7lx4wYTJkwoVPek1NRU9u7dC+i+cJ45c4aff/6ZjRs30rZtW4M7I61bt2bw4MEMGDCAffv20apVK+zs7IiPj2fnzp00bNiQoUOHArr3f926dXz33Xc0aNAAc3NzGjVqRN++fVm+fDkdOnRgxIgRNG7cGAsLCy5fvqxPUrt06UK1atUYM2YMU6ZMITU1lV69euHk5MSxY8dISUnRn5d5tWPMhAkT2LRpE23atOHjjz/GxcWFiIgIfv31V7744otHWsTOmPbt21O1alW6dOlCSEgI2dnZREdHM2PGDBwdHXMtROfq6kq/fv2YP38+QUFBudafGTt2LImJibRr1w4/Pz+Sk5OZNWsWVlZW+rtA1atXx8bGhoiICGrUqIG9vT1+fn74+Pjw3Xff0blzZzp27KhPApOTkzl+/DhHjhxh5cqVRbLfQogyxsQDsUUFVZhZda5du6beeecdFRgYqCwtLZWrq6tq1KiR+vDDD/Wzg+TMnvT5558brQMjsycppdTRo0dVly5dlLOzs7KyslL169fPNZ1hzuxJ/+///b8C78f48eMVoK5du2awPWcazvstWrRIhYSEKGtraxUUFKSmTJmiFi5cmGsmoSpVqhidtad169aqdevWBtsuXbqkBg8erLy9vZWlpaXy9fVVvXr1UomJifoyd+7cUR999JEKCQlRVlZWytnZWdWtW1eNHDlSP3tVfhYsWKDq1aunf223bt1yzU5UmPdZKd2Umb6+vsrMzEwB6o8//ij0vhfkfMnPvXv31FdffaWefPJJ5eDgoJ/RpkWLFrlm0cmPsff6QTmz7vz888+5nktPT1fvvvuu8vX1VTY2NqpRo0Zq3bp1qm/fvrlm/7l8+bLq0aOHcnV1VS4uLmrAgAFq3759RmfH+e6771RwcLCysrJSISEh6ocffjBapzEtW7Y0mAnK3t5eVatWTb344otq9erVRqfq1Wq1av78+app06bKzs5O2draquDgYDVw4EB16NAhfbnr16+rHj16KBcXFwUoc3Nz/XP37t1T06dPV/Xq1VM2NjbKwcFB1axZUw0bNizXNL6LFy9WjRs3VjY2NsrR0VGFhoaqH3744aHtGJs9SSnddKbh4eHKyclJWVtbqwYNGqglS5YYlMnrfTQ27a0xP/74o+rTp48KDg5WDg4OytLSUlWuXFkNGDBAP03yg3777TcFqBkzZuR6bt26deq5555Tvr6+ysrKSnl6eqrw8HC1a9cug3JLly5VISEhytLSMte+Hz58WPXs2VN5eHgoS0tL5ePjo9q1a6fmz5+vL5Mze9Lhw4cN6v3www8VoJKTkw229+3bVzk7O+d7LIQQpZdGqQemuxBCCKGXmZlJly5d2L17N1FRUUU205AQj2P48OEsWLCAuLi4Iu+6KYQQxsjsSUIIkQ9LS0tWrVpFSEgIHTt25MiRI6YOSVRge/fuZfHixcybN4/XX39dEgYhRImROw1CCCFEGZCVlYWlpSV2dnaEh4fz/fff69dmEEKI4iZJgxBCCCGEECJf0j1JCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQgghhBD5kqRBCCGEEEIIkS9JGoQQQhSbZs2a8cEHHxS4/ObNm9FoNKSnpz9Wu97e3nz77bePVUdZ8ODxLYr9rijHTghROJI0CCFEOaHRaPJ9DBo0yNQhlmqpqam8/vrruLm54eDgQI8ePYiPjy9UHd9++63+eJuZmeHr60ufPn24dOlSMUVt6OjRowwcOLBAZb/99lu8vb0fq47HMX78eJo1a4atra3ROIQQpYskDUIIUU7Ex8frH7NmzcLJyclg2+zZs42+LjMzs4QjLZ3efPNNNm3axKpVq9i+fTvXrl3j+eefRylVqHo8PDyIj48nLi6OiIgI9u3bl289RXn8PTw8sLW1NXkdBZGZmUnv3r0ZMmRIsbclhHh8kjQIIUQ54e3trX84Ozuj0WhybTtx4gQajYY1a9bw9NNPY21tzapVq/jggw9o1qyZQX1Tp06lZs2aBtvmzZtHSEgINjY21KpVi/nz5xcqxkWLFhEaGoqDgwM+Pj4MGDCApKSkXOW2bdtGnTp1sLGxoUWLFsTGxho8v2PHDlq2bImtrS2VK1dm1KhRpKWlFSqW+yUlJREREcHs2bMJCwujUaNGREREsH//fnbs2FGouszMzPD29sbX15d27drx4YcfcujQIS5evEh6ejoajYaFCxfSuXNn7OzsmDFjBqC7wt+hQwfs7e3x8fFh8ODBJCcn6+u9ffs2L7/8Mvb29vj5+fHVV1/lavvBrkXXr19n8ODBeHp6YmtrS7169fj111/ZvHkzr7/+OomJifo7I1OnTjVax9mzZwkPD8fe3h4XFxdefvllg/cs59xZtGgRlStXxsXFhf79+3P37t18j9Nnn33GiBEjqF27dqGOrxDCNCRpEEKICmjMmDGMHj2aEydOEBYWVqDXfPXVV3z66adMnz6d2NhYJk2axHvvvcfKlSsL3G5WVhZTpkwhJiaG1atXExsby9ChQ3OVe//995kzZw779+/HycmJbt26kZ2dDcDBgwfp1KkTffr04ejRoyxbtoyoqCjefffdPNvt3bs3zz33XJ7P79+/n+zsbJ599ln9tqpVq1KjRg12795d4P0zJueq/f13FD766CNeeukljh07Rt++fbl06RKtW7emefPmHDp0iMjISM6ePUvfvn31rxk+fDh79uxh/fr1bNq0icjISI4dO5Znuzn7c/jwYX788UeOHTvG5MmTMTMzo23btkybNk1/VyQ+Pp63337baB1dunQhNTWVP//8k02bNnHs2DH69etnUO748eNs2bKFTZs28fPPP7N582Zmzpypf/7bb7/FxsbmkY+hEML0LEwdgBBCiJI3evRounXrVuDySik+/fRT5s6dq39dYGAgR44cYd68ebz00ksFquf+BCEoKIiZM2fSpk0b7t27h5WVlf65yZMn07ZtWwCWLFlCQEAAGzZsoGvXrkybNo1XX32Vt956C4Dg4GBmzpxJp06d+Oqrr7CwyP2nzc/PD0dHxzzjSkhIwNHRMVe3HC8vLxISEgq0b8ZcuHCBmTNnEhgYSFBQEPfu3QNg0KBBDBgwQF/u/fff5+mnn2bChAn6bQsWLKB69epcvHgRBwcHlixZwurVq/XHJSIigoCAgDzb3rBhAzExMZw6dYrAwEBAd8xzODk56e+K5GXjxo2cPn2a33//XV/u+++/p1GjRhw9epS6desCuvE0ixYtws7OjieeeII+ffrw+++/8/HHHwNQqVIlQkJCCnPohBCljCQNQghRATVu3LhQ5ePi4khMTKRfv35oNBr99qysLLy8vApcz4EDB5g4cSIxMTEkJyej1WrRarXExcUZfKFt3ry5/v+enp4EBQURGxtL165dOXjwIJcvX2bhwoX6MkopMjMzuXTpkv4L8v2++OKLh8Z2/37dX6+x7fm5evUqDg4OaLVa0tLSaNKkCWvWrMHM7N+b+w8e/4MHD7Jz504cHBxy1XfmzBns7OzIzs42elzyEh0dTVBQkNHjUVCxsbEEBQUZJBahoaHY2toSGxurTxqCg4Oxs7PTl/Hx8eG3337T//zSSy8VOLEUQpROkjQIIUQFZG9vb/CzmZlZroG693en0Wq1APzwww80aNDAoJyxK/vG3Lx5kw4dOtCtWzeWL1+Oh4cHp06domvXrvor8PnJ+fKu1Wp5++23ee2113KV8ff3L1AsD/L29ubWrVukpaUZ3G24evVqoZIiADc3N/bs2aO/in//l+kcDx5/rVZLz549mTRpUq6yvr6+xMTEFCoGoEgGM+eVND243dLS0uB5jUHFJ6IAACAASURBVEajP2eEEOWDJA1CCCH0fdvvFx0drf9/QEAA7u7unDt3jp49ez5SG3///TfJyclMnz4dDw8PAHbu3Gm07N69e+natSug++J+9uxZ/aDs0NBQjh8/TnBw8CPFYUzTpk0xNzcnKipK3+6FCxc4deoULVq0KFRd5ubmhY4tNDSUqKgogoKCDO5I5KhRowbm5uZGj0te6tWrx9mzZzl//jxVq1bN9byVlZV+nEheateuzZkzZ0hISNDfbTh06BDp6enUqlWrEHsohCjrZCC0EEII2rZty6VLl/jyyy85ffo0s2bNYuvWrfrnzczMGD9+PJMmTeKbb77h1KlTxMTEsHDhQqOz+BhTtWpVLCwsmDNnDufOnWPNmjX6GXseNG7cOLZt26ZfM6By5cp06tQJgLFjx/Lbb78xcuRIjhw5wqlTp1i7di0jR47Ms+1Ro0blO7Wnu7s7/fv3Z/jw4Wzbto1Dhw7Rv39/mjRpQqtWrQq0f49j+PDhxMXF0a9fP/766y/OnDnD5s2bGTx4MACurq7079+fkSNHsm3bNmJiYhg0aJDBOJAHPfvsszRp0oTu3buzdetWzp07x4YNG/TdhqpWrcqNGzfYuXMnSUlJRmef6tSpE8HBwfTt25fo6Gj27NnDK6+8QocOHahTp06B92/lypXUr1/fYNuFCxeIjo4mLi6OrKwsoqOjiY6OJjU1tcD1CiFKjiQNQgghqF+/PrNnz+aLL76gYcOGHD16lOHDhxuUeeutt/j666/57rvvqFu3LmFhYSxdurTAfeb9/PxYsGABS5YsoVatWsyaNUs/3eiDpkyZwhtvvEHjxo1JSUlh7dq1+m5QjRo10n9xbtmyJY0aNWLixIn4+fnl2fbly5cfusDaN998Q4cOHejRowdPP/00bm5u/PLLLwbdcLy9vfNMdB5H5cqV2bVrF3fv3qV9+/bUrVuXd999Fzc3N32Z2bNn06RJEzp16kSHDh3o0KEDTzzxRJ51ajQafvnlF+rVq8eLL75I7dq1GTt2rL4bWlhYGIMGDeL555/Hw8PD6Doe5ubmrF+/HhsbG1q2bMlzzz3HE088wdKlSwu1f8nJyZw8edJg25gxY2jYsCGffvop169fp2HDhjRs2PCRumIJIYqfRhV21RohhBCiHGjWrBlt2rQpcBKwZs0aXnjhBf744w/atGnzyO16e3szYcIEhg0b9sh1lAW9e/cGYMWKFUDhj7cxRVGHEOLRyJ0GIYQQxS5nAbG8HoMGDTJ1iA+VM8bjwUXwSsI333xD69atcXR0RKPRkJ6eXug6Nm/ebHDMPT09CQ8Pz3eth6K0ceNGPvroowKVzYn1wf0sTB2PIzU1lddffx03NzccHBzo0aNHrjE/QlQ0kjQIIYQodjkLiMXHxzNr1iycnJwMthnrGgOGMziZWmEHRBeltLQ0OnfuzPvvv//YdZ0/f574+Hh++eUX4uPj6dixI3fu3DFatiCzWhWUq6ur0SllS7qOgnjzzTfZtGkTq1atYvv27Vy7do3nn38+1wxjQlQkkjQIIYQodt7e3vqHs7MzGo0m17YTJ06g0WhYs2YNTz/9NNbW1qxatYoPPvgg19X9qVOn6mdTyjFv3jxCQkKwsbGhVq1azJ8/v1AxLlq0iNDQUBwcHPDx8WHAgAEkJSXlKrdt2zbq1KmDjY0NLVq0IDY21uD5HTt20LJlS2xtbalcuTKjRo0yOsi4MEaPHs37779PkyZNHqse0C1Y5+3tTfPmzZk+fTqXLl3i4MGDgO59mjZtGv369cPR0VG/SvSFCxfo2bMnzs7OuLu706NHD4MxIllZWbz99tv6543dDWjWrBkffPCB/ue0tDTeffdd/Pz8sLa2pkaNGkRERHDixAk6duwI6KaN1Wg0+q5cD9aRlJTEyy+/jLOzM/b29oSHh3Pu3Dn9899++y3e3t5ERkYSEhKCo6Mj4eHhXLt2Lc/jk5SUREREBLNnzyYsLIxGjRoRERHB/v372bFjx6McciHKBUkahBBClCpjxoxh9OjRnDhxgrCwsAK95quvvuLTTz9l+vTpxMbGMmnSJN577z1WrlxZ4HazsrKYMmUKMTExrF69mtjYWIMVrHO8//77zJkzh/379+Pk5ES3bt30U5cePHiQTp060adPH44ePcqyZcuIiori3XffzbPd3r1789xzzxU4zqKUs5bD/Xd0pk6dSpMmTYiOjub999/n9u3btGnTBg8PD3bt2sX27duxsLCgc+fOZGVlAfDZZ5+xfPlylixZwo4dO7h48SIbN27Mt+3evXvz888/M3fuXGJjY/n666+xtbWlevXqLF++HPj3rsj06dON1tG3b1+OHTvGpk2b2LVrF2lpaYSHhxtMJZuSksLXX3/Njz/+yB9//MHJkycNEo+crlA5K3/v37+f7Oxsnn32WX2ZqlWrUqNGDXbv3l2YwytEuSLrNAghhChVRo8eTbdu3QpcXinFp59+yty5c/WvCwwM5MiRI8ybN6/AKxHfnyAEBQUxc+ZM2rRpw7179wymNp08eTJt27YFYMmSJQQEBLBhwwa6du3KtGnTePXVV3nrrbcA3UrJM2fOpFOnTnz11VdGF8Lz8/PD0dGxwPtbVK5du8Ynn3yCi4sLoaGh+u3PPfecwcxZ//d//4ezszNz587Vb/vhhx9wdnZm9+7dtGrVitmzZzNu3Dj98Z8/fz6//vprnm0fPXqUdevWsXPnTp566ikAg9WtK1WqBOjuitjY2ORZx5YtWzh48KA+/mXLllG5cmU2btxIly5dAMjIyGDhwoX62bVef/115syZo6/HwcGBkJAQ/XuTkJCAo6NjrsXxvLy89ImFEBWRJA1CCCFKlcaNGxeqfFxcHImJifTr189getSsrKxCreZ84MABJk6cSExMDMnJyWi1WrRaLXFxcQZfaJs3b67/v6enJ0FBQcTGxtK1a1cOHjzI5cuXWbhwob6MUorMzEwuXbpkdHraL774olD7+7jc3d0BuHv3LjVr1mTVqlW4urrqn3/w+B88eJBjx47lGkuQlZXFmTNnCAkJ4caNGwbHxdra2iARedDhw4f107g+qtjYWGxtbQ3a8fb2plq1asTGxuqTBldXV4PpeH18fLh69ar+56eeeooTJ04Y1F2QVbCFqGgkaRBCCFGq2NvbG/xsZmaWawDq/d1ptFotoLv63aBBA4Nyxq7sG3Pz5k06dOhAt27dWL58OR4eHpw6dYquXbsWaDBwzpdJrVbL22+/zWuvvZarjL+/f4FiKW779+/H2toaT09Po3c4Hjz+Wq2W5s2bs2jRolxlPT09H2kxtgev4j+KvAYlP/jl3tLS0uB5jUajP2eM8fb25tatW6SlpRnEefXq1UIloUKUN5I0CCHE49BmQ1YaZKVDZipkpv3v5wxQ2aAUKC3wv3+1wAUFGo3hw8ICrK1zP2xsdP9aWenKVUAeHh65prvMmf4UICAgAHd3d86dO0fPnj0fqY2///6b5ORkpk+fjoeHBwA7d+40Wnbv3r107doV0H2RPHv2rH5QdmhoKMePHyc4OPiR4igJQUFBeXb5MSY0NJSNGzfi4+OTK6EAcHJyolKlSuzdu5emTZsCulmXDh8+nOd6FvXq1SM9PZ1du3bpuyfdL6c72P1jEx5Uu3Zt0tLSOHTokP5uQ0JCAmfPnqVWrVoF3r8HNW3aFHNzc6KiovTv84ULFzh16pRJZ9ASwtQkaRBCiLxk34O0G5Ce8r+EIPV/yUHav//PzihcncoM/sn7KmeeNBpd4mBjA46OuR8uLrrny6G2bdvy7rvv8uWXX9KlSxciIyPZunWr/qqvmZkZ48eP57///S92dnY888wzpKenc+DAAVJTU/UzAOWnatWqWFhYMGfOHAYPHszhw4fzXEBs3LhxODk54ebmxvvvv0/lypXp1KkTAGPHjqVFixaMHDmSQYMGYWtry/Hjx9m+fTtffvml0fpGjRrFrVu38p3tKT4+nsTERM6ePQtATEwMVlZWVK1aFRcXl4fu3+MYOHAgX375Jd27d2f8+PH4+vpy/vx5Vq9ezbhx4/D09GT48OFMnjyZqlWrUr16daZNm5bvHYiQkBB69+5N//79mTNnDnXq1OHcuXMkJyfzwgsvULVqVQAiIyNp27YtdnZ2uRKWunXr0qFDB1555RW+/fZbbG1tGTVqFNWrV9fPvlQQf/75J//5z3/4888/cXd3x93dnf79+zN8+HCcnJxwcnJixIgRNGnShFatWj3SMRSiPJDZk4QQQml1icGNM3D5AJz+FY7+CNGL4eQ6uLADrvwF145D8lm4Ew8ZNwufMDxWjAoyMuDmTYiLg9hY2L8ffv8d1q6FxYshIgLWr4cdOyAmBuLjoRStc/Co6tevz+zZs/niiy9o2LAhR48eNRioC/DWW2/x9ddf891331G3bl3CwsJYunSp0TEExvj5+bFgwQKWLFlCrVq1mDVrFjNmzDBadsqUKbzxxhs0btyYlJQU1q5dq+8G1ahRI7Zt20ZMTAwtW7akUaNGTJw40aBP/YMuX75sMH2pMbNnz6Zhw4a8+eabADz55JM0bNiQzZs368s0a9asWFaZdnJyYufOnXh6etKtWzdq1arFkCFDyM7O1n+RHzt2LL169aJfv3489dRT+Pj46BOpvCxYsIAuXbowZMgQatWqxeuvv65fzC0oKIgPP/yQd955By8vL0aNGmW0jqVLl/LEE0/w3HPP6ae5jYyMxNzcvMD7d+fOHU6ePKmfCQp0i+l16NCBHj168PTTT+Pm5sYvv/wiYxpEhaZRslKJEKIiyUqH1CTdHQT9I1nXlagkKDM49Ah3Gh6VRgOVKoGnJ3h46P6tVAnM5JpReaKUwtfXly+//JLevXubOhwhRDkk3ZOEEOWbNhvuJMDty3ArDlKvAxXoWolScOOG7pEzQ4yFBbi76xIIT0/w9dV1exJl1pEjR/Dz8yvw9LJCCFFYcqdBCFH+pF7/N0m4kwDarIe/pqSU9J2GgtBodEmEnx/4+4O3t9yJEEIIYUCSBiFE2Xfv7r9Jwq3LutmLSqvSmDQ8YHdwGEccq1PXC+p6grPchBBCiApPuicJIcqmjFtw/R/dwOT0ZFNHU24o4NcMfxJSIToRNEAVF2jsC419oNLjT68vhBCiDJKkQQhRdmTf0yUJ10/puh2JInfP1YOE7H8zAwWcT9E9Vh+HYFdo4guNfMGhfM7wKoQQwghJGoQQpZvS6rodXT8FKRdKbpajCuqCYwDkMUurAv65oXusOAY13XUJRKgP2MhfEyGEKNfkY14IUTqlJukShRtnSvcYhXJmryagQOW0Co5f0z1W/K1LHp6uAlWLd50xIYQQJiJJgxCi9MhKh6STumRBximUOGVtzZ4MT91AhkLIyIY/L+keAU7Qqgo86QfW8hdGCCHKDZk9SQhhevfuQGIMJJ0oXdOjFodSPHvSdd9qjKVdkdRlawHNA6BNFfByKJIqhRBCmJBcBxJCmE5aMiREQ/IZ3dgFYVLHrAMgo2jqSsuCrefgj3PQ0BueC9bNwiSEEKJskqRBCFHy7iTqkoWbF0wdifgfBWzNKth4hsLWeyhB96jlDh2DIcS9yJsRQghRzCRpEEKUnJsXIeEI3Ik3dSTiAfcqeRCfXbyLMMQm6R6BLro7D/W9dItRCyGEKP0kaRBCFC+l1a2tkHAE0q6bOhqRh4tOeU+1WtTOpcDcv8DXEbrU0E3ZKoQQonSTpEEIUXxSzkPcPsi4aepIxEPsNSv6rkkPc+U2zDuom6a1e03dug9CCCFKJ5k9SQhR9NKSIW6PblE2YagUzp6krK15w3UAWhP3FarRIJHunk4EWRVvNykhhBCFJ3cahBBFJysD4g/C1WPohsCKsuCGmz/awi7OUMQCPO4RmZ3AxvgEOjq4MtjFG1dzS5PGJIQQ4l+SNAghHp9SkBQLV/7SLdAmypTjRTjV6qMw0yhOuV8EQAtsuHODrXdTeNnZk15OHlhpzEwXnBBCCADkk1gI8XhuX4HYNXDxT0kYyiAFbM32N2kM/j7pXLK4a7AtTWlZmJJA/8sn2HpXVgcXQghTkzsNQohHc+8OxO3VzYwkyqzMSu5cybIzWfvW5opDLnmv13E1O5PJSRfZlnqTka5+VJIuS0IIYRKSNAghCkebDQmHdVOoqmxTRyMeU0lOtWqMh+9djpo/vG/UztSbxKTf4R1XP9raVyqByIQQQtxPkgYhRMGlJcO5rbLeQjmyzwRTreZwsFLsdSr4quA3tdlMTrrIjtSbjHD1x8Vc/oQJIURJkU9cIcTDKQXXjunWXJC7C+WGsrJmV4anydp38Esh1Syr0K/bnnqTI+l3GeHqR2t7l2KITAghxINkILQQIn+ZqXB6E1zaLQlDOZPs5ke2if4MuNpp2W1/6ZFfn6LNYkLSBSZdu8DN7MInHkIIIQpHkgYhRN6Sz8Gx/yeLtJVTx21M1zVJ+SaRpXn8tTz+SE3hlSsn2ZGaUgRRCSGEyIskDUKI3LIz4fx2OBsF2SacwF8UGwVszTJN0uDllM1+m/giqy9Zm8X4axeYfO0Ct+SugxBCFAtJGoQQhu4kQuxquH7S1JGIYpTp4sblbNNMtXrTOwFVDAtQb01N4ZX4kxxJv1P0lQshRAUnSYMQQkdpdSs6n1wHGbdMHY0oZhedTXOXwd8tkxjrpGKr/0Z2FqMTz7LudvG1IYQQFZHMniSEgMw0XVekOwmmjkSUkP1mlUu8TQ2K8x6Xi72dLBRf3rjM6XvpvOPqh4WmGG5rCCFEBSN3GoSo6FKT4MTPkjBUIMrKij9NMNVqgNc9zlreLLH21t+5zqjEM6TIOAchhHhskjQIUZEln9V1R7onfcArkmQ3/xKfatXCTPG3W8EXcisqMRl3GRZ/in/upZZ420IIUZ5I0iBERaSUbvzC2d9AK1dhK5pY25Ifz+Djk0aCeVqJtwuQmJ3JOwln2Ho32STtCyFEeSBJgxAVjTZLlyzEHzJ1JMIEFLA1s2STBlsLxV8uJX+X4X7pSsvkpIvMT45Hqx5/fQghhKhoJGkQoiLJSodTkZByztSRCBPJdHEjroSnWq3kd5ubZvdKtM28LL91lY+uneeuVlY3F0KIwpCkQYiKIv0mnFgLd6+aOhJhQpdKeKpVJxvFHkfT3mV40J60W7wR/w9xmbJwoRBCFJQkDUJUBHcSdAmDrL9Q4e0zK9mkwdr3BhkabYm2WRAXszJ4K+EfztwzzTgLIYQoayRpEKK8Sz4HpzZAtlxVreiUlRW7MrxKrD0PBy177OJKrL3CuqnN5t3EMzKzkhBCFIAkDUKUZ8lndYOelfTfFpDi5kdWCX7sp/lcRVvK11W7pc1mVOJZTmZI4iCEEPmRpEGI8ir5HJz9Hd18OUJArE3JdU3ydcnmkE1iibX3OG5rsxmVeIbjGXdNHYoQQpRakjQIUR6lnIdzkjAIQ1uzSi5pSPC+XGJtFYW7Sst7iWc5mi6JgxBCGCNJgxDlTcr5/3VJKn2DT4Xp3HNx41K2fYm0FeBxjxOWZW8htVSlZczVsxxJlxXShRDiQRamDkAIUYRSLkjCUFqYW4OlHZhZgMbs3wfm4K/Vrcqt1f77yMyE1FS4VzzrGcQ5B0BmsVRtwEyj+Mf9UvE3VEzSlJYPrp7jM89AGto4mDocIYQoNSRpEKK8uHlREoaSYu0M1o66pED/sAdL23//Ncvn47VGPnVnZemSh/sfd+/++//bt+FW4afO3V9CU636+6Tzm0XZvlKfrrT89+pZPvEIpLGto6nDEUKIUkGjlJJOz0KUdTcvwZktMktScbBxATt3sPP4379uYG5l2pju3YOkJLh2TfdvUhLcvJlncWVlxVvuA4p95iQrc8WVkJPcMC8f0/taaTRM8qjKk7ZOpg5FCCFMTpIGIcq6W3Fw+ldJGIqClSM4eJWuBKGgHkwkEhJ0dyiAZJ9APtA8U+wh+AfcYavLmWJvpyRZouETz6o0lcRBCFHBSdIgRFkmCcPjs/cClyrgXAVsK5k6mqKVlAQXLxKT6co316oWa1MOVoqT1Y9z1yyrWNsxBTuNGV95BxNkZWvqUIQQwmQkaRCirEq7ASd+AW0JjG4tT8wswNEPXKqCc4BuPEIFkJwORxPhSCKcSIKsIh764h2YzA6Hi0VbaSniZW7J//lUx9Xc0tShCCGESUjSIERZlJUOsT/DvdumjqRsMLeGSoG6RMHRN/9ByhVAehbEJkFMAhxK0P38OFzttPwV9DdZmvL956SWlR2zvKthpZHZyoUQFY8kDUKUNUoLpyLhToKpIyn97NzB4wlwrVbhE4W8pGfBvsuw/TxcfsQctFLwVfbZxhdpXKVVGztnxrlXQaPRmDoUIYQoUZI0CFHWnN8O10+aOorSS2OuSxI8aoO9p6mjKVNO34Bt5+FQPGQX8C+Dl1M2f1b+G1WBvkP3d/ZksIuPqcMQQogSJZfehChLEo9KwpAXayddouBWAyxsTB1NmRTsqnvcyoBdF2H7Bd1YiPzc9E6oUAkDQMTNqwRY2PCMQzkbOC+EEPmQOw1ClBU3L8HpzYD8yhpw9AOveuDkD9JlpEhla+Hvq7DlrO4uxIP83LL4w/dYyQdWClii4QuvatS1sTd1KEIIUSJkNJcQZUF6Cpz7HUkY7mPnATXCoUZn3SxIZSRhmDJlChqNhhEjRhSo/IoVK9BoNDz//PMG22fMmIGXlxdeXl58+eWXBs/t27ePRo0akZ39eFPxmptBfW94rwW81RT871scWYPigkfcY9VflmWiGHftPPGZ5WMhOyGEeBi50yBEaZeVDifWQsYtU0dSOlg7g19T3WxIZcyBAwfo1asXTk5OhIWFMWvWrHzLX7hwgZYtWxIUFISrqytr164F4OjRozz55JNERkailCI8PJwDBw5Qp04dMjMzadq0Kd999x1NmjQp0vi1Cg5chl9Ogr1TBr95nijS+suiKpbWfO1dHQczc1OHIoQQxUruNAhRmiktnP1dEgYAS3uo0gqeeLFMJgx37tyhb9++zJ8/n0qVHt4XPjs7m759+zJx4kSCgoIMnouNjaVevXq0bduWdu3aUa9ePWJjYwH4/PPPadWqVZEnDABmGnjSHyaFKRrXSMVFZqTiQmYGE6+dJ1uuvwkhyjlJGoQozS7thtuXTR2FaZlbg9+TUOclcK8JZXSO/DfffJPOnTvTvn37ApWfNGkSHh4evPrqq7meq1u3LqdOneLixYtcuHCBU6dOUadOHU6fPs3ixYv55JNPijp8AxZmGjo4V2KZX01ecfbCroy+J0Xlr/Q7/HBTpkAWQpRvcplIiNIq+RxcO27qKEzL4wnwbQwW1qaO5LGsWLGCgwcP8tdffxWo/K5du1i4cCHR0dFGn69VqxafffYZzzzzDKAbJ1GrVi3at2/P9OnT+fXXX5kwYQKWlpbMnj2bVq1aFdm+3M/OzJwBLt50dXRnfko8G+8YGS1dQSy7eZUnbZ14wloGRgshyidJGoQojTJT4eJOU0dhOtZOUKU1OJb9ufAvXbrE8OHD2bJlCzY2D58K9vbt2/Tr14/58+fj7u6eZ7lhw4YxbNgw/c+LFy/G0dGR5s2bExISwoEDB4iLi6N3796cO3cOa+viS7xczC14zy2A1nbOzLgex7XszGJrq7TSAp8lXWSBTw1sZXyDEKIcqtj3lIUorc5v1w2Arog860CtF8pFwgBw8OBBrl69SqNGjbCwsMDCwoLt27czZ84cLCwscs1wdObMGc6fP0+XLl305ZcsWcK6deuwsLDgzJkzAMydO5d69erh5OSEg4MDw4YNo0ePHuzbt48aNWpQvXp1wsLCyMzM5NSpUyxevBiNRpPrkZ7+73m2bNkyAgICcHV15b333jOI6/z589SoUYNbt/IeX9PU1olFviF0cnAtwiNYdlzJusfXyVdMHYYQQhQLudMgRGlzLRZuXTJ1FCWvHN1duF+7du04evSowbZXXnmFmjVrMmbMGMzNDa9K16xZM1f5jz76iNu3bzN79mwCAgIA8Pf3Z+rUqQQHBzN69GjS09N59dVX+fzzz8nM/PdKf1ZWlj4xcXJy4uRJw8UBc+5+JCUl8Z///IfFixcTFBRE586dadOmDZ07dwbg9ddfZ+rUqTg5OeW7vw5m5hX6rsPGOzdobuvEU3bOpg5FCCGKlCQNQpQmGbcgbo+poyh5nnXAtwmYW5o6kiLn6OhInTp1DLbZ29vj5uam3z5gwAD8/PyYMmUKNjY2ucq7uLgAGGzv0qULAFFRUSQmJrJnzx7c3d3JzMzkxIkTbNq0iUuXLmFubk5ISAjR0dFoNBq8vb2Nxnn27FmcnZ156aWXAAgLC+P48eN07tyZ5cuXY2VlRY8ePQq83zl3HeYmX6lwYx2+uB5HbWs7XMvh+SyEqLgkaRCitFBaOPcHaLNMHUnJsXKEqm3K3d2Fwrp48SJmZoXvLZqWlsZbb73F8uXL+emnn7h79y6dOnXCxcWFV155BWtra3744QdsbW0B3bSvVapUITs7mwYNGjB58mQaNmwIQPXq1UlNTeXw4cNUqVKFAwcOMHjwYG7cuMG4ceP4448/Ch1fRb3rkKLNYsb1OD7zLHtTAwshRF5kcTchSov4w3DlgKmjKDnOVSAwDMytTB1JmXX06FGaN29Oeno6Dg4OLF++nE6dOhktu3fvXk6fPk3dunW5desWs2fPZuPGjRw5coTq1asD8PPPPzNu3DjS0tLo168fEyZMYPDgwdSvX5+GDRsyfPhwMjMzmTBhAj179ixUrDezs5iUdIFD6Xcee7/Lindd/eni6GbqMIQQokhI0iBEaZCapFv1WWlNHUnJ8G6om0pVozF1JGXavXv3uHjxIikpKaxevZoFCxawfft2ateu/dDXarVaQkNDadWqFXPmzDFaZtu2bbz33nts376d4OBgfvzxR7y9vWnatCn/U45x/gAAIABJREFU/PMPnp6ehYo3SynmJl9hze2kQr2urLLRmDHfpwb+lmV7ymAhhABJGoQwPW02xK6B9GRTR1L8NOa67kiu1UwdSbnUvn17qlWrxrx58wpUfsiQIcTFxbFp06Zcz2VkZNCwYUOWLl2KhYUF7du35+rVqwA0adKEcePG6cdVFFbk7evMvnGZLMr/n59aVnZ85R2MuSTIQogyTqZcFcLUrhyoGAmDpT2EdJWEoRgppcjIyChw2ejoaHx8jI8nmTx5Mh07diQ0NJTs7Gyysv4da5OZmZlrqtjCCHd04wuvIFzMyv+wuth7qSy9mWjqMIQQ4rGV/09sIUqzOwmQePTh5co6ey+o9gxY2pk6knJj7NixdOzYkYCAAG7fvs2KFSvYtm0bmzdvBgxnZAKYOHEizZo1o3r16ty6dYs5c+YQHR3NN998k6vuY8eOsXLlSv2K1DVr1sTMzIyFCxfi7e3NiRMnaNKkyWPFX8/Ggbk+1fn46nlOZ6Y9Vl2lXcTNRFrZORNoZWvqUIQQ4pFJ0iCEqSgFl3ZDee+i4RYClZ8CWSW3SCUmJtK/f3/i4+NxdnamXr16bN68mWeeeQbIPSNTSkoKQ4cOJSEhAWdnZxo2bMiOHTto2rSpQb1KKYYOHcqXX36Jvb09ALa2tixevJg333yTjIwMvv76a/z8/B57H7wtrJjjXY1p1y+xPfXmY9dXWmUD/5d8hc+95C6bEKLskjENQphK0km4sN3UURQvvyfBu76poxBlwKKUeCJuXjV1GMXqM49AmtvlvzieEEKUVjKmQQhTyM4s/9OrVn5KEgZRYINdfBjqUr7X65ibfIUsuU4nhCijJGkQwhQSoiEz1dRRFJ8qrcHj4dN+CnG/Ps6evFXJ19RhFJtLWRmsrSDTzQohyh9JGoQoaffuQGKMqaMoJhoIbAvuIaYORJRRLzh5MNL18cdLlFZLbiZyK7sCrfouhCg3JGkQoqTF7QP16NNVll7/Sxhcg00diCjjujq6M8rV39RhFIvb2mwWyxSsQogySJIGIUrSnURIPmPqKIpH1dayBoMoMuGObrxdTrsqrbudxMXMdFOHIYQQhSJJgxAlRT/FajlU+Wlwq2HqKEQ508PJg9fK4eDonClYhRCiLJGkQYiScuM0pF4zdRRFz785eNQydRSinOrt7MlAZy9Th1Hk9qXdZn/aLVOHIYQQBSZJgxD/n707j4uq3h8//pph31UWB0guEIq7hunFrdQWFRfUvC4paXq1zC2XzMovleVVqVA0C1Psl5blTc1KRbMMNbM0FHFBuSoqekEgcWVn+P3BZWIEdAZGDoPv5+Mxj4ecOfM573MQOO/zeX8+n9qgLYLLB5WOwvTcW0HjNkpHIeq5sQ00POnQQOkwTO7j7DSKZQpWIYSZkKRBiNqQfhQKbysdhWk5eUGTzkpHIR4Qsxs1obm1ndJhmNT5wjy+v/Wn0mEIIYRBJGkQ4n4rLoSM40pHYVrWTuD/JKjkV4ioHTZqNfPd/XC1sFQ6FJNae+0KBSVapcMQQoh7kr/4QtxvWaegOF/pKExHbQUBvcHSVulIxAPG3dKKd9x9sUKldCgmk60t4odb2UqHIYQQ9yRJgxD3U4kWMo4pHYVp+fUEu0ZKRyEeUC1sHJjtWr/WcPj6RiYlMrZBCFHHSdIgxP109WzpCtD1hVdHaOCrdBTiAfe0YyOGObsrHYbJXCzK57fcm0qHIYQQdyVJgxD305VEpSMwnYYPg+cjSkchBAATG3jSydZJ6TBMZsONDKVDEEKIu5KkQYj75cYlyK0nM6PYNSpd8VmIOsJCpeL/3P+Gl6W10qGYxNH825zOz1E6DCGEqJIkDULcL+lHlY7ARFTg2xPU9WvWGmH+HNUWzHX1qTfDov99ox4u/iiEqDckaRDifsjJgpuXlY7CNDyDwN5V6SiEqFQbWweecXJTOgyT2JNzjStFBUqHIYQQlZKkQYj7ob70Mti5yjgGUeeNb+DJQ/WgTKkY2Ci9DUKIOkqSBiFMLf8mZJ9TOgoTUIFvD1nATdR5tmo1c+pJmdL2W1e5pS1WOgwhhKhA7gaEMLUriUA9mHNdypKEGakvZUo5JVq23qwnEygIIeoVSRqEMKXiAvjztNJR1JyUJQkzVF/KlDbfzKJIFnsTQtQxkjQIYUrZKaAtUjqKmlGppSxJmCVbtZpX60GZUmZxIT/fvqZ0GEIIoUfuCoQwpav/UTqCmpOyJGHGWts6MLQelCnF3rqqdAhCCKFHkgYhTKXgNtxMUzqKmrF2gsbtlI5CiBp5voGGhma+rkhC/i0yZPpVIUQdIkmDEKZy9QxmPwDauyOoLZSOQogasVNbMKZBY6XDqJES4CcpURJC1CGSNAhhKlfPKB1Bzdi5QsOHlY5CCJMIcWyEl5kPit51O1vpEIQQQkeSBiFMITcbcs18mkTvTqAy9yGkQpSyUqkZ10CjdBg1klKYx5mCXKXDEEIIQJIGIUzD3AdAO3qCSxOloxDCpHraNyDAyk7pMGrkR+ltEELUEZI0CFFTJSXmX5rk3UnpCIQwObVKxYSG5t3b8NPta5TImg1CiDpAkgYhaur2FSi4pXQU1dfAFxzNe9CoEFXpZOdMexsHpcOotqziQk4W5CgdhhBCSNIgRI39ac6lSSrw6qh0EELcVxMaeiodQo3szbmudAhCCIF5T2QthNJKtJB9Tukoqs+1Gdg1VDoKIe6rljYOdLdzYV+ued587719nUkNvZQOw6S0Wi0FBbIOhRBKs7a2Rq02rA9BkgYhauJ6KhTnKx1F9WlkITfxYBjp4mG2SUN6cQHJ+Tk0s7FXOhSTKCgoICUlBa1Wq3QoQjzw1Go1fn5+WFvfe4pqSRqEqIkbqUpHUH1O3mDbQOkohKgVLWzsaWZtR7KZTmG6N+d6vUgaSkpKSEtLw8LCgiZNmhj8hFMIYXparZb//ve/pKWl4ePjg+oe065L0iBETdy4rHQE1efeUukIhKhVg5zciPjTPBP9vTnX+aeZj80AKCoqIicnBy8vL+ztzT8JEsLcubu789///peioiKsrKzuuq+k+EJUV8FtyDfPcgesHKDB35SOQoha1dO+AY5qC6XDqJbUonzSi8x/DEBxcTGAQaUQQoj7r+xnsexn824kaRCium6acy9DC1DJj794sNiq1fR1MN+B/4l5Zjy18x3uVQYhhKgdxvwsyl2DENV1879KR1BNKnBrrnQQQihigJOb0iFUW2L+baVDEEI8wCRpEKK6zHU8Q0M/sJJaYvFgamJlw6O2jkqHUS2JeZI0CCGUIwOhhaiOvOtQaKZ/wGUAtHjAhTq58YcZlvqkFuWTXVxIQ4u7D1Y0S/Gf1O7xOkys3eOZWHBwMD169GDRokUG7b9jxw769u1Lbm4utra21T6uRqPhrbfe4sUXX6x2G+ZgxIgRAHz11VeA8de7MqZoQ2nS0yBEdZjreAbbhuBUvxaJEsJYne2ccTfTG2/pbah9KpXqrq+xY8cqHWKdtmLFCh5//HGcnJxQqVTk5eUZ3caOHTv0rrmHhwf9+/fnxIkT9yHiirZv3868efMM2rcs1jvP05g2asIU17sqkjQIUR3mWprkFqh0BEIozkKloo+jeQ6IPibjGmpdWlqa7rV06VKcnZ31tkVFRVX6ucLCwlqOtG7Kzc2lX79+zJkzp8ZtnT9/nrS0NL799lvS0tLo27cvt25V3mtoyhXHGzVqhKNjzcoaTdGGIUx5ve8kSYMQxiopgVtpSkdRPQ18lY5AiDqhq52L0iFUiwyGrn0ajUb3cnFxQaVSVdh26tQpVCoVmzdvpnv37tjY2LBx40bmzp1LcHCwXnuLFi2ieXP9yShWrlxJYGAgtra2tGjRglWrVhkV45o1awgKCsLR0RFPT0+ee+45srKyKuwXFxdH69atsbW1pUuXLiQlJem9v3fvXrp27YqdnR0+Pj7MmjWL3NyaLYg4e/Zs5syZQ8eOHWvUDkDjxo3RaDR07tyZiIgIUlNTiY+PB0q/T4sXL2b06NE4OTkxdepUAC5cuMDQoUNxcXHBzc2NIUOGkJr613otRUVFTJ06Vfd+Zb0BwcHBzJ07V/d1bm4uM2fOxNvbGxsbG5o1a8a6des4deoUffv2BcDOzg6VSqUr5bqzjaysLJ599llcXFxwcHCgf//+pKSk6N6Pjo5Go9GwdetWAgMDcXJyon///mRmZt71Gpnyet/JrJOGO78B91JVl5GxNBoN0dHRNWrDHIwYMUJX1wfGX+/KmKINxeX+CUWm6+6rNbYNwcZZ6SiEqBOaWtvhZmF+w/rOFuRyW3vv+dSFMl599VVmz57NqVOn6Nmzp0GfWb58OQsWLCAiIoKkpCTmz5/PK6+8woYNGww+blFREQsXLiQxMZFNmzaRlJTExIkVx23MmTOHZcuWcfDgQZydnQkNDdXNzx8fH09ISAgjR47k2LFjfPHFF+zatYuZM2dWedwRI0bQp08fg+M0JTs7O0C/R2fRokV07NiRhIQE5syZw82bN+nRowfu7u7s37+fPXv2YGlpSb9+/SgqKgLgX//6F+vXr2ft2rXs3buXixcvsn379rsee8SIEXzzzTd8/PHHJCUl8eGHH2JnZ0fTpk1Zv3498FevSERERKVtjBo1ihMnThAbG8v+/fvJzc2lf//+euslXLt2jQ8//JAvv/ySn3/+mdOnT+vdQ5Xd16anp1fvIhrJ4N+Y95rHdcyYMfy///f/ahpPvbVixQr+/e9/c/jwYW7dulWtwUhlA5nKuLu706lTJxYvXkyrVq1MHXIF27dvN3hBnqoGXRnTRk2Y4npXyVynWpXF3ITQUatUdLZz4ftbfyodilG0wPH82/zdTh4A1EWzZ88mNDTU4P1LSkpYsGABH3/8se5zfn5+HD16lJUrVzJ8+HCD2imfIPj7+xMZGUmPHj0oKCjQ+5v7zjvv0KtXLwDWrl1LkyZN2LZtGwMHDmTx4sWMHz+eKVOmABAQEEBkZCQhISEsX74cS8uKt4ze3t44OTkZfL6mkpmZybvvvkuDBg0ICgrSbe/Tpw/Tp0/Xff3RRx/h4uLCxx9/rNv22Wef4eLiwq+//spjjz1GVFQU4eHhuuu/atUqdu7cWeWxjx07xnfffce+ffvo1q0bUHrNyzRsWFr62Lhx4yrvO44dO8YPP/xAfHy8Lv4vvvgCHx8ftm/fzoABAwDIz88nJiYGb29vACZNmsSyZct07Tg6OhIYGFjp9+Z+MPgoaWl/lWNs2LCB8PBwTp8+rdtWlvHdqbCw8J7LUj8IymrMnnzyScLDw2vU1vnz57GxsSElJYUpU6bQt29fTp48WWmt3J2/MGqiUaNGdaINQ5jyeldwq3YyepNzkaRBiPK62DubXdIApYOhJWmomx599FGj9r906RJXrlxh9OjReg9ni4qKaNy4scHtHDp0iLfffpvExESys7PRarVotVouXbqkd0PbuXNn3b89PDzw9/cnKSmJgQMHEh8fz+XLl4mJidHtU1JSQmFhIampqfj5+VU47gcffGDU+daUm1vpOiu3b9+mefPmbNy4Ue++4s7rHx8fz4kTJyrcHxUVFXH27FkCAwO5evWq3nWxsbHRS0TudOTIEWxtbenatWu1zyMpKQk7Ozu942g0Gh5++GGSkpJ0SUOjRo10CQOAp6cnGRkZuq+7devGqVOnqh2HsQwuT5KaPqnpk5q+/8kxv5sMLO3AwV3pKISoU4JsHbE1w5XRE/PNb7rYB4WDg4Pe12q1mpKSEr1t5ctptFotUPr0OyEhQfc6fvw4e/bsMeiY169fp3fv3ri7u7N+/Xr++OMP3VShhgwGLktWtFotU6dO1Yvj6NGj/Oc//+Ghhx4yKJb77eDBgxw9epQbN26QlJTEE088off+nddfq9XSuXNnvXNKSEggOTmZZ555psL3xhBVPSQ3RlXHLSkp0Use73zorlKpdP9nlHBffltKTV/tkZq+Wq7pKy6Agpv3/zim5uIDZnhzJMT9ZK1S09G29ksraup0fi4FJcrdOAjDubu761VqACQkJOj+3aRJE9zc3EhJSSEgIEDv5evra9Axjh8/TnZ2NhEREXTr1o3AwECuXLlS6b6//fab7t8ZGRmcO3dO9wA3KCiIkydPVogjICCgzlSM+Pv78/DDDxtcEhUUFMTp06fx9PSscE7Ozs5oNBoaNmyod10KCgo4cuRIlW22bduWvLw89u/fX+n7ZdUd5e9j7tSyZUtyc3M5fPiwblt6ejrnzp2jRYsWBp2bEu5LEZTU9NUOqekrVas1febYywAynkGIKnSxd2Zf7nWlwzBKISWczs+lja3DvXcWiurVqxczZ85kyZIlDBgwgK1bt7J7925d6ZFarebNN9/ktddew97enqeeeoq8vDwOHTpETk6Orlrgbnx9fbG0tGTZsmWMGzeOI0eOVLmAWHh4OM7Ozri6ujJnzhx8fHwICQkB4PXXX6dLly7MmDGDsWPHYmdnx8mTJ9mzZw9LliyptL1Zs2Zx48aNu1aGpKWlceXKFc6dOwdAYmIi1tbW+Pr60qBBg3ueX02MGTOGJUuWMHjwYN588028vLw4f/48mzZtIjw8HA8PD6ZPn84777yDr68vTZs2ZfHixeTk5FTZZmBgICNGjCAsLIxly5bRunVrUlJSyM7O5plnntEle1u3bqVXr17Y29tX6AFp06YNvXv35vnnnyc6Oho7OztmzZpF06ZN9cau3ssvv/zCP//5T3755Rdd6db9vN735S5LavruL6npU7CmL9cMkwaVBTh533s/IR5Af7dzQgUYX6SgrEtF+bShHiUNZr5Cc1XatWtHVFQUERERhIeHM2zYMKZPn64rHwKYMmUKTk5OREZGMnPmTBwdHWnbti2zZs0y6Bje3t6sXr2a8PBw3nvvPTp16sT777/P4MGDK+y7cOFCXnrpJc6ePUtQUBBbtmzRPXDr0KEDcXFxzJs3j65du6JSqQgICGDUqFFVHvvy5ctcu3btrvFFRUWxePFi3dd///vfAfjyyy91MzQGBwfTvn17k89M6ezszL59+3j11VcJDQ3l1q1bPPTQQzz11FO6G/nXX3+djIwMRo8ejZWVFRMnTtQlUlVZvXo1r732GhMmTODatWv87W9/042f9Pf354033mDatGlkZmYyceLESs/r888/Z9q0afTp04eioiJ69uzJ1q1bsbCwMPj8bt26xenTp3VVI2DY9a6u+5I01KSmr3379voBGvj0uKymLzQ0lPXr1+Pu7k5ycjIDBw6sVk3fCy+8UGGfulTTZ2Njg4eHR6U9HFXV9K1Zs6bCvh4eHnfNqKvywNb0mWNPg5MXmOnqt0Lcbw0trGhlY8/xfON/DyrpcmG+0iE8kMaOHVvpCtDNmzev8m/atGnTmDZtmt62t956S+/rMWPGMGbMGIPjKF9OU9Xny8fTp08f3dd3e5LduXNnfvrppyrfv7MMuHzyU5VFixZV2fNRFueFCxd4+eWXq9ynfPyGxlbG29ubzz//vMrPWVlZsWLFClasWFHlPndebwcHB5YtW6ZX9VDeu+++y7vvvnvXNtzc3HSl3JV58cUXdeNBy9w5FX5l1+Ve17smamWOJmNq+oYOHVqtY5Sv6XN3Lx3wuW/fvkr3/e233xg4cCBw95q+usrf39+o6UODgoLYvn07np6eFRIKKM3Ey2r6OnXqBPxV09ejR49K2yxf01dWnlSesTV9Zb0Ndb6mL/eq0hEYz8lL6QiEqNPa2TiaXdJwqUiSBlE/HD16FG9vb4NL0YVyamVkZK9evUhNTWXJkiWcOXOGpUuXsnv37r+C+F9N3/z581mxYgXJyckkJiYSExPD8uXLDTpG+Zq+lJQUNm/efNeavri4OI4dO8aYMWMq1PT9+OOPzJgxg6NHj5KcnMyWLVuYMWNGlceeNWsWEyZMuGt8aWlpJCQk6NWYJSQk3LNbzxTGjBmDg4MDgwcPZv/+/aSkpPDzzz8zZcoUXZlPWU3fd999pxtAbmhN3/fff09KSgq7d+9m06ZNAHo1fZmZmdy+XXEV0/I1fQcOHCAhIYFRo0ZVq6avefPmejNl3bfrnXf/v18mJ7MmCXFXzWxq3nNa26SnQdQX7du3548//rjnemBCebWSNJTV9H3wwQc88sgjHDt2TG+gLpTW9H344Yd88skntGnThp49e/L5559XOoagMmU1fWvXrqVFixYsXbqU999/v9J9y2r6Hn30Ua5du1ZpTV9iYiJdu3alQ4cOvP3223o19Xe6fPmy3vSllYmKiuKRRx5h8uTJQGmN2SOPPMKOHTt0+wQHB1foijKFspo+Dw8PQkNDadGiBRMmTKC4uFivpm/YsGGMHj2abt264enpaVBN34ABA5gwYQItWrRg0qRJutW2y9f0NW7cuMrazM8//5xWrVrRp08f3TS3pqrpu9f1NlrBbdAW3nu/usbeTekIhKjTmlnbKx2C0S4X3bvsVgghTElVUp1JaoXJlZSU4OXlxZIlS2o8UEXcJzcuw3+2KR2FcWycobX8fxLiXkJTj3NDW3U5ZV200bslrpbmNV4pLy+PlJQU/Pz8jCqzFULcH8b8TMrE7XWE1PSZgXzzmpYRAHspTRLCEIFm2Nsg4xqEELVJkoY6Qmr6zICZjGdY+Gksqkdf4OUPNlQoTSopKaFv376oVCq2bNly13ZUKlWlr/feew8oXT8jLCwMZ2dnAgMD9cYpAURERBg0x7gQdUEzazMc1yBJgxCiFtXK7ElC1At5db+n4dCJ83zyzT7aNv3f9MB3DIJeunSpwYnpnTOexcbGMn78eJ555hkAPvnkE+Lj4zlw4ACxsbGMHDmS9PR0VCoVKSkprF69mj/++KPmJyVELTDPwdAyrkEIUXukp0EIQxXcVDqCu7qVk8eo/4th1RthNHT6X6lFuZ6Go0ePEhkZWel6HZXRaDR6r2+//ZaePXvqFkosWxCxVatWTJ48mYyMDN0MVpMmTWLx4sU4Ozub9iSFuE/McTC0lCcJIWqTJA1CGKowV+kI7mry4i/p17UNT/79f2tcWFiXvoCcnBxGjhzJhx9+iEajMbrtK1eusG3bNsaPH6/b1q5dO3755Rdyc3PZuXMnnp6euLm58fnnn2Nra1vpaqRC1FUaS2uc1YbP2lYXXJJpV4UQtUjKk4QwREkJFNfdUoCvdh4iPukif6x7/a+Nln89OZ0xYwZdunQhNDS0Wu1/9tlnODk5MWTIEN22cePGkZiYSMuWLXFzc+Pf//432dnZvPnmm/z888/MmzePr776iocffpg1a9bcddpiIeqCQGt7DuXV7R7F8v5bVEBJSUm9GAvX88LRWj3ez39rV6vHM7Xg4GB69Ohh8Mq/O3bsoG/fvuTm5tZo1iqNRsNbb711X6aHr0vuvL6mOO/6cO2kp0EIQxTnA3VzduLU9KtM/2ADX7w7DlubctMv/q+X4bvvvmP37t0sXbq02sdYs2YNo0aN0vtjY2VlxYoVK0hJSeHQoUN069aNmTNnMm3aNBISEtiyZQtHjx4lODiYadOmVfvYQtQWb0trpUMwSl6JlqzionvvKGqkqkkhyl5jx45VOsQ6LScnh0mTJuHq6oqjoyNDhgypMGbuXqKjo3XXW61W4+XlxciRI++5RpaplC0GbIjo6OhKe/SNaaMmsrKyGDlyJM7OzjRo0IBx48Zx86ZpHoZI0iCEIYrylI6gSvGnLpJx9SYdwv6F5d8nYfn3Sew5nMyyNRuwtLRk165dnD17lgYNGmBpaalbyPCZZ56hR48e92x/3759nD59mn/+85933W/37t2cPHmSKVOmEBcXR0hICA4ODgwbNoy4uDgTnKkQ95e5rXkAkF1shgtOmpm0tDTda+nSpTg7O+tti4qKqvRzhYXyvQGYPHkysbGxbNy4kT179pCZmcmgQYMwdpkwd3d30tLSuHTpEuvWreP333+/azumvP7u7u7Y2dVssgRTtGGIYcOGcfr0aX788Ue2bt3Kr7/+yrhx40zStiQNQhiiDicNT3RszrGvwkn4Yp7u9WjLvzHqH6EkJCTwxhtvkJiYSEJCgu4FsGTJEj799NN7th8TE0OHDh1o167q7vy8vDwmT57MypUrsbCwoLi4WPcLu7CwkOJi81o0SzyYXC3ML2nIKdEqHUK9V35CCBcXF1QqVYVtp06dQqVSsXnzZrp3746NjQ0bN25k7ty5BAcH67W3aNEimjdvrrdt5cqVBAYGYmtrS4sWLVi1apVRMa5Zs4agoCAcHR3x9PTkueee001MUV5cXBytW7fG1taWLl26kJSUpPf+3r176dq1K3Z2dvj4+DBr1ixyc6s/ni8rK4t169YRFRVFz5496dChA+vWrePgwYPs3bvXqLbUajUajQYvLy+eeOIJ3njjDQ4fPszFixfJy8tDpVIRExNDv379sLe35/333wdKn/D37t0bBwcHPD09GTduHNnZ2bp2b968ybPPPouDgwPe3t4sX768wrE1Gg3R0dG6r//880/GjRuHh4cHdnZ2tG3blp07d7Jjxw4mTZrElStXdD0j5Uucyrdx7tw5+vfvj4ODAw0aNODZZ5/V+56V/d9Zs2YNPj4+NGjQgLCwMG7fvl3lNTpy5Ag///wzn376KZ06daJbt25ER0ezceNGzp8/b9T1rowkDUIYog4nDU4OtrQO8NZ7Odja4OrmTuvWrdFoNLRu3VrvBeDj44Ofn5+unebNm/PNN9/otX3jxg2+/vrre/YyzJ8/n379+vHII48A0LVrVzZv3kxiYiIffvghXbt2NfFZC2F6jSzMb5hfrlaShrrk1VdfZfbs2Zw6dYqePXsa9Jnly5ezYMECIiIiSEpKYv78+bzyyits2LDB4OMWFRWxcOFCEhMT2bRpE0lJSUycOLHCfnPmzGHZsmUcPHgQZ2dnQkNDdQ914uPjCQkJYeTIkRw7doyJX/I1AAAgAElEQVQvvviCXbt2MXPmzCqPO2LECPr06VPl+wcPHqS4uJinn35at83X15dmzZrx66+/Gnx+lSl7al++R2HevHkMHz6cEydOMGrUKFJTU3n88cfp3Lkzhw8fZuvWrZw7d45Ro0bpPjN9+nQOHDjA999/T2xsLFu3buXEiRNVHrfsfI4cOcKXX37JiRMneOedd1Cr1fTq1YvFixfrekXS0tIqXa+ouLiYAQMGkJOTwy+//EJsbCwnTpxg9OjRevudPHmSH374gdjYWL755ht27NhBZGSk7v3o6Gi9suEDBw7g4eGh95Dvsccew9bWlgMHDhhxdStnfr8hhVBCHU4aqqQ27qnp6dOnuX5dfy2Kr776ipKSEkaOHFnl544fP87XX3+t68EAGDp0KHFxcXTv3p3AwEDWr19vXOxCKMDNLHsapBevLpk9e7ZRE06UlJSwYMECPv74Y93n/Pz8OHr0KCtXrmT48OEGtVM+QfD39ycyMpIePXpQUFCAtfVfY3XeeecdevXqBcDatWtp0qQJ27ZtY+DAgSxevJjx48czZcoUAAICAoiMjCQkJITly5frSlvL8/b2xsnJqcq40tPTcXJyqlCW07hxY9LT0w06t8pcuHCByMhI/Pz88Pf3p6CgdKKSsWPH8txzz+n2mzNnDt27d+ett97SbVu9ejVNmzbl4sWLODo6snbtWjZt2qS7LuvWraNJkyZVHnvbtm0kJiaSnJyse/BWNhU5gLOzs65XpCrbt2/nzJkz/PTTT7r9Pv30Uzp06MCxY8do06YNUDqeZs2aNdjb29OqVStGjhzJTz/9xP/93/8B0LBhQwIDA3Xtpqen07hxY71jqdVq3N3da3S9y0jSIIQhzCxpiPtkFjwyvsr3K6sBrWzbxIkTK31aVV7r1q35z3/+o7dNrVbz0Ucf8dFHHxkYsRDKk54GUVOPPvqoUftfunSJK1euMHr0aL1ZsIqKiirc/N3NoUOHePvtt0lMTCQ7OxutVotWq+XSpUt6N7SdO3fW/dvDwwN/f3/dmjvx8fFcvnyZmJgY3T4lJSUUFhaSmpqq1zNd5oMPPrhnbJXN7lWdWb8yMjJwdHREq9WSm5tLx44d2bx5M2r1X0Uzd17/+Ph49u3bh6OjY4X2zp49i729PcXFxZVel6okJCTg7+9f6fUwVFJSEv7+/nqJRVBQEHZ2diQlJemShoCAAOzt/5oJ0dPTkx9//FH39fDhwysklqa63pUxv9+QQijBzJIGLG3BzOacF0JpLmpLLABzenYvYxrqFgcHB72v1Wp1hQcy5ctptP9L+j777DPat2+vt19lT/Yrc/36dXr37k1oaCjr16/H3d2d5ORkBg4cqHsCfzdlN5NarZapU6fywgsvVNjnoYceMiiWO2k0Gm7cuEFubq5eb0NGRoZRSRGAq6srBw4c0D3FL38zXebO66/Vahk6dCjz58+vsK+XlxeJiYlGxQCYZDBzVTfxd263stLv/VSpVLr/M5XRaDQVehS0Wi1ZWVlGX+/KSNIghCHMLWmwMr/VbYVQmlqlopGFFZlmNCNRrtacUpwHT1lte3nlSzmbNGmCm5sbKSkpDB06tFrHOH78ONnZ2URERODu7g6UznpXmd9++42BAwcCpTfu586d0w3KDgoK4uTJkwQEBFQrjsp06tQJCwsLdu3apTvuhQsXSE5OpkuXLka1ZWFhYXRsQUFB7Nq1C39/f70eiTLNmjXDwsKi0utSlbZt23Lu3DnOnz+Pr69vhfetra3vOflHy5YtOXv2LOnp6brehsOHD5OXl0eLFi2MOEN9nTt3JiMjg8TERNq2bQuU/l/Iy8vT602pLhkILYQhJGkQ4oHgamYlStLTULf16tWL1NRUlixZwpkzZ1i6dCm7d+/Wva9Wq3nzzTeZP38+K1asIDk5mcTERGJiYiqdxacyvr6+WFpasmzZMlJSUti8eXOVi76Fh4cTFxenWzPAx8eHkJAQAF5//XV+/PFHZsyYwdGjR0lOTmbLli3MmDGjymPPmjWLCRMmVPm+m5sbYWFhTJ8+nbi4OA4fPkxYWBgdO3bkscceM+j8amL69OlcunSJ0aNH88cff3D27Fl27Nihm4K0UaNGhIWFMWPGDOLi4khMTGTs2LF640Du9PTTT9OxY0cGDx7M7t27SUlJYdu2bbqyIV9fX65evcq+ffvIysqqdPapkJAQAgICGDVqFAkJCRw4cIDnn3+e3r176yYrMcSGDRv0Bj0/8sgj9OjRg3HjxnHw4EF+/fVXJk2axNChQytNcIxlXr8dhVCKJA1CPBBKp12t/hSTtS2nnvQ0mPsKzVVp164dUVFRREREEB4ezrBhw5g+fTpfffWVbp8pU6bg5OREZGQkM2fOxNHRkbZt2zJr1iyDjuHt7c3q1asJDw/nvffeo1OnTrz//vsMHjy4wr4LFy7kpZde4uzZswQFBbFlyxZdGVSHDh2Ii4tj3rx5dO3aFZVKpbuxrcrly5e5du3aXeNbsWIFM2fOZMiQIeTn5/P000/z0Ucf6ZXhaDQaXn75ZebOnWvQORvKx8eH/fv3M3fuXJ588kkKCgrw9fWlX79+un2ioqKYOHEiISEhuLi4MHfuXK5evVplmyqVim+//ZbZs2fzj3/8g5ycHJo1a6ab4rVnz56MHTuWQYMGcfXqVRYuXFjhvCwsLPj++++ZOnUqXbt2xdLSkn79+rFs2TKjzi87O5vTp0/rbfv666+ZMmUKTzzxBGq1miFDhlS5loixVCXGrq4hxIPoxNeQl33v/eoKjzbQpOZdkUI8aBZlXWTnbfP5WX/CvgHz3P+mdBgGy8vLIyUlBT8/P72pIsWD6+bNm7i6urJnzx6TlNAI4xjzMynlSUIYwgSzDtQqlfxoC1Edlmb2s54r5UnCzP30008MGDBAEgYzIOVJQhjEvG4kzC7JEaKOsDCzn/X6Up4kHlyDBg1i0KBBSochDCCPI4UwhLndhEtPgxDVYmFmP+syEFoIUVvkzkIIg5jXjYT5xStE3WBuq5vkyeJuQohaIkmDEIYws6ePZhevEHWEyswSbnMbgyGEMF+SNAhhEDP7wyyToglRLVrM62fHrpIFq4QQ4n6Q3zZCGMLcnuZJnbMQ1VJsZgm3rYxfEkLUEvltI4RBJGkQ4kFQZGY9DZI0CCFqi/y2EcIQ0tMgxAOh2LxyBkkahBC1RtZpEMIg5pY0yNztQlRHvpkl3Lb1ZEzDC1tr93gr+9fu8UwtODiYHj16sGjRIoP237FjB3379iU3N7dGK3FrNBreeustXnzxxWq3YQ5GjBgBwFdffQUYf70rY4o2lFY/ftsIcb+ZW09DYY7SEQhhlq4WFyodglGkp+H+U6lUd32NHTtW6RDrtBUrVvD444/j5OSESqUiLy/P6DZ27Nihd809PDzo378/J06cuA8RV7R9+3bmzZtn0L5lsd55nsa0URM5OTlMmjQJV1dXHB0dGTJkCGlpaSZpW37bCGEQSRqEeBD8WVykdAhGkaTh/ktLS9O9li5dirOzs962qKioSj9XWGheCej9kpubS79+/ZgzZ06N2zp//jxpaWl8++23pKWl0bdvX27dulXpvgUFBTU+XplGjRrh6OioeBuGmDx5MrGxsWzcuJE9e/aQmZnJoEGDKDHBJA/y20YIQ6jNrJJPkgYhquVPc+tpqCflSXWZRqPRvVxcXFCpVBW2nTp1CpVKxebNm+nevTs2NjZs3LiRuXPnEhwcrNfeokWLaN68ud62lStXEhgYiK2tLS1atGDVqlVGxbhmzRqCgoJwdHTE09OT5557jqysrAr7xcXF0bp1a2xtbenSpQtJSUl67+/du5euXbtiZ2eHj48Ps2bNIjc316hY7jR79mzmzJlDx44da9QOQOPGjdFoNHTu3JmIiAhSU1OJj48HSr9PixcvZvTo0Tg5OTF16lQALly4wNChQ3FxccHNzY0hQ4aQmpqqa7OoqIipU6fq3q+sNyA4OJi5c+fqvs7NzWXmzJl4e3tjY2NDs2bNWLduHadOnaJv374A2NnZoVKpdKVcd7aRlZXFs88+i4uLCw4ODvTv35+UlBTd+9HR0Wg0GrZu3UpgYCBOTk7079+fzMzMKq9PVlYW69atIyoqip49e9KhQwfWrVvHwYMH2bt3b3UuuR75bSOEIazslY7AOJI0CGG0XG0xOeY2pkF6GuqUV199ldmzZ3Pq1Cl69uxp0GeWL1/OggULiIiIICkpifnz5/PKK6+wYcMGg49bVFTEwoULSUxMZNOmTSQlJTFx4sQK+82ZM4dly5Zx8OBBnJ2dCQ0Npbi4dAxcfHw8ISEhjBw5kmPHjvHFF1+wa9cuZs6cWeVxR4wYQZ8+fQyO05Ts7OwA/R6dRYsW0bFjRxISEpgzZw43b96kR48euLu7s3//fvbs2YOlpSX9+vWjqKi0V/Ff//oX69evZ+3atezdu5eLFy+yffv2ux57xIgRfPPNN3z88cckJSXx4YcfYmdnR9OmTVm/fj3wV69IREREpW2MGjWKEydOEBsby/79+8nNzaV///667wfAtWvX+PDDD/nyyy/5+eefOX36tF7iUVYKlZ6eDsDBgwcpLi7m6aef1u3j6+tLs2bN+PXXX425vJUys8enQijE3JKGEi0U5YFl9Qe8CfGgMbfSJAA7SRrqlNmzZxMaGmrw/iUlJSxYsICPP/5Y9zk/Pz+OHj3KypUrGT58uEHtlE8Q/P39iYyMpEePHhQUFGBtba1775133qFXr14ArF27liZNmrBt2zYGDhzI4sWLGT9+PFOmTAEgICCAyMhIQkJCWL58OZaWFW8Zvb29cXJyMvh8TSUzM5N3332XBg0aEBQUpNvep08fpk+frvv6o48+wsXFhY8//li37bPPPsPFxYVff/2Vxx57jKioKMLDw3XXf9WqVezcubPKYx87dozvvvuOffv20a1bN6D0mpdp2LAhUNorUtWg82PHjvHDDz8QHx+vi/+LL77Ax8eH7du3M2DAAADy8/OJiYnB29sbgEmTJrFs2TJdO46OjgQGBuq+N+np6Tg5OekSqjKNGzfWJRY1IUmDEIYwt6QBSnsbJGkQwmDmVpoEYCPlSXXKo48+atT+ly5d4sqVK4wePRpVuQk3ioqKaNy4scHtHDp0iLfffpvExESys7PRarVotVouXbqkd0PbuXNn3b89PDzw9/cnKSmJgQMHEh8fz+XLl4mJidHtU1JSQmFhIampqfj5+VU47gcffGDU+daUm5sbALdv36Z58+Zs3LiRRo0a6d6/8/rHx8dz4sSJCmMJioqKOHv2LIGBgVy9elXvutjY2OglInc6cuQItra2dO3atdrnkZSUhJ2dnd5xNBoNDz/8MElJSbqkoVGjRrqEAcDT05OMjAzd1926dePUqVN6basqmbilpKSk0u3GkqRBCEOYY9JQcBvsGt17PyEEYJ5Jg5Qn1S0ODg56X6vV6goDUMuX02i1peVwn332Ge3bt9fbr7In+5W5fv06vXv3JjQ0lPXr1+Pu7k5ycjIDBw40aDBw2c2kVqtl6tSpvPDCCxX2eeihhwyK5X47ePAgNjY2eHh4VNrDcef112q1dO7cmTVr1lTY18PDg5wc40t573yKXx1VDUq+8+beyspK732VSqX7P1MZjUbDjRs3yM3N1YszIyPDqCS0KpI0CGEIc0waZFyDEEYxx/IkJ7WF0iGIu3B3d68w3WVCQoLu302aNMHNzY2UlBSGDh1arWMcP36c7OxsIiIicHd3B2Dfvn2V7vvbb78xcOBAoPRG8ty5c7pB2UFBQZw8eZKAgIBqxVEb/P39jVpnIigoiO3bt+Pp6VkhoQBwdnamYcOG/Pbbb3Tq1AkonXXpyJEj9OjRo9I227ZtS15eHvv379eVJ5VXVg5WfmzCnVq2bElubi6HDx/W9Takp6dz7tw5WrRoYfD53alTp05YWFiwa9cu3ff5woULJCcn06VLl2q3W0YeUQhhCEkahKj3zLGnwcvS+t47CcX06tWL1NRUlixZwpkzZ1i6dCm7d+/Wva9Wq3nzzTeZP38+K1asIDk5mcTERGJiYli+fLlBx/D19cXS0pJly5aRkpLC5s2bq1xALDw8nLi4OI4dO8aYMWPw8fEhJCQEgNdff50ff/yRGTNmcPToUZKTk9myZQszZsyo8tizZs1iwoQJd40vLS2NhIQEzp07B0BiYiIJCQlcu3bNoPOriTFjxuDg4MDgwYPZv38/KSkp/Pzzz0yZMkVX5jN9+nTeeecdvvvuO90A8rv1QAQGBjJixAjCwsL4/vvvSUlJYffu3WzatAko/X4AbN26lczMTG7fvl2hjTZt2tC7d2+ef/55Dhw4QEJCAqNGjaJp06a62ZcM8csvv9C8eXPdTFlubm6EhYUxffp04uLiOHz4MGFhYXTs2JHHHnvM4HarIj0NQhjC0hZU6tIBxuaioPK5q4UQlcssMq+kwQoVbhZW997RDJj7Cs1VadeuHVFRUURERBAeHs6wYcOYPn26bqVhgClTpuDk5ERkZCQzZ87E0dGRtm3bMmvWLIOO4e3tzerVqwkPD+e9996jU6dOvP/++wwePLjCvgsXLuSll17i7NmzBAUFsWXLFl0ZVIcOHYiLi2PevHl07doVlUpFQEAAo0aNqvLYly9fvufNf1RUFIsXL9Z9/fe//x2AL7/8UrfycnBwMO3btyc6OtqgczaUs7Mz+/bt49VXXyU0NJRbt27x0EMP8dRTT+l6Hl5//XUyMjIYPXo0VlZWTJw4UZdIVWX16tW89tprTJgwgWvXrvG3v/2N8PBwoLQ35I033mDatGlkZmYyceLESs/r888/Z9q0afTp04eioiJ69uzJ1q1bsbAwvPfw1q1bnD59WjcTFJQupjdz5kyGDBlCfn4+Tz/9NB999JFJxjSoSkyx2oMQD4LEL6Cw4hODOsuhMTQ3fBYPIR50Yy6f4mJRvtJhGMzH0obPvJvfe8c6JC8vj5SUFPz8/IwqMxH1V0lJCV5eXixZskSXRIjaY8zPpJQnCWEocytRyskyr54RIRSUoy0m1YwSBgAvKylNEubv6NGjeHt7Gzy9rFCOJA1CGMrckoaSYsi7/zWjQtQH/ynIxdy63b0sbZQOQYgaa9++PX/88YdJymfE/SVJgxCGMrekAeB21cvNCyH+klyQq3QIRpNB0EKI2iRJgxCGMsekISdL6QiEMAvJBeY325j0NAghapMkDUIYytZF6QiMJ0mDEAZJzjfDngYZ0yCEqEWSNAhhKDtXpSMwngyGFuKezHEQtArwlPIkIUQtkqRBCEPZuoDKzFZflcHQQtyTOQ6CdrOwwlolf8KFELVHfuMIYSiVGuwaKR2F8WQwtBB3ZY6DoKWXQQhR2yRpEMIY9mZYonQ7Q+kIhKjTkvLNcRC0JA1CiNplqXQAQpgVcxzXcP2i0hEIUWcVlZRwKO+m0mEYzduqns2c9MkntXu8iRNr93gmFhwcTI8ePVi0aJFB++/YsYO+ffuSm5tbo5W4NRoNb731Fi+++GK12zAHd15fU5x3fbh20tMghDHMsaeh8LbMoiREFY7l3+aWtljpMIzW1NpO6RAeGCqV6q6vsWPHKh1inZaTk8OkSZNwdXXF0dGRIUOGkJaWZlQb0dHRuuutVqvx8vJi5MiRpKam3qeo9R07dowxY8YYtG90dDQajaZGbdTEm2++SXBwMHZ2dpXGUROSNAhhDHMc0wBw7YLSEQhRJ/2ac13pEIymBlrZOCgdxgMjLS1N91q6dCnOzs5626Kioir9XGFhYS1HWjdNnjyZ2NhYNm7cyJ49e8jMzGTQoEGUlBg3/YC7uztpaWlcunSJdevW8fvvv9+1HVNef3d3d+zsapaom6INQxQWFjJixAgmTJhg8rYlaRDCGBbWYOOsdBTGuy5JgxCV2Z97Q+kQjOZrZYuj2sxmcjNjGo1G93JxcUGlUlXYdurUKVQqFZs3b6Z79+7Y2NiwceNG5s6dS3BwsF57ixYtonnz5nrbVq5cSWBgILa2trRo0YJVq1YZFeOaNWsICgrC0dERT09PnnvuObKyKvYwx8XF0bp1a2xtbenSpQtJSUl67+/du5euXbtiZ2eHj48Ps2bNIje3+hMFZGVlsW7dOqKioujZsycdOnRg3bp1HDx4kL179xrVllqtRqPR4OXlxRNPPMEbb7zB4cOHuXjxInl5eahUKmJiYujXrx/29va8//77QOkT/t69e+Pg4ICnpyfjxo0jOztb1+7Nmzd59tlncXBwwNvbm+XLl1c4tkajITo6Wvf1n3/+ybhx4/Dw8MDOzo62bduyc+dOduzYwaRJk7hy5YquZ6R8iVP5Ns6dO0f//v1xcHCgQYMGPPvss3rfs7L/O2vWrMHHx4cGDRoQFhbG7du373qd/vWvf/Hyyy/TsmVLo66vISRpEMJY5jiuIScLCm4pHYUQdcr5gjzSigqUDsNoraWXoc569dVXmT17NqdOnaJnz54GfWb58uUsWLCAiIgIkpKSmD9/Pq+88gobNmww+LhFRUUsXLiQxMRENm3aRFJSEhMrGbcxZ84cli1bxsGDB3F2diY0NJTi4tLyvPj4eEJCQhg5ciTHjh3jiy++YNeuXcycObPK444YMYI+ffpU+f7BgwcpLi7m6aef1m3z9fWlWbNm/PrrrwafX2XKntqX71GYN28ew4cP58SJE4waNYrU1FQef/xxOnfuzOHDh9m6dSvnzp1j1KhRus9Mnz6dAwcO8P333xMbG8vWrVs5ceJElcctO58jR47w5ZdfcuLECd555x3UajW9evVi8eLFul6RtLQ0pk6dWmkbAwYMICcnh19++YXY2FhOnDjB6NGj9fY7efIkP/zwA7GxsXzzzTfs2LGDyMhI3fvR0dE1GqNiLEkahDCWOY5rABkQLeqMhQsXolKpePnllwE4f/58lfXiX3/9tUFtvvDCC6hUKpYuXarblp+fT1hYGM7OzgQGBrJ79269z7y2aAH/edOwgaR1iSQNddfs2bMJDQ3Fz8/PoHrykpISFixYQFRUlO5z//jHP5gyZQorV640+LgTJ06kd+/e+Pv706VLFyIjI/n2228pKNBPit955x169epF27ZtWbt2LRcuXGDbtm0ALF68mPHjxzNlyhQCAgLo3r07kZGRxMTEUFRUVOlxvb29adKkSZVxpaen4+TkVKEsp3HjxqSnpxt8fne6cOECkZGR+Pn54e/vr9s+duxYnnvuOfz8/PDx8WH58uV0796dt956i8DAQDp06MDq1auJjY3l4sWLXL16lbVr17J06VLddVm3bl2F61betm3bSExMZPPmzTzxxBP4+/sTGhrKU089hbW1Nc7OzrpeEY1Gg4NDxZ/X7du3c+bMGdavX88jjzxC586d+fTTT9m5cyfHjh3T7adSqVizZg2tWrWiZ8+ejBw5kp9++kn3fsOGDQkMDKz2dTSWzJ4khLHMsacBSsc1uJu+u1IIYxw6dIhPPvmEtm3b6rY1adKkwsDITz75hIiICPr27XvPNrds2cLvv/+Ol5dXhTbi4+M5cOAAsbGxjBw5kvT0dFQqFSkpKcR+9jltvv3cNCdWi9rYStJQVz366KNG7X/p0iWuXLnC6NGjUalUuu1FRUU0btzY4HYOHTrE22+/TWJiItnZ2Wi1WrRaLZcuXdK7qe7cubPu3x4eHvj7+5OUlMTAgQOJj4/n8uXLxMTE6PYpKSmhsLCQ1NRU/Pz8Khz3gw8+uGds5c+rfLuVbb+bjIwMHB0d0Wq15Obm0rFjRzZv3oxa/dfz7zuvf3x8PPv27cPR0bFCe2fPnsXe3p7i4uJKr0tVEhIS8Pf3r/R6GCopKQl/f3+9xDIoKAg7OzuSkpJo06YNAAEBAdjb2+v28fT05Mcff9R9PXz4cIYPH17tOIwlSYMQxrJ3UzqC6rn5XyguBAsrpSMRD6hbt24xatQoVq1axbvvvqvbbmFhUeGp7DfffMPw4cMr/WNf3uXLl5kyZQo7d+6kX79+eu+V3Qy1atUKf39/XnnlFbKysnB3d+efL75IkznTsHS6e/t1jZuFJRpZo6HOuvOpslqtrjBQt3w5jVarBeCzzz6jffv2evtZWhp2i3b9+nV69+5NaGgo69evx93dneTkZAYOHHjXJ+Zlym7etVotU6dO5YUXXqiwz0MPPWRQLHfSaDTcuHGD3Nxcvd6GjIwMo5IiAFdXVw4cOKB7il/+ZrrMnddfq9UydOhQ5s+fX2FfLy8vEhMTjYoBMMlg5qqSpju3W1np/71WqVS6/zNKkPIkIYxl7QA2LkpHYbySYrhxSekoxANs8uTJ9OvXjyeffPKu+8XHx5OQkMD48ePvup9WqyUsLIxXXnmFVq1aVXi/Xbt2/PLLL+Tm5rJz5048PT1xc3Pj888/J8fKArc+vWp0PkqQWZPMS1lte3kJCQm6fzdp0gQ3NzdSUlIICAjQe/n6+hp0jOPHj5OdnU1ERATdunUjMDCQK1euVLrvb7/9pvt3RkYG586d0w3KDgoK4uTJkxXiCAgIqHDzaqhOnTphYWHBrl27dNsuXLhAcnIyXbp0MaotCwsLAgIC8Pf3rzRhqExQUBDHjx/H39+/wjnZ29vTrFkzLCwsKr0uVWnbti3nzp3j/Pnzlb5vbW2tGydSlZYtW3L27Fm9Eq3Dhw+Tl5dHixYtDDo3JUjSIER1OFfvqYvirp5ROgLxgPrqq6+Ij49n4cKF99w3JiaGFi1a3POmYvHixVhaWjJt2rRK3x83bhzt2rWjZcuWLFiwgH//+99kZ2fz5ptv0ubt1zj3/of89vgAjoZNIj+98pusuqaNJA1mpVevXqSmprJkyRLOnDnD0qVL9cbWqNVq3nzzTebPn8+KFStITk4mMTGRmJiYSmfxqYyvry+WlpYsW7aMlJQUNm/eXOWib+Hh4cTFxenWDPDx8Rtkq0AAACAASURBVCEkJASA119/nR9//JEZM2Zw9OhRkpOT2bJlCzNmzKjy2LNmzbrr1J5ubm6EhYUxffp04uLiOHz4MGFhYXTs2JHHHnvMoPOrienTp3Pp0iVGjx7NH3/8wdmzZ9mxYwfjxo0DoFGjRoSFhTFjxgzi4uJITExk7NixWFtX3Zv39NNP07FjRwYPHszu3btJSUlh27ZturIhX19frl69yr59+8jKyqp09qmQkBACAgIYNWoUCQkJHDhwgOeff57evXvTunVrg89vw4YNtGvXTm/bhQsXSEhI4NKlSxQVFZGQkEBCQgI5OTVf+V7Kk4SoDpcmkFn17Ap11rXzUHC7tLdEiFqSmprK9OnT+eGHH+4500dubi7r16/n//7v/+66X3x8PFFRURw+fLjK2mgrKytWrFiht23s2LGETZ7EpiOHyfrhZzrG/puL0Z/yn7ciaB197/pspdXbQdBmvkJzVdq1a0dUVBQRERGEh4czbNgwpk+fzldffaXbZ8qUKTg5OREZGcnMmTNxdHSkbdu2zJo1y6BjeHt7s3r1asLDw3nvvffo1KkT77//PoMHD66w78KFC3nppZc4e/YsQUFBbNmyRVcG1aFDB+Li4pg3bx5du3ZFpVLpbmyrcvnyZa5du3bX+FasWMHMmTMZMmQI+fn5PP3003z00Ud6P7cajYaXX36ZuXPnGnTOhvLx8WH//v3MnTuXJ598koKCAnx9ffVKGaOiopg4cSIhISG4uLgwd+5crl69WmWbKpWKb7/9ltmzZ/OPf/yDnJwcmjVrppvitWfPnowdO5ZBgwZx9epVFi5cWOG8LCws+P7775k6dSpdu3bF0tKSfv36sWzZMqPOLzs7m9OnT+tte/XVV/Vm3nrkkUcAOHDgQIXpf42lKjF2dQ0hBGiLIOGz0pIfc+PZAbw6KB2FeIBs2bKFwYMHY2Hx19oCxcXFutVd8/Pzde+tW7eO8ePHc/nyZdzd3atsc+nSpcycOVNvEGRxcTFqtZomTZpUWjqwe/du5s6dS9i2jUS89gYqSwsefm0Gt5PPcGTYeLol7DHdSd8Hdio13zdpjYWRA0jrkry8PFJSUvDz86vVqSJF3XXz5k1cXV3Zs2eP3oBkUTuM+ZmUngYhqkNtCY4auHlZ6UiMl5UEno+ASqoTRe144okn9KYRBHj++edp3rw5r776ql4yERMTw8CBA++aMACEhYVVGBvRu3dvwsLCeP755yvsn5eXx+TJk1nz+ToW5F6npLgY/vfMTFtYVPp1HdfCxt6sEwYhKvPTTz8xYMAASRjMgCQNQlSX80PmmTQU5pROv9qw+tPFCWEMJyenCnW6Dg4OuLq66m0/c+YMe/fuZfv27ZW207x5cxYuXMjgwYNxdXXF1VV/+mMrKys0Gk2l85bPnz+ffv36kR3ox+0/L+HyaHvOLlyK5h+hXF67AZdH21f4TF1Tb0uTxANt0KBBDBo0SOkwhAEkaRCiulyawOXflY6iejJPStIg6pw1a9bg7e2tt3pseadPn+b69etGt3v8+HG+/vprEhISmHGjNNF3D3mKa7/9wZFh47D3/xstou49QFtpj9ia1/SwQoj6RcY0CFETiV9A4W2lo6ieVsPAtoHSUQhRa5Lyb/NSunnOINbIwpKvvVuiNvPypLL6aV9fX5PMdy+EqJnc3FzOnz9v0JgGKWoWoiacvZWOoPoyTyodgRC16tubfyodQrV1t3Mx+4QB0I1fMWTRMSHE/Vf2s1h+bFlVpDxJiJpwbgJ/JisdRfX8mQxeHWWFaPFAuF5cxO7bd58asi7r4VA/egUtLS2xt7cnMzMTKysrvdmvhBC1S6vVkpmZib29vUErkEvSIERNOHsDKsAMq/yKC0oTB4+KK+kKUd9svfUnheb4cwo0VFvStp4MglapVHh6epKSksKFCxeUDkeIB55arcbHx6fK9W7Kk6RBiJqwtAUHd7idoXQk1ZN+BNwCS6eQFaKeullcxFc3MpUOo9q629eP0qQy1tbWNG3aVEqUhKgDrK2tDe7xkzsFIWqqga/5Jg2FOZBxHDR1f7pJIapr/Y0Mbmnr/joMVakvpUnlqdVqWdxNCDMjxYRC1FSjppSWKJmp9AQoylc6CiHui8yiQjbfzFI6jGqrT6VJQgjzJkmDEDVl7VC60Ju5Ki4oTRyEqIfWXk+nwIxnFu9u7yKrQAsh6gRJGoQwBddmSkdQMxnHoeCW0lEIYVIXC/PYfuuq0mHUyOP2LkqHIIQQgCQNQphGA1+wsFE6iuorKYa0w0pHIYRJrbmWjlbpIGqggdqSdrIKtBCijpCkQQhTUFtAo4eVjqJmsk5DnvnOYy9Eeafyc9iTc13pMGpESpOEEHWJJA1CmIq5lyhRApcPKR2EECax6lqa0iHUWA8pTRJC1CGSNAhhKg4eYNtQ6Shq5loK3DT/my3xYDuQc4PDeeY9RsfNQkqThBB1iyQNQpiSa1OlI6i5C3tAW6R0FEJUy83iIiKvpiodRo31d3SV0iQhRJ0iSYMQpuTaDLNeswEg/4aUKQmztSL7v2QVm3fSa4mKAU6uSochhBB6JGkQwpSs7M17zYYyGcfgVrrSUQhhlN9ybrDzdrbSYdTYY/YuNLKwUjoMIYTQI0mDEKZm9gOi/+d8nJQpCbNxs7iID+pBWRLAICc3pUMQQogKJGkQwtQa+Jb2OJg7KVMSZqQ+lCUBNLW2o42tg9JhCCFEBZI0CGFqagvwaKN0FKYhZUrCDNSXsiSAUEcZyyCEqJskaRDifnBvad4rRJcnZUqiDqtPZUkN1ZY85Wjm0zaXExwczNy5cw3ef8eOHahUKvLy8mp0XI1GQ3R0dI3aMAd3Xl9TnPeDcu1E9UjSIMT9YGEFHq2VjsI08m/A5YNKRyFEpepLWRLAEGc3rFXV+7OsUqnu+ho7dqxpg61ncnJymDRpEq6urjg6OjJkyBDS0oxbsyY6Olp3vdVqNV5eXowcOZLU1NpJao8dO8aYMWMM2jc6OhqNRlOjNmoiKyuLkSNH4uzsTIMGDRg3bhw3b96878cVNSNJgxD3i0drUNeTGVAyjsPVs0pHIYSe72/+WW/KkuxU6hqVJqWlpeleS5cuxdnZWW9bVFRUpZ8rLCys9jHrk8mTJxMbG8vGjRvZs2cPmZmZDBo0iJKSEqPacXd3Jy0tjUuXLrFu3Tp+//33u7Zjyuvv7u6OnZ2d4m0YYtiwYZw+fZoff/yRrVu38uuvvzJu3Lj7flxRM5I0CHG/WNqUlinVF+fjICdL6SiEAOBo3i2irl5SOgyTCXFshJOFZbU/r9FodC8XFxdUKlWFbadOnUKlUrF582a6d++OjY0NGzduZO7cuQQHB+u1t2jRIpo3b663beXKlQQGBmJra0uLFi1YtWqVUTGuWbOGoKAgHB0d8fT05LnnniMrq+LvlLi4OFq3bo2trS1dunQhKSlJ7/29e/fStWtX7Ozs8PHxYdasWeTm5hoVS3lZWVmsW7eOqKgoevbsSYcOHVi3bh0HDx5k7969RrWlVqvRaDR4eXnxxBNP8MYbb3D48GEuXrxIXl4eKpWKmJgY+vXrh729Pe+//z5Q+oS/d+/eODg44Onpybhx48jO/ishvnnzJs8++ywODg54e3uzfPnyCsf+/+3deXDU9eH/8ddnN9nsbu5sLhITQwygnApii4IitWCpgHbwRy3QL7W1akeLWLXz9dfWimOtzoj3aPvz+FZtO4714KsUFTkUayyWK4iBCOFIkAQCIeRO9vj9sbASSJYEkv3s8XzMMMDmk91XlkzY176vk6cWHTp0SDfeeKOys7PlcDg0evRovf/++3rvvfd06623qra2NjAy8sc//rHb+6isrNQ111yjxMREpaWl6Uc/+lGXf7Pj3zsvvviiCgsLlZaWpvnz56u5ubnH52jjxo1avXq1XnrpJV1yySWaOHGinnvuOf3jH//Q7t27+/R8I7QoDcBAyhktGVazU/QPn0fa8b7U2WJ2EsS4GneH7ju4Wx6zg/QTq6TrU7JC9ni//vWvddddd2nbtm268sore/U5Tz31lB588EE98sgjKi8v1+LFi3X33Xfrtdde6/Xjut1uPfTQQyorK9Mbb7yh8vJy/fznPz/lunvuuUdPPvmk1q1bp5SUFM2aNUsej/9fe/369Zo+fbpuuOEGbdmyRX/961+1YsUK3XnnnT0+7g9/+ENdffXVPX583bp18ng8mjp1auC2oqIiDR06VJ9++mmvv77uHH/X/sQRhd/85jeaM2eOtm7dqrlz56qqqkpXXHGFJkyYoA0bNujdd99VZWWl5s6dG/ichQsXqrS0VO+8846WL1+ud999V1u3bu3xcY9/PRs3btTf//53bd26VQ888IAsFoumTJmihx9+ODAqsn//ft1+++3d3seMGTPU0tKiTz75RMuXL9fWrVs1b968Ltd9+eWX+uCDD7R8+XK99dZbeu+997RkyZLAx5977jnZ7fbA30tLS5Wdna0xY8YEbrv88stlt9tVWlrah2cXoXbmb2sAOL14h5R5vnSw5x/uEaWzWdq5Qhp6jX+XKCDEWr0e/ebALjV4o6UySJOdacqJs4Xs8e666y7NmjWr19f7fD49+OCDevbZZwOfN3jwYG3evFl/+tOfNGfOnF7dz4kFobi4WEuWLNHkyZPV0dEhm+2br/+BBx7QlClTJEkvv/yyCgoKtGzZMs2cOVMPP/ywfvrTn+q2226TJJWUlGjJkiWaPn26nnrqKcXFnfqyJj8/X8nJyT3mqqmpUXJy8inTcnJyclRTc+a7x+3Zs0dLlizR4MGDVVxcrI6ODknSggUL9OMf/zhw3T333KNJkybp97//feC2559/XkOGDNHevXuVlJSkl19+WW+88UbgeXnllVdUUFDQ42MvW7ZMZWVlqqio0ODBgyX5n/PjUlJSAqMiPfnnP/+pHTt2aOXKlYHrXnrpJY0bN05btmzRqFH+XQINw9CLL74op9OpESNG6IYbbtDKlSv129/+VpKUnp6uYcOGBe63pqZGOTk5XR7LYrEoKyvrrJ5vDDxKAzDQcsdIdeWSz2t2kv7RXCvt/UQqusLsJIgxXp9PfzxUpZ2dZ7e7TjixSpqflnPa6/rTxRdf3Kfrq6urVVtbq3nz5skwjMDtbrf7lBd/wXz++ee6//77VVZWpvr6enm9Xnm9XlVXV3d5QTthwoTAn7Ozs1VcXKzy8nLNnDlT69ev1759+/TCCy8ErvH5fOrs7FRVVVXgBfKJHn300dNmO/HrOvF+u7s9mAMHDigpKUler1etra0aP3683nzzTVks30zsOPn5X79+vdauXaukpKRT7m/nzp1yOp3yeDzdPi892bRpk4qLi7t9PnqrvLxcxcXFXYrF2LFj5XA4VF5eHigNJSUlcjq/OZto0KBB+vDDDwN/nzNnzinFsr+eb4QWpQEYaLYkKWOIdGi72Un6z6HtktMVPTtEISK80lCrj1sazI7Rr76f5NK58fbTX9iPEhO7Hh5nsVhOWah74nQar9f/hsdf/vIXXXjhhV2u6+6d/e40NDRo2rRpmjVrlv72t78pKytLFRUVmjlzZuAd+GCOv5j0er26/fbbdfPNN59yzTnnnNOrLCfLzc3V0aNH1dra2mW04cCBA30qRZLkcrlUWloaeBf/xBfTx538/Hu9Xs2ePVuLFy8+5dq8vDyVlZX1KYOkflnM3NOL+JNvj4/vuuGHYRiB75nu5ObmnjKi4PV6VVdX1+fnG6FFaQBCIfdC6VCFpL7txBHWqkole5qUcmb/UQN98XHLEf1PQ63ZMfpVomHRT9J6nh4SKsfntp9o06ZNgT8XFBQoMzNTu3bt0uzZs8/oMb744gvV19frkUceUVaWf/3G2rVru732s88+08yZMyX5X7hXVlYGFmWPHTtWX375pUpKSs4oR3cuueQSWa1WrVixIvC4e/bsUUVFhS699NI+3ZfVau1ztrFjx2rFihUqLi7uMiJx3NChQ2W1Wrt9XnoyevRoVVZWavfu3SoqKjrl4zabLbBOpCfDhw/Xzp07VVNTExht2LBhg9ra2nTBBRf04SvsasKECTpw4IDKyso0evRoSf7vhba2ti6jKQg/LIQGQsGeKqX3PJQcmXxS5Uqp7YjZQRDldnS06qG66DjA7UQ3pGYr7Sx2TOovU6ZMUVVVlR577DHt2LFDjz/+uFatWhX4uMVi0X333afFixfrmWeeUUVFhcrKyvTCCy90u4tPd4qKihQXF6cnn3xSu3bt0ptvvhnYsedkv/vd77RmzZrAmQGFhYWaPn26JOnee+/Vhx9+qEWLFmnz5s2qqKjQ22+/rUWLFvX42L/61a9000039fjxzMxMzZ8/XwsXLtSaNWu0YcMGzZ8/X+PHj9fll1/eq6/vbCxcuFDV1dWaN2+e/vOf/2jnzp167733AluQZmRkaP78+Vq0aJHWrFmjsrIyLViwoMs6kJNNnTpV48eP13XXXadVq1Zp165dWrZsWWDaUFFRkQ4fPqy1a9eqrq6u292npk+frpKSEs2dO1ebNm1SaWmpfvKTn2jatGkaObL3o8yvvfZal0XPF110kSZPnqwbb7xR69at06effqpbb71Vs2fP7rbgIHxQGoBQyRsnneHBTWHL0y5VLJPaOZQHA2N3R5vurq1UW7SsCTomxxof0h2TghkzZoyeeOIJPfroo7rooou0ZcsWLVy4sMs1t912m55++mn9+c9/1qhRo3TllVfq1Vdf7fWc+fz8fD3//PN6+eWXdcEFF+jxxx8PbDd6soceeki/+MUvdPHFF+vIkSN6++23A9Ogxo0bF3jhfNlll2ncuHG6//77lZ+f3+Nj79u377QHrD3zzDOaNm2afvCDH2jSpElyuVxaunRpl2k4ubm5PRads1FYWKh//etfam5u1lVXXaVRo0bpzjvvlMv1zbkdTzzxhMaPH6/p06dr2rRpmjZtmkaMGNHjfRqGoaVLl2r06NG6/vrrNXz4cN17772BaWhXXnmlFixYoGuvvVZZWVndnuNhtVr1zjvvyG6367LLLtPVV1+tESNG6NVXX+3T11dfX6/t27tOz3399ddVUlKi73znO/re976nb33rW13WqSA8Gb6+nlwC4MxVlUoHtpidov/ZkqVhM/zrN4B+Ut3Zrjtqd+hQlJz4fKJ7XYX6blK62THQS42NjXK5XProo4+YQoOYFWVvewJhLm+cFBfaRY8h0dHoH3HgDAf0k/3udt1ZuzMqC8P5NoeuSkwzOwb6YOXKlZoxYwaFATGNkQYg1A5+6d+yNBrZ06Wh35fiT90xBOitGneH7qzdqf3u0++qE4kezzlPY+yMygGILIw0AKGWeb7kyDA7xcBoq5e2vyN1NJmdBBFqX2e7FtbsiNrCMNGRQmEAEJEoDUCoGRapoG/b+EWU9gZ/cWBxNPpob2ebFtbu0AFP5+kvjkBxMnRzep7ZMQDgjFAaADMk50np55mdYuB0NErb/1dqi66DuDBwKjtadUdNdK5hOG5GskvnxCeYHQMAzgilATBLwQTJ2vM+2xGvs1navlRq3H/6axHT/t16VL+s2aF6b/QWhiSLVf+Vymm3ACIXpQEwS7xTyhtvdoqB5W6TKt6VDpabnQRh6rWGA/rvA7vUHGXnMJzsprRcpYbBQW4AcKYoDYCZsoZLzvA44Gng+KS9a/07RkX5C0P0XofPqz/U7dVzR/Yr2rfwu8SerJnJmWbHAICzQmkAzGQYUuFEScZpL414B7/0n+XgbjM7CUxW5+7UHTU7taK53uwoAy7ZYtXdrgKzYwDAWaM0AGZLzJJyRpmdIjSa9kvlb0mth81OApOUt7folpoKlXfExkGACzPylRkXb3YMADhrlAYgHOSNlxwus1OERkejtO1t6chus5MgxFY01WthzY6o3iHpRJOdqfpOYrrZMQCgX3AiNBAu2o5I5W9KUbyDzClyL5IGjZUsVrOTYAC1e736f0f2643GOrOjhIzLGqcXBg1j8TOAqEFpAMJJ3TZpz8dmpwgth0squkJyslA0Gn3R1qxHDlWpyt1udpSQeih7sL7tSDE7BgD0G0oDEG52rpCO7DI7RYgZ0qCL/CMPjDpEhXavVy821Oj1owejfnekk30/KUN3sfgZQJShNADhxt0uffkP/+FosYZRh6gQq6MLkpQXZ9Pzg4bKQfkFEGUoDUA4atzvPxQt5t6jlRh1iFyxPLog+XcWeSznPI22J5kdBQD6HaUBCFf7PpdqNpqdwjyMOkSUWB5dOG5OSpZuSc8zOwYADAhKAxCufF5p+ztSc63ZSUxkSJnn+3dYsiWaHQbdqHF36H+O1OiD5vqYHF04bojNoadzS2Qz2MkcQHSiNADhrL3Rv77B22l2EnMZVv8BeDljpLgEs9NA0hGPW6821Op/Gw+pM6brgpRhjdNzuUOUFWczOwoADBhKAxDuDu+Qdq0yO0V4sCZIuRdK2SMkC/vfm6HF69HrRw/qtaMH1erzmh3HdDbD0GM552l4AiNhAKIbpQGIBNX/lmo3m50ifMQnSnnjJNdQiekgIdHh8+rdxkN6peGAjsTSAYSn8X8zC3UVpz4DiAGUBiAS+HzSrpVSfaXZScJLQqqUP15KK6I8DBC3z6dVzfV6qaFWNe4Os+OElbkp2fpZ+iCzYwBASFAagEjhdUsVy2J8YXQPbElS1nDJNUyKd5idJirUuTu1rOmQ3m06pDoPIwsnm+RI1f1Z58owDLOjAEBIUBqASOJuk7a9LbUfNTtJeDIsUnqxv0Ak5ZqdJiJtbGvS0sY6rW1pECsWulcSb9eTuSUc4AYgplAagEjTdkTatlTyxO5++L3icPnLQ0aJZI03O01Ya/J6tKKpXkub6rSnk++rYDKscXo2d4iy2SkJQIyhNACRqHG/9NUy/1kOCM4SL2UOkzKGSIlZZqcJG16fT+UdLXq/qV4rmuvVxvfSacXL0OO57JQEIDZRGoBIdegrafdqs1NEjKY4h25yjdN4R4oudaZorD0p5g7iavV6tKGtSZ+2HlVpy1HVswtSn9zrKtR3k9gpCUBsojQAkezr9dL+9WaniAjLXSP1iPHNO8R2w6Lx9mRNcKbo245kpUfpFKY6d6c+az2qf7U2aENbkzr4kX9GfpSSrZvYKQlADON0JCCS5Y2TOhqlQxVmJwl7q+LTJPc3J2u3+bxa29qgta0NMiSNSHDq244UjUxI1BCbQ84IXeTa5PWooqNFX7S1qLS1Qds6Ws2OFPGuScrQz9JYWA8gtjHSAEQ6n1f6arnUuM/sJGGrPj5Js1NH9Ho3IEPSOXEJGpbg0FCbU0NtDpXYHEoMsyJxvCBUtLeqoqNV2zta9DVnKfSrGUkuLcrIZ2tVADGP0gBEA69b2vmBdLTa7CRh6a3MMXpS9rO6jxOLxBCbQzlWm1zWeLni4uSyxg/Y+og2r1eHPJ067HHrkKdTNe4OCkKIzExy6Q4KAwBIojQA0cPrkSpXSA17zU4Sdn6ZO1FbTpiaNBCSLVa5rP4C4bLGK+PYnx2GRXGGIathyCpDx8cqPJI88snt88nj86nF59VhT6fqTigIde5ONbOrkSkoDADQFaUBiCY+r1T5oXRkt9lJwsYBW5p+mDJM/KBDb81KcmkhhQEAuoit/QaBaGdYpOKrpPTzzE4SNlalFlMY0GsUBgDoHqUBiDaGRRo8xX+YGbTK6jQ7AiLErCSX7nCdQ2EAgG5QGoBoZBhS0WQp83yzk5iqyp6prwZ4LQOiw7XJ/sIAAOgepQGIVoYhFU6SsoabncQ0q1KKzI6ACHBdsksLMygMABAMpQGIZoYhFU6UskeZncQUq40EsyMgzM1OztQvKQwAcFqcCA3EgoIJkiVOqtlodpKQ2ZE4SHs8brNjIExZJd2Wka9rkzPNjgIAEYHSAMSK/PGSPU3a87Hk85idZsCtSirwH4YAnCTFYtV9medqrCPZ7CgAEDEoDUAscQ2R7KnSzhVSZ7PZaQbUasVLYqQBXRXGJegP2YOVH8/UNQDoC9Y0ALEmMVu64DopMcfsJANma3KBapiahJNcYk/WM4OGUBgA4AxQGoBYFO+Uhl4juYaanWRArHLmmx0BYeb65Cz9IXuwkixWs6MAQERiehIQqyxW/1kODpdU/ZkUJecme2Rojc8iFjRAkuJlaJHrHH0vKcPsKAAQ0SgNQKzLGSU5MqTKDyVPu9lpztqmlCId9lIYIKVZ4rQ4q0ij7IlmRwGAiMf0JABSSr5/nYM93ewkZ221M9fsCAgDxfF2PTtoCIUBAPoJpQGAX0KKdP61UlqR2UnOmNuw6mMGGWLedxPT9XRuiXLjbGZHAYCowfQkAN+wxkvnTZXqtklVpZK30+xEffJ5arEafV6zY8AkaZY4LXLl63JnmtlRACDqUBoAnCrzfCk5X9rzkdT4tdlpem2lPUtyM9QQiyY6UnSn6xylW+PNjgIAUcnw+XzRsWUKgP7n80kHt0r71kne8D73oN2I03WZ49XKSENMSbJYdXt6nqayOxIADChGGgD0zDCk7JFSSoG0e43UXGt2oh59mj6EwhBjLrYn6R5XgbJYuwAAA47SAOD07KnSsBlSbZn09XrJF35TgFbbMiR3eI+GoH/YDYtuSR+kWcmZZkcBgJhBaQDQO4ZFyr1QSi30jzq01JmdKKDJatdnrGWICaMSEvVrV4Hy4xPMjgIAMYXSAKBvHBn+rVn3b5D2b1Q4nCT9SVqJOsMgBwZOvAz9NC1X16dkyWIYZscBgJhDaQDQd4ZFyrtYSi+Wqv8tHa0yNc6q+DTJHVnbw6L3LnOk6Ob0PBUwugAApmH3JABn72i1VP2Z1Ho45A99JD5Rs1NHislJ0ed8m0O3pOdpjD3J7CgAEPMoDQD6h88nHdou7ftccreG7GGXZo7R47KH7PEw8HKtNv0sPVdTnGkymIoEAGGB6UkA+odh+A+FXkOeGAAABPNJREFUSz9Pqt3s32kpBGc7rIpLZmpSlEiyWDUvJVvXpWTKZljMjgMAOAEjDQAGRkez9PXn0qGvNFCLpQ/aUjUn5XyWQEe4OBmalezSj1NzlGLlvSwACEf8dAYwMGyJUtFk/+Fw1Z9JjV/3+0OsTi0Wb3tEtiucqbopbRBbqAJAmKM0ABhYzkxp6DVSw16pZpPUVNNvd73KmsjUpAg1MsGpm9PyNNKeaHYUAEAvUBoAhEZqof9X8wH/eof6XTqbaUv77C5tpzBEFEPSBEeK5qRkaTQ7IgFARKE0AAitxGyp+CqpvVE68IVUt03y9v3F/6qUIsnb//HQ/+JlaGpSuv5PSpYK49npCgAiEQuhAZjL0+EvDrVbpM7mXn/agpyJ2uNhpCGcpVqsmpHk0nUpmcqwxpsdBwBwFigNAMKDzyvVV/rLQ8vBoJfudObqZ85zQxQMfXWBzalrk12anJjG1qkAECWYngQgPBgWKaPE/6txv3Rgi3/xtO/UOUirkgqZmhRmbIahKc40XZucqWEJTrPjAAD6GaUBQPhJHuT/5W6TDu+UDn/lX0B9zGojXtLAHxyH4AxJIxKcusKZpqmJ6ZyxAABRjOlJACJDW4N0+Cttb2vSLdZ0s9PELIukkQmJusKZqsudacqMY60CAMQCSgOAiLO9vUUftzToo5Yj2ufuMDtO1LNIGpOQpCsSUzXJmcqiZgCIQZQGABFtR0erPm45oo+aG7TX3W52nKhhlXSRPUmXO9M0yZmqNKYeAUBMozQAiBrVne3a3Nakze3NKmtrUi1bsvZJssWqUQmJutSRoonOVKVSFAAAx1AaAEStGneHNrc1qay9WZvbmpjKdJKCuASNSHBqZEKiRiQk6tz4BBmGYXYsAEAYojQAiBl17k6VtTdpc1uzNrc3aW9nu2LlB6DNMDTM5tTIBKdGJCRqZEIiIwkAgF6jNACIWQ0et7a0N2t3Z5uqOtu1z92uqs52HfV6zI52VmyGoVyrTUU2+7GC4NQQm0PxHLQGADhDlAYAOEmjx60qd7uqO9tV7e449nu79nW2q6Wbw+ZCzZCUYY3ToDibBsUlKC/OpkFxNuXF2ZQbl6BMaxzTjAAA/YrSAAB9cNjTqarOdtW6O9Ts86rF61GL16sWn0fN3mN/7+b2tpPKhkWS3bAowbDIbjn2u2Ec+92ihMBtFjksFmVb45UXl3CsKNiUYGHUAAAQOpQGAAgBj8+n1mPFIcEwmCoEAIgolAYAAAAAQfFWFwAAAICgKA0AAAAAgqI0AAAAAAiK0gAAAAAgKEoDAAAAgKAoDQAAAACCojQAAAAACIrSAAAAACAoSgMAAACAoCgNAAAAAIKiNAAAAAAIitIAAAAAIChKAwAAAICgKA0AAAAAgqI0AAAAAAiK0gAAAAAgKEoDAAAAgKAoDQAAAACCojQAAAAACIrSAAAAACAoSgMAAACAoCgNAAAAAIKiNAAAAAAIitIAAAAAIChKAwAAAICgKA0AAAAAgqI0AAAAAAiK0gAAAAAgKEoDAAAAgKAoDQAAAACCojQAAAAACIrSAAAAACAoSgMAAACAoCgNAAAAAIKiNAAAAAAIitIAAAAAIChKAwAAAICgKA0AAAAAgqI0AAAAAAiK0gAAAAAgKEoDAAAAgKAoDQAAAACCojQAAAAACIrSAAAAACAoSgMAAACAoP4/FUi9MtjsK7UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the values for the pie chart\n",
    "sizes = [44.7,47.4,3.48,4.42]\n",
    "\n",
    "# Define the labels for each value\n",
    "labels = ['True label: 1, Prediction:1','True label: 0, Prediction:0', 'True label: 1, Prediction:0', 'True label: 0, Prediction:1']\n",
    "\n",
    "# Define the colors for each value\n",
    "colors = ['#ffcc99','#40e0d0','#66b3ff','#ff9999']\n",
    "\n",
    "# Create the pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Draw a circle at the center of the pie chart\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Move the legend to the right of the pie chart\n",
    "ax1.legend(labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set the title of the pie chart\n",
    "plt.title(\"Performance of the Q-Fraud Detection System\")\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0855b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
