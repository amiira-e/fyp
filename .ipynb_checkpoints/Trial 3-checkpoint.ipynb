{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bbe5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca576dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6362620, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949ab2e",
   "metadata": {},
   "source": [
    "## Separate remaining data into transfer learning data and Out-sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23377b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def reservoir_sampling(iterable, k, header=True):\n",
    "    reservoir = []\n",
    "    for i, item in enumerate(iterable):\n",
    "        if i < k:\n",
    "            reservoir.append(item)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < k:\n",
    "                reservoir[j] = item\n",
    "    return reservoir\n",
    "\n",
    "# Open the input CSV file\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\") as f:\n",
    "    # Check if header line exists\n",
    "    header = True\n",
    "    first_line = f.readline()\n",
    "    if not first_line.startswith('step,type,amount,nameOrig,oldbalanceOrg,newbalanceOrig,nameDest,oldbalanceDest,newbalanceDest,isFraud,isFlaggedFraud'):\n",
    "        header = False\n",
    "        f.seek(0)  # Rewind file pointer to beginning\n",
    "\n",
    "    # Sample from remaining lines\n",
    "    sampled_lines = reservoir_sampling(f, k=2500000, header=header)\n",
    "\n",
    "# Open the output CSV file and write the subsample to it\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\", mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    if header:\n",
    "        writer.writerow(first_line.strip().split(','))\n",
    "    for line in sampled_lines:\n",
    "        writer.writerow(line.strip().split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329a5aa",
   "metadata": {},
   "source": [
    "## Pre-process larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed453d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf9f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500000, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_big.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846a5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['type'])\n",
    "label\n",
    "df_sample_big.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample_big[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameDest'])\n",
    "label\n",
    "df_sample_big.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameOrig'])\n",
    "label\n",
    "df_sample_big.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4fdb235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998712\n",
      "1    0.001288\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998712\n",
      "1    0.001288\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998712\n",
      "1    0.001288\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample_big.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample_big['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865f7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a30f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a4de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e882ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1506ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1557613, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8027782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0         304  157520.530000   47407.850000             0.0            0.00   \n",
      "1         153  267217.000000       0.000000             0.0        16211.49   \n",
      "2         273    1991.940000   48955.000000             0.0            0.00   \n",
      "3         304    6426.000000   47407.850000             0.0            0.00   \n",
      "4         158  191095.750000    1307.000000             0.0        16211.49   \n",
      "...       ...            ...            ...             ...             ...   \n",
      "1557608   530  123629.480000   47407.850000             0.0        16211.49   \n",
      "1557609   273  384004.649679   47407.850000             0.0        16211.49   \n",
      "1557610   273   82601.112113   82601.112113             0.0            0.00   \n",
      "1557611   408   61046.702672   61046.702672             0.0            0.00   \n",
      "1557612   273  123629.480000   47407.850000             0.0            0.00   \n",
      "\n",
      "         newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0             157520.53               0     1    217690    278456  \n",
      "1             162782.57               0     1    363321   2358429  \n",
      "2                  0.00               0     3    574325    349645  \n",
      "3                  0.00               0     3    579320    115605  \n",
      "4             162782.57               0     0    402727    967130  \n",
      "...                 ...             ...   ...       ...       ...  \n",
      "1557608       162782.57               0     1     82537   1828827  \n",
      "1557609       162782.57               0     1    160484   1515318  \n",
      "1557610            0.00               0     1    440182    173344  \n",
      "1557611            0.00               0     1    287575    783945  \n",
      "1557612            0.00               0     1    225509   2411980  \n",
      "\n",
      "[1557613 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "447306    395   75632.19       31680.00             0.0            0.00   \n",
      "171889    240    1012.91       18951.00             0.0            0.00   \n",
      "20848     379  237419.78       14355.94             0.0       135556.91   \n",
      "1087770   329   10440.17       21528.00             0.0            0.00   \n",
      "1601207   210    2164.89       14355.94             0.0            0.00   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "1879180   164  209355.58        4044.00             0.0       135556.91   \n",
      "1640974   240   75632.19       14355.94             0.0       135556.91   \n",
      "289095    210     385.26       20803.00             0.0            0.00   \n",
      "509185     20   94365.39       20933.00             0.0            0.00   \n",
      "1423381   139   75632.19           0.00             0.0       135556.91   \n",
      "\n",
      "         newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "447306       251100.930               0     1  329224.0    409132  \n",
      "171889            0.000               0     3  631512.0    324590  \n",
      "20848        217266.905               0     1  213313.0   1664496  \n",
      "1087770           0.000               0     3  339047.5    116982  \n",
      "1601207           0.000               0     3  339047.5   2137489  \n",
      "...                 ...             ...   ...       ...       ...  \n",
      "1879180      217266.905               0     1  175481.0   2401408  \n",
      "1640974      217266.905               0     0  408388.0   1505546  \n",
      "289095            0.000               0     3  497151.0    766560  \n",
      "509185        94365.390               0     1   15954.0    160961  \n",
      "1423381      217266.905               0     1    1109.0   1125336  \n",
      "\n",
      "[250000 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ca43564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da522fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big = df_sample_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1c38e",
   "metadata": {},
   "source": [
    "### Big dataset: Pre-train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d217322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define number of folds for cross-validation\n",
    "num_folds = 3\n",
    "\n",
    "# create KFold cross-validation object\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# create arrays to store training and validation loss for each epoch\n",
    "train_losses = np.zeros((num_folds, 10))\n",
    "val_losses = np.zeros((num_folds, 10))\n",
    "\n",
    "import time\n",
    "\n",
    "# loop over the folds\n",
    "fold_no = 1\n",
    "for train, val in kfold.split(X_train_resampled_final, y_train_resampled_final):\n",
    "    \n",
    "    # create model\n",
    "    pre_trained_model = Sequential()\n",
    "    # add convolutional layer\n",
    "    pre_trained_model.add(Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=(10, 1)))\n",
    "    # add pooling layer\n",
    "    pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "    # flatten output to feed into fully connected layer\n",
    "    pre_trained_model.add(Flatten())\n",
    "    # add fully connected layer\n",
    "    pre_trained_model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt_new = Adam(lr=0.00033)\n",
    "    # compile model\n",
    "    pre_trained_model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "    \n",
    "    # record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # train model for each fold\n",
    "    history = pre_trained_model.fit(X_train_resampled_final[train], y_train_resampled_final[train],\n",
    "                                     epochs=10, batch_size=32, validation_data=(X_train_resampled_final[val], y_train_resampled_final[val]))\n",
    "    \n",
    "    # record end time and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'Fold {fold_no} elapsed time: {elapsed_time:.2f} seconds')\n",
    "    \n",
    "    # store training and validation loss for each epoch\n",
    "    train_losses[fold_no-1] = history.history['loss']\n",
    "    val_losses[fold_no-1] = history.history['val_loss']\n",
    "    \n",
    "    # increment fold number\n",
    "    fold_no += 1\n",
    "    \n",
    "# calculate mean training and validation loss across all folds for each epoch\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "\n",
    "# plot training and validation loss curves for each epoch\n",
    "plt.plot(mean_train_loss, label='Training Loss')\n",
    "plt.plot(mean_val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
