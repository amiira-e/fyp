{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bbe5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca576dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6362620, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949ab2e",
   "metadata": {},
   "source": [
    "## Separate remaining data into transfer learning data and Out-sampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23377b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def reservoir_sampling(iterable, k, header=True):\n",
    "    reservoir = []\n",
    "    for i, item in enumerate(iterable):\n",
    "        if i < k:\n",
    "            reservoir.append(item)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < k:\n",
    "                reservoir[j] = item\n",
    "    return reservoir\n",
    "\n",
    "# Open the input CSV file\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\") as f:\n",
    "    # Check if header line exists\n",
    "    header = True\n",
    "    first_line = f.readline()\n",
    "    if not first_line.startswith('step,type,amount,nameOrig,oldbalanceOrg,newbalanceOrig,nameDest,oldbalanceDest,newbalanceDest,isFraud,isFlaggedFraud'):\n",
    "        header = False\n",
    "        f.seek(0)  # Rewind file pointer to beginning\n",
    "\n",
    "    # Sample from remaining lines\n",
    "    sampled_lines = reservoir_sampling(f, k=2500000, header=header)\n",
    "\n",
    "# Open the output CSV file and write the subsample to it\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\", mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    if header:\n",
    "        writer.writerow(first_line.strip().split(','))\n",
    "    for line in sampled_lines:\n",
    "        writer.writerow(line.strip().split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329a5aa",
   "metadata": {},
   "source": [
    "## Pre-process larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed453d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf9f6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500000, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample_big.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "846a5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['type'])\n",
    "label\n",
    "df_sample_big.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample_big[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameDest'])\n",
    "label\n",
    "df_sample_big.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameOrig'])\n",
    "label\n",
    "df_sample_big.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4fdb235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.9987\n",
      "1    0.0013\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.9987\n",
      "1    0.0013\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.9987\n",
      "1    0.0013\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample_big.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample_big['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865f7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a30f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a4de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e882ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1506ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1561503, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8027782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0         306  174553.150000   19924.000000             0.0             0.0   \n",
      "1          95   36629.510000    6309.610000             0.0             0.0   \n",
      "2         282   14903.000000       0.000000             0.0             0.0   \n",
      "3         133   51303.560000       0.000000             0.0         16312.0   \n",
      "4         371  121634.620000   67062.000000             0.0         16312.0   \n",
      "...       ...            ...            ...             ...             ...   \n",
      "1561498   236  121634.620000   48639.000000             0.0             0.0   \n",
      "1561499   434  121634.620000   48639.000000             0.0         16312.0   \n",
      "1561500   149  312745.914508   48639.000000             0.0             0.0   \n",
      "1561501   274   19410.857831   19410.857831             0.0             0.0   \n",
      "1561502   381  121634.620000   48639.000000             0.0             0.0   \n",
      "\n",
      "         newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0             174553.15               0     1    218455   1963077  \n",
      "1                  0.00               0     3    294316    554354  \n",
      "2                  0.00               0     3    294316    830243  \n",
      "3             160828.71               0     1    222216    261861  \n",
      "4             160828.71               0     1    114115    900673  \n",
      "...                 ...             ...   ...       ...       ...  \n",
      "1561498            0.00               0     1    135650   1195993  \n",
      "1561499       160828.71               0     1    104679    398524  \n",
      "1561500            0.00               0     1    194199    681616  \n",
      "1561501            0.00               0     1    313616    923295  \n",
      "1561502            0.00               0     1    107470    872771  \n",
      "\n",
      "[1561503 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "2327183   188   37915.51           0.00             0.0       131079.93   \n",
      "2115284   331  141048.53       20063.00             0.0       131079.93   \n",
      "524689    394   51139.01           0.00             0.0       131079.93   \n",
      "187448    256  102696.12           0.00             0.0       278610.52   \n",
      "1450539   140  112284.98       14452.34             0.0            0.00   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "1280245   135    5240.02       16374.00             0.0            0.00   \n",
      "381930    212   74581.47           0.00             0.0       131079.93   \n",
      "1936943   302   74581.47       14452.34             0.0            0.00   \n",
      "2315369   188   36646.93       14452.34             0.0       131079.93   \n",
      "1833654   163   19823.39           0.00             0.0            0.00   \n",
      "\n",
      "         newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "2327183       213184.43               0     1    409381    890858  \n",
      "2115284       213184.43               0     1    413011   1323909  \n",
      "524689        609318.76               0     1     48816    575330  \n",
      "187448        381306.64               0     1    338051   1703093  \n",
      "1450539       112284.98               0     1    418121     18729  \n",
      "...                 ...             ...   ...       ...       ...  \n",
      "1280245            0.00               0     3    339464   1304309  \n",
      "381930        213184.43               0     1    340614   1104868  \n",
      "1936943       289103.28               0     1    334127    113588  \n",
      "2315369       213184.43               0     0    423613   1008077  \n",
      "1833654            0.00               0     3    339464   1059589  \n",
      "\n",
      "[250000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca43564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dd34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da522fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big = df_sample_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1c38e",
   "metadata": {},
   "source": [
    "### Big dataset: Pre-train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0384fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000015B8238AD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000015B8238AD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000015B8238AD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24375/24399 [============================>.] - ETA: 0s - loss: 0.1515 - accuracy: 0.9328WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000015B91EBBF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000015B91EBBF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000015B91EBBF78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24399/24399 [==============================] - 70s 3ms/step - loss: 0.1514 - accuracy: 0.9328 - val_loss: 0.1231 - val_accuracy: 0.9456\n",
      "Epoch 2/10\n",
      "24399/24399 [==============================] - 70s 3ms/step - loss: 0.1117 - accuracy: 0.9514 - val_loss: 0.1003 - val_accuracy: 0.9566\n",
      "Epoch 3/10\n",
      "24399/24399 [==============================] - 70s 3ms/step - loss: 0.0937 - accuracy: 0.9611 - val_loss: 0.0872 - val_accuracy: 0.9641\n",
      "Epoch 4/10\n",
      "24399/24399 [==============================] - 69s 3ms/step - loss: 0.0836 - accuracy: 0.9663 - val_loss: 0.0792 - val_accuracy: 0.9686\n",
      "Epoch 5/10\n",
      "24399/24399 [==============================] - 68s 3ms/step - loss: 0.0780 - accuracy: 0.9691 - val_loss: 0.0747 - val_accuracy: 0.9707\n",
      "Epoch 6/10\n",
      "24399/24399 [==============================] - 71s 3ms/step - loss: 0.0747 - accuracy: 0.9709 - val_loss: 0.0740 - val_accuracy: 0.9711\n",
      "Epoch 7/10\n",
      "24399/24399 [==============================] - 70s 3ms/step - loss: 0.0723 - accuracy: 0.9720 - val_loss: 0.0716 - val_accuracy: 0.9719\n",
      "Epoch 8/10\n",
      "24399/24399 [==============================] - 69s 3ms/step - loss: 0.0705 - accuracy: 0.9728 - val_loss: 0.0682 - val_accuracy: 0.9739\n",
      "Epoch 9/10\n",
      "24399/24399 [==============================] - 68s 3ms/step - loss: 0.0690 - accuracy: 0.9738 - val_loss: 0.0675 - val_accuracy: 0.9748\n",
      "Epoch 10/10\n",
      "24399/24399 [==============================] - 73s 3ms/step - loss: 0.0677 - accuracy: 0.9745 - val_loss: 0.0678 - val_accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 elapsed time: 698.29 seconds\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000015B923DD8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000015B923DD8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000015B923DD8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24382/24399 [============================>.] - ETA: 0s - loss: 0.1530 - accuracy: 0.9318WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000015B91BBAE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000015B91BBAE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000015B91BBAE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "24399/24399 [==============================] - 71s 3ms/step - loss: 0.1530 - accuracy: 0.9318 - val_loss: 0.1237 - val_accuracy: 0.9465\n",
      "Epoch 2/10\n",
      "24399/24399 [==============================] - 70s 3ms/step - loss: 0.1124 - accuracy: 0.9516 - val_loss: 0.1008 - val_accuracy: 0.9571\n",
      "Epoch 3/10\n",
      "24399/24399 [==============================] - 69s 3ms/step - loss: 0.0932 - accuracy: 0.9616 - val_loss: 0.0864 - val_accuracy: 0.9657\n",
      "Epoch 4/10\n",
      "24399/24399 [==============================] - 72s 3ms/step - loss: 0.0824 - accuracy: 0.9673 - val_loss: 0.0796 - val_accuracy: 0.9683\n",
      "Epoch 5/10\n",
      "24399/24399 [==============================] - 68s 3ms/step - loss: 0.0763 - accuracy: 0.9705 - val_loss: 0.0760 - val_accuracy: 0.9712\n",
      "Epoch 6/10\n",
      "24399/24399 [==============================] - 68s 3ms/step - loss: 0.0723 - accuracy: 0.9724 - val_loss: 0.0711 - val_accuracy: 0.9740\n",
      "Epoch 7/10\n",
      "24399/24399 [==============================] - 68s 3ms/step - loss: 0.0695 - accuracy: 0.9740 - val_loss: 0.0688 - val_accuracy: 0.9746\n",
      "Epoch 8/10\n",
      "24399/24399 [==============================] - 69s 3ms/step - loss: 0.0675 - accuracy: 0.9748 - val_loss: 0.0667 - val_accuracy: 0.9750\n",
      "Epoch 9/10\n",
      "24399/24399 [==============================] - 88s 4ms/step - loss: 0.0660 - accuracy: 0.9757 - val_loss: 0.0653 - val_accuracy: 0.9760\n",
      "Epoch 10/10\n",
      "24399/24399 [==============================] - 119s 5ms/step - loss: 0.0646 - accuracy: 0.9762 - val_loss: 0.0643 - val_accuracy: 0.9769\n",
      "Fold 2 elapsed time: 762.32 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define number of folds for cross-validation\n",
    "num_folds = 2\n",
    "\n",
    "# create KFold cross-validation object\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# create arrays to store training and validation loss for each epoch\n",
    "train_losses = np.zeros((num_folds, 10))\n",
    "val_losses = np.zeros((num_folds, 10))\n",
    "\n",
    "import time\n",
    "\n",
    "# loop over the folds\n",
    "fold_no = 1\n",
    "for train, val in kfold.split(X_train_resampled_final, y_train_resampled_final):\n",
    "    \n",
    "    # create model\n",
    "    pre_trained_model = Sequential()\n",
    "    # add convolutional layer\n",
    "    pre_trained_model.add(Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=(10, 1)))\n",
    "    # add pooling layer\n",
    "    pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "    # flatten output to feed into fully connected layer\n",
    "    pre_trained_model.add(Flatten())\n",
    "    # add fully connected layer\n",
    "    pre_trained_model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt_new = Adam(lr=0.00033)\n",
    "    # compile model\n",
    "    pre_trained_model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "    \n",
    "    # record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # train model for each fold\n",
    "    history = pre_trained_model.fit(X_train_resampled_final[train], y_train_resampled_final[train],\n",
    "                                     epochs=10, batch_size=32, validation_data=(X_train_resampled_final[val], y_train_resampled_final[val]))\n",
    "    \n",
    "    # record end time and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'Fold {fold_no} elapsed time: {elapsed_time:.2f} seconds')\n",
    "    \n",
    "    # store training and validation loss for each epoch\n",
    "    train_losses[fold_no-1] = history.history['loss']\n",
    "    val_losses[fold_no-1] = history.history['val_loss']\n",
    "    \n",
    "    # increment fold number\n",
    "    fold_no += 1\n",
    "    \n",
    "# calculate mean training and validation loss across all folds for each epoch\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "\n",
    "# plot training and validation loss curves for each epoch\n",
    "plt.plot(mean_train_loss, label='Training Loss')\n",
    "plt.plot(mean_val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd75dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model.save('pre_trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cc733df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "pre_trained_model = load_model('pretrained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "172b1326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 8, 32)             128       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 4, 32)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 257\n",
      "Trainable params: 257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "print(pre_trained_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e1c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Conv1D, LSTM, Dense, Activation\n",
    "# from keras.models import Model\n",
    "\n",
    "# # Define input shape\n",
    "# n_features = F.shape[1]  # number of input features\n",
    "# input_shape = (None, n_features)\n",
    "\n",
    "# # Define input layer\n",
    "# inputs = Input(shape=input_shape)\n",
    "\n",
    "# # Define autoencoder to encode input features\n",
    "# encoded_features = []\n",
    "# for i in range(n_features):\n",
    "#     feature_input = Input(shape=(1,))\n",
    "#     encoded = Dense(32, activation='relu')(feature_input)\n",
    "#     decoded = Dense(1, activation='linear')(encoded)\n",
    "#     autoencoder = Model(feature_input, decoded)\n",
    "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
    "#     autoencoder.fit(F[:,i], F[:,i], epochs=10, batch_size=32)\n",
    "#     encoded_feature_i = autoencoder.encoder(F[:,i])\n",
    "#     encoded_features.append(encoded_feature_i)\n",
    "\n",
    "# # Add a CNN layer for each encoded feature\n",
    "# cnn_features = []\n",
    "# for i in range(n_features):\n",
    "#     cnn_i = Conv1D(filters=32, kernel_size=3, padding='same')(encoded_features[i])\n",
    "#     cnn_features.append(cnn_i)\n",
    "\n",
    "# # Concatenate the CNN features\n",
    "# concatenated_features = keras.layers.concatenate(cnn_features, axis=2)\n",
    "\n",
    "# # LSTM layer to capture temporal dependencies\n",
    "# lstm_features = []\n",
    "# for i in range(n_features):\n",
    "#     lstm_i = LSTM(units=64, return_sequences=True)(concatenated_features[:, :, i])\n",
    "#     lstm_features.append(lstm_i)\n",
    "\n",
    "# # Concatenate the LSTM features\n",
    "# concatenated_lstm = keras.layers.concatenate(lstm_features, axis=2)\n",
    "\n",
    "# # Fully connected layer with ReLU activation function\n",
    "# fc_layer = Dense(units=128, activation='relu')(concatenated_lstm)\n",
    "\n",
    "# # Output layer with sigmoid activation function\n",
    "# output_layer = Dense(units=1, activation='sigmoid')(fc_layer)\n",
    "\n",
    "# # Define model\n",
    "# model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train model\n",
    "# model.fit(x_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# # Evaluate model\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "# from sklearn.model_selection import KFold\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # define number of folds for cross-validation\n",
    "# num_folds = 3\n",
    "\n",
    "# # create KFold cross-validation object\n",
    "# kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# # create arrays to store training and validation loss for each epoch\n",
    "# train_losses = np.zeros((num_folds, 10))\n",
    "# val_losses = np.zeros((num_folds, 10))\n",
    "\n",
    "# import time\n",
    "\n",
    "# # loop over the folds\n",
    "# fold_no = 1\n",
    "# for train, val in kfold.split(X_train_resampled_final, y_train_resampled_final):\n",
    "    \n",
    "#     # create model\n",
    "#     pre_trained_model = Sequential()\n",
    "#     # add convolutional layer\n",
    "#     pre_trained_model.add(Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=(10, 1)))\n",
    "#     # add pooling layer\n",
    "#     pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "#     # flatten output to feed into fully connected layer\n",
    "#     pre_trained_model.add(Flatten())\n",
    "#     # add fully connected layer\n",
    "#     pre_trained_model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     opt_new = Adam(lr=0.00033)\n",
    "#     # compile model\n",
    "#     pre_trained_model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "    \n",
    "#     # record start time\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # train model for each fold\n",
    "#     history = pre_trained_model.fit(X_train_resampled_final[train], y_train_resampled_final[train],\n",
    "#                                      epochs=10, batch_size=32, validation_data=(X_train_resampled_final[val], y_train_resampled_final[val]))\n",
    "    \n",
    "#     # record end time and calculate elapsed time\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "    \n",
    "#     print(f'Fold {fold_no} elapsed time: {elapsed_time:.2f} seconds')\n",
    "    \n",
    "#     # store training and validation loss for each epoch\n",
    "#     train_losses[fold_no-1] = history.history['loss']\n",
    "#     val_losses[fold_no-1] = history.history['val_loss']\n",
    "    \n",
    "#     # increment fold number\n",
    "#     fold_no += 1\n",
    "    \n",
    "# # calculate mean training and validation loss across all folds for each epoch\n",
    "# mean_train_loss = np.mean(train_losses, axis=0)\n",
    "# mean_val_loss = np.mean(val_losses, axis=0)\n",
    "\n",
    "# # plot training and validation loss curves for each epoch\n",
    "# plt.plot(mean_train_loss, label='Training Loss')\n",
    "# plt.plot(mean_val_loss, label='Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7bd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# # create model\n",
    "# pre_trained_model = Sequential()\n",
    "# # add convolutional layer\n",
    "# pre_trained_model.add(Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=(10, 1)))\n",
    "# # add pooling layer\n",
    "# pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "# # flatten output to feed into fully connected layer\n",
    "# pre_trained_model.add(Flatten())\n",
    "# # add fully connected layer\n",
    "# pre_trained_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# opt_new=Adam(lr=0.00033)\n",
    "# # compile model\n",
    "# pre_trained_model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "\n",
    "# # train model\n",
    "# pre_trained_model.fit(X_train_resampled_final, y_train_resampled_final, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f73a8",
   "metadata": {},
   "source": [
    "## Go back to smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c088a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f2bdbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "837a8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "036edbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39800044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d689efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fbaa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6b3ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29beb78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0        310    9762.170000   59057.000000             0.0            0.00   \n",
      "1        138   44423.330000   51357.539530             0.0        85240.12   \n",
      "2        325  144628.270000    4564.000000             0.0        85240.12   \n",
      "3        308  300712.340000   51474.000000             0.0        85240.12   \n",
      "4        349   47243.760000   11262.000000             0.0            0.00   \n",
      "...      ...            ...            ...             ...             ...   \n",
      "404612   278  111168.880136  111168.880136             0.0        85240.12   \n",
      "404613   274  144628.270000   51357.539530             0.0        85240.12   \n",
      "404614    60  144628.270000   51357.539530             0.0            0.00   \n",
      "404615   449   44882.356239   44882.356239             0.0            0.00   \n",
      "404616   220   39953.091459   29059.334627             0.0        85240.12   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0             0.000000               0     3    156157    319786  \n",
      "1        258760.470000               0     0     92881    180374  \n",
      "2        258760.470000               0     1     80756    482539  \n",
      "3        654217.020000               0     1    175711    597630  \n",
      "4             0.000000               0     3    156157     26253  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "404612   258760.470000               0     1     90379    472585  \n",
      "404613   258760.470000               0     1    112071    494845  \n",
      "404614        0.000000               0     1    154830    240268  \n",
      "404615    36237.626509               0     1    122579     88980  \n",
      "404616   258760.470000               0     1     93537    130866  \n",
      "\n",
      "[404617 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "485376  278.0   22928.58            0.0             0.0           0.000   \n",
      "642214   45.0    8606.90         5764.0             0.0           0.000   \n",
      "192982  237.0  220046.83            0.0             0.0      130797.505   \n",
      "99091   328.0   83938.53        13653.5             0.0      130797.505   \n",
      "203398  307.0   74636.86            0.0             0.0      130797.505   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "230877  154.0  195805.05        31725.0             0.0           0.000   \n",
      "315026  301.0   36352.03        13653.5             0.0           0.000   \n",
      "661254  238.5  163969.90        13653.5             0.0      130797.505   \n",
      "688112  280.0    3092.79            0.0             0.0           0.000   \n",
      "642560   35.0   74636.86        30807.0             0.0      130797.505   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "485376           0.000               0     3    291184    424837  \n",
      "642214           0.000               0     3    363649    442961  \n",
      "192982      214326.245               0     1      1853    410946  \n",
      "99091       537297.070               0     0    252825    347652  \n",
      "203398      214326.245               0     1    201182    417173  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "230877      195805.050               0     1    181881    192704  \n",
      "315026           0.000               0     3    458861    630843  \n",
      "661254      706564.020               0     0     37270    676511  \n",
      "688112           0.000               0     3    455345    152073  \n",
      "642560      214326.245               0     1    214954    689599  \n",
      "\n",
      "[70000 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "     X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "   # X_train_resampled_final[:, i] = replace_outliers_with_mad(X_train_resampled_final[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0811d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed77df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31f1e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000015B97289E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000015B97289E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000015B97289E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_2\" is incompatible with the layer: expected shape=(None, None, 16), found shape=(1, 10, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16776\\3694100818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mLSTM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Define the final classification layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    294\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                         raise ValueError(\n\u001b[1;32m--> 296\u001b[1;33m                             \u001b[1;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                             \u001b[1;34m\"incompatible with the layer: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                             \u001b[1;34mf\"expected shape={spec.shape}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm_2\" is incompatible with the layer: expected shape=(None, None, 16), found shape=(1, 10, 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "encoding_dim=5\n",
    "num_hidden=10\n",
    "num_features=10\n",
    "num_filters=16\n",
    "num_units=32\n",
    "N=10\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=encoding_dim, activation='relu', input_shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(units=num_hidden, activation='relu')\n",
    "])\n",
    "decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=encoding_dim, activation='relu', input_shape=(num_hidden,)),\n",
    "    tf.keras.layers.Dense(units=num_features, activation='sigmoid')\n",
    "])\n",
    "autoencoder = tf.keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# Freeze all layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "# Encode each feature and pass it through the pre-trained CNN and LSTM layers\n",
    "V = np.zeros((N, num_hidden))\n",
    "for i in range(N):\n",
    "    V[i] = encoder.predict(X_train_resampled_final[i:i+1])[0]\n",
    "V = V.reshape(N, num_hidden, 1)\n",
    "C = CNN(V)\n",
    "#C = tf.squeeze(C, axis=2)\n",
    "#C = tf.squeeze(C)\n",
    "C = np.expand_dims(C, axis=0)  # add a new dimension at the beginning\n",
    "\n",
    "LSTM = tf.keras.layers.LSTM(units=num_units)\n",
    "LSTM.build((None, num_hidden, num_filters))\n",
    "O = LSTM(C)\n",
    "\n",
    "# Define the final classification layer\n",
    "FL = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# Pass the vector representations through the final layer to obtain the fraud labels\n",
    "Y_pred = FL(O)\n",
    "\n",
    "# Define the loss function and optimize it using an optimizer such as Adam\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "model = tf.keras.models.Model(inputs=X_train_resampled_final, outputs=Y_pred)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train_resampled_final, y_train_resampled_final, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9f46db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000015BA1263678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000015BA1263678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000015BA1263678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_3\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (10, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16776\\3648806058.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_filters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# Define the final classification layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    231\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 raise ValueError(\n\u001b[1;32m--> 233\u001b[1;33m                     \u001b[1;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m                     \u001b[1;34m\"is incompatible with the layer: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m                     \u001b[1;34mf\"expected ndim={spec.ndim}, found ndim={ndim}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"lstm_3\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (10, 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "encoding_dim = 5\n",
    "num_hidden = 10\n",
    "num_features = 10\n",
    "num_filters = 16\n",
    "num_units = 32\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "encoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=encoding_dim, activation='relu', input_shape=(num_features,)),\n",
    "    tf.keras.layers.Dense(units=num_hidden, activation='relu')\n",
    "])\n",
    "\n",
    "decoder = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=encoding_dim, activation='relu', input_shape=(num_hidden,)),\n",
    "    tf.keras.layers.Dense(units=num_features, activation='sigmoid')\n",
    "])\n",
    "\n",
    "autoencoder = tf.keras.models.Sequential([encoder, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# Freeze all layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "# Encode each feature and pass it through the pre-trained CNN and LSTM layers\n",
    "V = np.zeros((num_features, num_hidden))\n",
    "for i in range(num_features):\n",
    "    V[i] = encoder.predict(X_train_resampled_final[i:i+1])[0]\n",
    "V = V.reshape(num_features, num_hidden, 1)\n",
    "C = CNN(V)\n",
    "\n",
    "# Call LSTM.build() method to set the input shape of the LSTM layer\n",
    "LSTM = tf.keras.layers.LSTM(units=num_units)\n",
    "LSTM.build((None, num_hidden, num_filters))\n",
    "\n",
    "O = LSTM(C)\n",
    "\n",
    "# Define the final classification layer\n",
    "FL = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# Pass the vector representations through the final layer to obtain the fraud labels\n",
    "Y_pred = FL(O)\n",
    "\n",
    "# Define the loss function and optimize it using an optimizer such as Adam\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "model = tf.keras.models.Model(inputs=X_train_resampled_final, outputs=Y_pred)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train_resampled_final, y_train_resampled_final, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd567b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       "array([[0.9267062 ],\n",
       "       [0.9999088 ],\n",
       "       [0.9997403 ],\n",
       "       [0.96466154],\n",
       "       [0.71089274],\n",
       "       [0.9995042 ],\n",
       "       [0.6468542 ],\n",
       "       [0.9997174 ],\n",
       "       [0.97051245],\n",
       "       [0.99972844]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "406886e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(autoencoder, to_file='autoencoder_last.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b6adf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(pre_trained_model, to_file='pre_trained_cnn.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f231a4",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcf11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# def create_model(filters=32, kernel_size=3, activation='tanh', dropout_rate=0.2, loss='binary_crossentropy', opt='adam'):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(64, activation=activation))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# param_grid = {\n",
    "#     'filters': [16, 32, 64],\n",
    "#     'kernel_size': [3, 5],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'dropout_rate': [0.2, 0.5],\n",
    "#     'loss': ['binary_crossentropy', 'mse'],\n",
    "#     'opt': ['adam', 'sgd']\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3)\n",
    "# grid_result = grid.fit(X_train_resampled_final, y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint\n",
    "\n",
    "# def create_model(filters=32, kernel_size=3, activation='tanh', dropout_rate=0.2, loss='binary_crossentropy', opt='adam'):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Flatten())\n",
    "#     model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# param_dist = {\n",
    "#     'filters': randint(16, 64),\n",
    "#     'kernel_size': [3, 5],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'dropout_rate': [0.2, 0.5],\n",
    "# }\n",
    "\n",
    "# n_iter_search = 10\n",
    "# random_search = RandomizedSearchCV(estimator=keras_model, param_distributions=param_dist, n_iter=n_iter_search, cv=3)\n",
    "# random_search_result = random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "# print(\"Best hyperparameters: \", random_search_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Conv1D, LSTM, Dense, Activation\n",
    "# from keras.models import Model\n",
    "\n",
    "# # Define input shape\n",
    "# n_features = 10  # number of input features\n",
    "# input_shape = (None, n_features)\n",
    "\n",
    "# # Define input layer\n",
    "# inputs = Input(shape=input_shape)\n",
    "\n",
    "# # Define autoencoder to encode input features\n",
    "# encoded_features = []\n",
    "# for i in range(n_features):\n",
    "#     feature_input = Input(shape=(1,))\n",
    "#     encoded = Dense(32, activation='relu')(feature_input)\n",
    "#     decoded = Dense(1, activation='linear')(encoded)\n",
    "#     autoencoder = Model(feature_input, decoded)\n",
    "#     autoencoder.compile(optimizer='adam', loss='mse')\n",
    "#     autoencoder.fit(X_train_resampled_final[:,i], X_train_resampled_final[:,i], epochs=1, batch_size=32)\n",
    "#     encoded_feature_i = autoencoder.predict(X_train_resampled_final[:,i].reshape(-1, 1))\n",
    "#     encoded_features.append(encoded_feature_i)\n",
    "\n",
    "# # Convolutional layer to extract relevant features\n",
    "# conv_layer = Conv1D(filters=32, kernel_size=3, padding='same')(encoded_features)\n",
    "\n",
    "# # LSTM layer to capture temporal dependencies\n",
    "# lstm_layer = LSTM(units=64, return_sequences=True)(conv_layer)\n",
    "\n",
    "# # Fully connected layer with ReLU activation function\n",
    "# fc_layer = Dense(units=128, activation='relu')(lstm_layer)\n",
    "\n",
    "# # Output layer with sigmoid activation function\n",
    "# output_layer = Dense(units=1, activation='sigmoid')(fc_layer)\n",
    "\n",
    "# # Define model\n",
    "# model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train model\n",
    "# model.fit(x_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "# # Evaluate model\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', score[0])\n",
    "# print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7f9ff",
   "metadata": {},
   "source": [
    "# New CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45c90e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dim = 10\n",
    "\n",
    "# Define a custom scaling function\n",
    "def scale(x):\n",
    "    mean = K.mean(x)\n",
    "    std = K.std(x)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# Create K-fold cross-validator\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store training and validation loss for each fold\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# Loop over each fold\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train_resampled_final, y_train_resampled_final)):\n",
    "    print(f'Fold {fold + 1}...')\n",
    "    \n",
    "    # Split data into training and validation sets for this fold\n",
    "    X_train_fold = X_train_resampled_final.iloc[train_index]\n",
    "    y_train_fold = y_train_resampled_final.iloc[train_index]\n",
    "    X_val_fold = X_train_resampled_final.iloc[val_index]\n",
    "    y_val_fold = y_train_resampled_final.iloc[val_index]\n",
    "    \n",
    "    # Create model\n",
    "    model_new = Sequential()\n",
    "    model_new.add(Lambda(scale, input_shape=(input_dim,)))\n",
    "    model_new.add(Dense(32, activation='tanh', kernel_regularizer=regularizers.l1(0.000811)))\n",
    "    model_new.add(Dense(1, activation='sigmoid'))\n",
    "    opt_new = Adam(lr=0.00033)\n",
    "    model_new.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "    \n",
    "    # Train model on this fold's training data\n",
    "    history = model_new.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, verbose=1, validation_data=(X_val_fold, y_val_fold))\n",
    "    \n",
    "    # Append this fold's training and validation loss to the lists\n",
    "    train_loss.append(history.history['loss'])\n",
    "    val_loss.append(history.history['val_loss'])\n",
    "    \n",
    "    # Plot this fold's training and validation loss\n",
    "    plt.plot(history.history['loss'], label=f'Fold {fold + 1} Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label=f'Fold {fold + 1} Validation Loss')\n",
    "    \n",
    "# Plot the average training and validation loss across all folds\n",
    "plt.plot(np.mean(train_loss, axis=0), label='Average Training Loss', color='black', linewidth=2)\n",
    "plt.plot(np.mean(val_loss, axis=0), label='Average Validation Loss', color='red', linewidth=2)\n",
    "\n",
    "# Add labels and legend to the plot\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Across Folds')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e25681a",
   "metadata": {},
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model.save('pre_trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efac9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "pre_trained_model = load_model('pretrained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f56a0a0",
   "metadata": {},
   "source": [
    "## Extract learnt weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_weights = pre_trained_model.layers[2].get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9984974",
   "metadata": {},
   "source": [
    "## Transfer learnt weights to the new CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9349452",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.layers[0].set_weights(cnn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765efa8",
   "metadata": {},
   "source": [
    "## Create LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "# Define LSTM model\n",
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2,input_shape=(X_train_resampled_final.shape[1], 1))\n",
    "        )\n",
    "# Add drop out layer\n",
    "LSTM_model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "LSTM_model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "LSTM_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.fit(X_train_resampled_final, y_train_resampled_final, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610752f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = LSTM_model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1dd046",
   "metadata": {},
   "source": [
    "# New steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a041765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-train CNN model\n",
    "pretrained_cnn = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "pretrained_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "pretrained_cnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Transfer learning to create new CNN\n",
    "new_cnn = Sequential(pretrained_cnn.layers[:-2])\n",
    "for layer in new_cnn.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "# The TimeDistributed layer is used to apply the same CNN layers to every time step of the input sequence. The output of this layer is \n",
    "# then passed to an LSTM layer, which is followed by a Dense layer with a softmax activation function to predict the class labels.\n",
    "# Add RNN layers to create CNN-RNN model\n",
    "cnn_rnn = Sequential([\n",
    "    TimeDistributed(new_cnn, input_shape=input_shape),\n",
    "    LSTM(64),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Train CNN-RNN model\n",
    "cnn_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_rnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Thaw CNN model\n",
    "for layer in new_cnn.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add attention layer\n",
    "attention = Sequential([\n",
    "    Dense(1, activation='tanh'),\n",
    "    Flatten(),\n",
    "    Activation('softmax')\n",
    "])\n",
    "cnn_rnn_attention = Sequential([\n",
    "    TimeDistributed(new_cnn, input_shape=input_shape),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    TimeDistributed(attention),\n",
    "    Dot(axes=1),\n",
    "    Flatten(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Train CNN-RNN-Attention model\n",
    "cnn_rnn_attention.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn_rnn_attention.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600620a",
   "metadata": {},
   "source": [
    "## Soidisant feature fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab87643",
   "metadata": {},
   "source": [
    "https://www.bing.com/videos/search?&q=feature+fusion+deep+learning&docid=603484746686478091&mid=ED175C063F17E1FDCFCEED175C063F17E1FDCFCE&view=detail&FORM=VDRVRV&rvsmid=083DA2472C6F7DD33EE1083DA2472C6F7DD33EE1&ajaxhist=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67aeb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "\n",
    "# Pre-train CNN model\n",
    "pretrained_cnn = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "pretrained_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "pretrained_cnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Transfer learning to create new CNN\n",
    "new_cnn = Sequential(pretrained_cnn.layers[:-2])\n",
    "for layer in new_cnn.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add RNN layers to create CNN-RNN model\n",
    "cnn_rnn = Sequential([\n",
    "    TimeDistributed(new_cnn, input_shape=input_shape),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Flatten()\n",
    "])\n",
    "\n",
    "# Add merge layer\n",
    "merged = concatenate([cnn_rnn.output, new_cnn.output])\n",
    "\n",
    "# Add dense layer for classification\n",
    "output_layer = Dense(num_classes, activation='softmax')(merged)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=[cnn_rnn.input, new_cnn.input], outputs=output_layer)\n",
    "\n",
    "# Train model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, validation_data=([x_test, x_test], y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(X, y):\n",
    "    # Initialize theta with zeros\n",
    "    n_features = X.shape[1]\n",
    "    theta = np.zeros(n_features)\n",
    "    \n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    J = -1/m * np.sum(y*np.log(h) + (1-y)*np.log(1-h))\n",
    "    return J\n",
    "\n",
    "# Define the MBOA function\n",
    "def MBOA(dim, maxiter, Bf, c, a, p, x,y):\n",
    "    # Initialization\n",
    "\n",
    "    n = len(df.columns)\n",
    "    X = np.random.rand(n, dim)\n",
    "    I = np.zeros(n)\n",
    "    F = np.zeros(n)\n",
    "\n",
    "    # Step 1: Set the initial generation/iteration number G=0\n",
    "    G = 0\n",
    "\n",
    "    # Step 2: Generate initial population of n butterflies 𝑋𝑖 = (i=1,2,..,n)\n",
    "\n",
    "    # define the size of the population\n",
    "    n = 10000\n",
    "\n",
    "    # extract the variables of interest\n",
    "    variables = X_train_resampled_final.iloc[:, :dim]\n",
    "\n",
    "    # define the ranges for each variable\n",
    "    ranges = [(0, 10000)]*dim\n",
    "\n",
    "    # generate the initial population\n",
    "    population = []\n",
    "    for i in range(n):\n",
    "        butterfly = []\n",
    "        for j in range(variables.shape[0]):\n",
    "            var_range = ranges[j]\n",
    "            var_value = np.random.uniform(var_range[0], var_range[1])\n",
    "            butterfly.append(var_value)\n",
    "        population.append(butterfly)\n",
    "\n",
    "\n",
    "    # Main loop\n",
    "    while G < maxiter:\n",
    "        # Calculate stimulus intensity for each butterfly\n",
    "        for i in range(Bf):\n",
    "            I[i] = objective_function(X[:, i], y)\n",
    "            F[i] = c * I[i]**a\n",
    "\n",
    "        # Find the best butterfly\n",
    "        best_butterfly = X[:, np.argmin(I)]\n",
    "\n",
    "        # Update each butterfly in the population\n",
    "        for i in range(n):\n",
    "            r = np.random.rand()\n",
    "            if r < p:\n",
    "                # Move towards the best butterfly\n",
    "                X[:, i] += np.random.rand(dim) * (best_butterfly - X[:, i])\n",
    "                # Determine mutual relationship vector and update butterfly positions\n",
    "                j = np.random.randint(n)\n",
    "                if i != j:\n",
    "                    MutualVector = (X[:, i] - X[:, j]) / np.linalg.norm(X[:, i] - X[:, j])\n",
    "                    X[:, i] += np.random.rand(dim) * MutualVector\n",
    "                    X[:, j] += np.random.rand(dim) * MutualVector\n",
    "                   # Calculate fitness value of the new butterfly\n",
    "                    I[i] = fitness(X[:, i], fun)\n",
    "                    # Update the best butterfly if a new one is found\n",
    "                    if I[i] < best_fitness:\n",
    "                        best_butterfly = X[:, i]\n",
    "                        best_fitness = I[i]\n",
    "\n",
    "                    # Update the mutation probability\n",
    "                    p = p * alpha\n",
    "\n",
    "                    # Store the best fitness value of this iteration\n",
    "                    best_fitness_values.append(best_fitness)\n",
    "\n",
    "\n",
    "    # Return the best butterfly and its fitness value\n",
    "    return best_butterfly, best_fitness, best_fitness_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b439111",
   "metadata": {},
   "source": [
    "## Freeze layer in NEW CNN (DO NOT FREEZE IN PRE-TRAINED CNN): To be done b4 training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7394195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers in the pre-trained model\n",
    "for layer in model_new.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a72a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(X, y):\n",
    "    # Initialize theta with zeros\n",
    "    n_features = X.shape[1]\n",
    "    theta = np.zeros(n_features)\n",
    "    \n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    J = -1/m * np.sum(y*np.log(h) + (1-y)*np.log(1-h))\n",
    "    return J\n",
    "\n",
    "# Define the MBOA function\n",
    "def MBOA(dim, maxiter, Bf, c, a, p, x, y):\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "    I = np.zeros(Bf)\n",
    "    F = np.zeros(Bf)\n",
    "    best_fitness_values = []\n",
    "    \n",
    "    # Step 1: Set the initial generation/iteration number G=0\n",
    "    G = 0\n",
    "    \n",
    "    # Step 2: Generate initial population of n butterflies 𝑋𝑖 = (i=1,2,..,n)\n",
    "    \n",
    "    # define the size of the population\n",
    "    n = 10000\n",
    "\n",
    "    # extract the variables of interest\n",
    "    variables = X_train_resampled_final.iloc[:, :dim]\n",
    "\n",
    "    # define the ranges for each variable\n",
    "    ranges = [(0, 10000)]*dim\n",
    "\n",
    "    # generate the initial population\n",
    "    population = []\n",
    "    for i in range(n):\n",
    "        butterfly = []\n",
    "        for j in range(variables.shape[0]):\n",
    "            var_range = ranges[j]\n",
    "            var_value = np.random.uniform(var_range[0], var_range[1])\n",
    "            butterfly.append(var_value)\n",
    "        population.append(butterfly)\n",
    "\n",
    "\n",
    "    # Main loop\n",
    "    while G < maxiter:\n",
    "        # Calculate stimulus intensity for each butterfly\n",
    "        for i in range(Bf):\n",
    "            I[i] = objective_function(X[:, i], y)\n",
    "            F[i] = c * I[i]**a\n",
    "        \n",
    "        # Find the best butterfly\n",
    "        best_butterfly = X[:, np.argmin(I)]\n",
    "\n",
    "        # Update each butterfly in the population\n",
    "        for i in range(n):\n",
    "            r = np.random.rand()\n",
    "            if r < p:\n",
    "                # Move towards the best butterfly\n",
    "                X[:, i] += np.random.rand(dim) * (best_butterfly - X[:, i])\n",
    "                # Determine mutual relationship vector and update butterfly positions\n",
    "                j = np.random.randint(n)\n",
    "                if i != j:\n",
    "                    MutualVector = (X[:, i] - X[:, j]) / np.linalg.norm(X[:, i] - X[:, j])\n",
    "                    X[:, i] += np.random.rand(dim) * MutualVector\n",
    "                    X[:, j] += np.random.rand(dim) * MutualVector\n",
    "                   # Calculate fitness value of the new butterfly\n",
    "                    I[i] = fitness(X[:, i], fun)\n",
    "                    # Update the best butterfly if a new one is found\n",
    "                    if I[i] < best_fitness:\n",
    "                        best_butterfly = X[:, i]\n",
    "                        best_fitness = I[i]\n",
    "\n",
    "                    # Update the mutation probability\n",
    "                    p = p * alpha\n",
    "\n",
    "                    # Store the best fitness value of this iteration\n",
    "                    best_fitness_values.append(best_fitness)\n",
    "\n",
    "        # Increment the generation number\n",
    "        G += 1\n",
    "\n",
    "\n",
    "    # Return the best butterfly and its fitness value\n",
    "    return best_butterfly, best_fitness, best_fitness_values\n",
    "\n",
    "# Call the function\n",
    "best_butterfly, best_fitness = MBOA(objective_function, 5, 50, 10, 1, 2, 0.8, X_train_resampled_final)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# Objective function\n",
    "def f(X):\n",
    "    return sum([x**2 for x in X])\n",
    "\n",
    "# Butterfly Optimization Algorithm\n",
    "def butterfly_optimization(f, dim, Maxiter, Bf, c, a, p):\n",
    "    \n",
    "    # Initialize\n",
    "    \n",
    "    # Set the initial generation/iteration number G=0\n",
    "    \n",
    "    G = 0\n",
    "    n = Bf\n",
    "    \n",
    "    # Generate initial population of n butterflies 𝑋𝑖 = (i=1,2,..,n)\n",
    "    \n",
    "    X = [[random.uniform(-5,5) for _ in range(dim)] for _ in range(n)]\n",
    "    I = [f(x) for x in X]\n",
    "    best_index = I.index(min(I))\n",
    "    best = X[best_index]\n",
    "    while G < Maxiter:\n",
    "        # Calculate fragrance\n",
    "        F = [c * I[i]**a for i in range(n)]\n",
    "        # Find the best butterfly\n",
    "        best_index = F.index(max(F))\n",
    "        best = X[best_index]\n",
    "        # Update each butterfly\n",
    "        for i in range(n):\n",
    "            # Move towards best butterfly\n",
    "            if random.random() < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += random.uniform(-1,1) * abs(best[j] - X[i][j])\n",
    "            # Mutual relationship\n",
    "            else:\n",
    "                j = random.randint(0,n-1)\n",
    "                while j == i:\n",
    "                    j = random.randint(0,n-1)\n",
    "                MutVect = [random.uniform(-1,1) for _ in range(dim)]\n",
    "                # Collaboration strategy\n",
    "                X[i] = [X[i][k] + random.uniform(0,1) * MutVect[k] * abs(X[j][k] - X[i][k]) for k in range(dim)]\n",
    "            # Update fitness value\n",
    "            I[i] = f(X[i])\n",
    "        # Update the best value\n",
    "        if min(I) < f(best):\n",
    "            best_index = I.index(min(I))\n",
    "            best = X[best_index]\n",
    "        G += 1\n",
    "    return best, f(best)\n",
    "\n",
    "# Call the function\n",
    "best_butterfly, best_fitness = butterfly_optimization(f, dim=5, Maxiter=50, Bf=10, c=1, a=2, p=0.8)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Butterfly Optimization Algorithm\n",
    "# def butterfly_optimization(f, dim, Maxiter, Bf, c, a, p, X_train):\n",
    "#     # Initialize\n",
    "#     G = 0\n",
    "#     n = Bf\n",
    "#     if len(X_train) < n:\n",
    "#         n = len(X_train)\n",
    "    \n",
    "#     X = [X_train[i] for i in range(n)]\n",
    "    \n",
    "#     I = [f(x) for x in X]\n",
    "#     best_index = I.index(min(I))\n",
    "#     best = X[best_index]\n",
    "#     while G < Maxiter:\n",
    "#         # Calculate fragrance\n",
    "#         F = [c * I[i]**a for i in range(n)]\n",
    "#         # Find the best butterfly\n",
    "#         best_index = F.index(max(F))\n",
    "#         best = X[best_index]\n",
    "#         # Update each butterfly\n",
    "#         for i in range(n):\n",
    "#             # Move towards best butterfly\n",
    "#             if random.random() < p:\n",
    "#                 for j in range(dim):\n",
    "#                     X[i][j] += random.uniform(-1,1) * abs(best[j] - X[i][j])\n",
    "#             # Mutual relationship\n",
    "#             else:\n",
    "#                 j = random.randint(0,n-1)\n",
    "#                 while j == i:\n",
    "#                     j = random.randint(0,n-1)\n",
    "#                 MutVect = [random.uniform(-1,1) for _ in range(dim)]\n",
    "#                 # Collaboration strategy\n",
    "#                 X[i] = [X[i][k] + random.uniform(0,1) * MutVect[k] * abs(X[j][k] - X[i][k]) for k in range(dim)]\n",
    "#             # Update fitness value\n",
    "#             I[i] = f(X[i])\n",
    "#         # Update the best value\n",
    "#         if min(I) < f(best):\n",
    "#             best_index = I.index(min(I))\n",
    "#             best = X[best_index]\n",
    "#         G += 1\n",
    "#     return best, f(best)\n",
    "\n",
    "# # X_train_resampled_final = X_train_resampled_final.values.tolist()\n",
    "\n",
    "# # Call the function\n",
    "# best_butterfly, best_fitness = butterfly_optimization(f, dim=5, Maxiter=50, Bf=10, c=1, a=2, p=0.8, X_train=X_train_resampled_final)\n",
    "\n",
    "# # Print the best butterfly and its fitness value\n",
    "# print(\"Best butterfly:\", best_butterfly)\n",
    "# print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa284d65",
   "metadata": {},
   "source": [
    "# Work refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d612d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n",
    "def f(X):\n",
    "    return sum([x**2 for x in X])\n",
    "\n",
    "def butterfly_optimization(f, X, Maxiter, Bf, c, a, p):\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "    n = len(X) # Obtain length of X_train_resampled_final\n",
    "    print(\"n\",n)\n",
    "    \n",
    "    dim = len(X[0])\n",
    "    print(\"dim\",dim)\n",
    "    \n",
    "    I = [f(x) for x in X]\n",
    "    best_index = I.index(min(I))\n",
    "    best = X[best_index]\n",
    "    G = 0\n",
    "    while G < Maxiter:\n",
    "        # Calculate fragrance\n",
    "        F = [c * I[i]**a for i in range(n)]\n",
    "        # Find the best butterfly\n",
    "        best_index = F.index(max(F))\n",
    "        best = X[best_index]\n",
    "        # Update each butterfly\n",
    "        for i in range(n):\n",
    "            # Move towards best butterfly\n",
    "            if random.random() < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += random.uniform(-1,1) * abs(best[j] - X[i][j])\n",
    "            # Mutual relationship\n",
    "            else:\n",
    "                j = random.randint(0,n-1)\n",
    "                while j == i:\n",
    "                    j = random.randint(0,n-1)\n",
    "                MutVect = [random.uniform(-1,1) for _ in range(dim)]\n",
    "                # Collaboration strategy\n",
    "                X[i] = [X[i][k] + random.uniform(0,1) * MutVect[k] * abs(X[j][k] - X[i][k]) for k in range(dim)]\n",
    "            # Update fitness value\n",
    "            I[i] = f(X[i])\n",
    "        # Update the best value\n",
    "        if min(I) < f(best):\n",
    "            best_index = I.index(min(I))\n",
    "            best = X[best_index]\n",
    "        G += 1\n",
    "    return best, f(best)\n",
    "\n",
    "# Convert Pandas DataFrame to list of lists\n",
    "X_train_resampled_final_list = X_train_resampled_final.values.tolist()\n",
    "\n",
    "# Call the function with the modified input data\n",
    "best_butterfly, best_fitness = butterfly_optimization(f, X_train_resampled_final_list, Maxiter=3, Bf=10, c=1, a=2, p=0.8)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c16e2d",
   "metadata": {},
   "source": [
    "# Current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5810f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# @ Input 1: Objective function\n",
    "\n",
    "def f(X):\n",
    "    return sum([x**2 for x in X])\n",
    "\n",
    "# @ Input 2: 𝑋𝑖 = (𝑋1, 𝑋2,𝑋3, … … , 𝑋𝑑𝑖𝑚),\n",
    "\n",
    "# X_train_resampled_final_list = X_train_resampled_final.tolist()\n",
    "X_train_resampled_final_list = X_train_resampled_final.values.tolist()\n",
    "\n",
    "def butterfly_optimization(f, X, Maxiter, Bf, c, a, p):\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "    n = len(X) # Obtain length of X_train_resampled_final\n",
    "    \n",
    "    # @ Input 3: dim= no. of dimensions \n",
    "\n",
    "    dim = len(X[0])\n",
    "    \n",
    "    I = [f(x) for x in X]\n",
    "    \n",
    "    best_index = I.index(min(I))\n",
    "    \n",
    "    best = X[best_index]\n",
    "    \n",
    "    G = 0\n",
    "    \n",
    "    while G < Maxiter:\n",
    "        \n",
    "        # Calculate fragrance\n",
    "        F = [c * I[i]**a for i in range(n)] # Equation(8)\n",
    "        \n",
    "        print(type(F))\n",
    "\n",
    "        # Find the best butterfly\n",
    "        best_index = F.index(max(F))\n",
    "        best = X[best_index]\n",
    "        \n",
    "        # Update each butterfly\n",
    "        \n",
    "        for i in range(n):\n",
    "            \n",
    "            # Move towards best butterfly\n",
    "            if random.random() < p:\n",
    "                for j in range(dim):\n",
    "                    #X[i][j] += random.uniform(-1,1) * abs(best[j] - X[i][j]) # Equation (9)\n",
    "                    X[i][j] += random.uniform(-1,1) * abs(best[j] - X[i][j]) # Equation (9)\n",
    "                    #X[i][j] += pow(random.uniform(-1, 1),2) * abs(best[j] - X[i][j]) * F[i]\n",
    "\n",
    "            # Mutual relationship\n",
    "            else:\n",
    "                j = random.randint(0,n-1)\n",
    "                while j == i:\n",
    "                    j = random.randint(0,n-1)\n",
    "                MutVect = [random.uniform(-1,1) for _ in range(dim)]\n",
    "                \n",
    "                # Collaboration strategy\n",
    "                X[i] = [X[i][k] + random.uniform(0,1) * MutVect[k] * abs(X[j][k] - X[i][k]) for k in range(dim)]\n",
    "            # Update fitness value\n",
    "            I[i] = f(X[i])\n",
    "        # Update the best value\n",
    "        if min(I) < f(best):\n",
    "            best_index = I.index(min(I))\n",
    "            best = X[best_index]\n",
    "        G += 1\n",
    "    return best, f(best)\n",
    "\n",
    "    \n",
    "# Call the function with the modified input data\n",
    "# Define sensor modality c, power exponent a and switch probability p \n",
    "best_butterfly, best_fitness = butterfly_optimization(f, X_train_resampled_final_list, Maxiter=3, Bf=10, c=1, a=2, p=0.8)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8dd9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def f(X):\n",
    "    # Calculate the mutual information between each feature and the target variable\n",
    "    scores = mutual_info_classif(X, y_train_resampled)\n",
    "    # Return the sum of the scores as the fitness value\n",
    "    return sum(scores)\n",
    "\n",
    "def butterfly_optimization(X, y, Maxiter, Bf, c, a, p):\n",
    "    n = len(X)\n",
    "    dim = X.shape[1]\n",
    "    decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)] # Convert all values to Decimal\n",
    "    #F = [decimal.Decimal(random.uniform(0, 1)) for i in range(n)]\n",
    "    F = [c * I[i]**a for i in range(n)]\n",
    "    fitness = [mutual_info_classif(X[i], y)[0] for i in range(n)]\n",
    "    best = X[fitness.index(max(fitness))]\n",
    "    best_fitness = max(fitness)\n",
    "    for t in range(Maxiter):\n",
    "        for i in range(n):\n",
    "            eps = decimal.Decimal(random.gauss(0, 1))\n",
    "            for j in range(dim):\n",
    "                X[i][j] += eps * F[i] * abs(best[j] - X[i][j])\n",
    "            score = mutual_info_classif(X[i], y)[0]\n",
    "            if score > fitness[i]:\n",
    "                fitness[i] = score\n",
    "                if score > best_fitness:\n",
    "                    best_fitness = score\n",
    "                    best = X[i]\n",
    "            r = random.uniform(0, 1)\n",
    "            if r < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "            else:\n",
    "                j = random.randint(0, n-1)\n",
    "                for k in range(dim):\n",
    "                    X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "            for j in range(dim):\n",
    "                X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "            F[i] = decimal.Decimal(c) / decimal.Decimal(pow((1 + a * t), 1.5))\n",
    "    best = [float(best[i]) for i in range(dim)] # Convert best values back to float\n",
    "    return best, best_fitness\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863c6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def f(X):\n",
    "    # Calculate the mutual information between each feature and the target variable\n",
    "    scores = mutual_info_classif(X, y_train_resampled)\n",
    "    # Return the sum of the scores as the fitness value\n",
    "    return sum(scores)\n",
    "\n",
    "def butterfly_optimization(X, y, Maxiter, Bf, c, a, p):\n",
    "    n = len(X)\n",
    "    dim = X.shape[1]\n",
    "    decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)] # Convert all values to Decimal\n",
    "    #F = [decimal.Decimal(random.uniform(0, 1)) for i in range(n)]\n",
    "    F = [c * I[i]**a for i in range(n)]\n",
    "    fitness = [mutual_info_classif(X[i], y)[0] for i in range(n)]\n",
    "    best = X[fitness.index(max(fitness))]\n",
    "    best_fitness = max(fitness)\n",
    "    for t in range(Maxiter):\n",
    "        for i in range(n):\n",
    "            eps = decimal.Decimal(random.gauss(0, 1))\n",
    "            for j in range(dim):\n",
    "                X[i][j] += eps * F[i] * abs(best[j] - X[i][j])\n",
    "            score = mutual_info_classif(X[i], y)[0]\n",
    "            if score > fitness[i]:\n",
    "                fitness[i] = score\n",
    "                if score > best_fitness:\n",
    "                    best_fitness = score\n",
    "                    best = X[i]\n",
    "            r = random.uniform(0, 1)\n",
    "            if r < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "            else:\n",
    "                j = random.randint(0, n-1)\n",
    "                for k in range(dim):\n",
    "                    X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "            for j in range(dim):\n",
    "                X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "            F[i] = decimal.Decimal(c) / decimal.Decimal(pow((1 + a * t), 1.5))\n",
    "    best = [float(best[i]) for i in range(dim)] # Convert best values back to float\n",
    "    return best, best_fitness\n",
    "\n",
    "# Call the function with the modified input data\n",
    "# Define sensor modality c, power exponent a and switch probability p \n",
    "best_butterfly, best_fitness = butterfly_optimization(f, X_train_resampled_final_list, Maxiter=3, Bf=10, c=1, a=2, p=0.8)\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4394805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "\n",
    "# Objective function: Sphere function\n",
    "def f(X):\n",
    "    return sum([x**2 for x in X])\n",
    "\n",
    "# # Rosenbrock\n",
    "# def f(X):\n",
    "#     # return sum([100 * (X[i+1] - X[i]**2)**2 + (1 - X[i])**2 for i in range(len(X)-1)])\n",
    "    \n",
    "#     f(x,y)=(1-x)^2+100(y-x^2)^2 \n",
    "\n",
    "def butterfly_optimization(X, y, Maxiter, Bf, c, a, p):\n",
    "    \n",
    "    dim =10\n",
    "    n=10\n",
    "    \n",
    "    decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    \n",
    "    # calculate initial stimulus intensity for each butterfly\n",
    "    I = np.zeros(Bf)\n",
    "    for i in range(Bf):\n",
    "        I[i] = f(X[i])\n",
    "        \n",
    "    #I = [f(x) for x in X]\n",
    "    \n",
    "    X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)] # Convert all values to Decimal\n",
    "    \n",
    "    #F = [decimal.Decimal(random.uniform(0, 1)) for i in range(n)]\n",
    "    F = [decimal.Decimal(c) * decimal.Decimal(I[i])** decimal.Decimal(a) for i in range(n)]\n",
    "\n",
    "    # Find the best butterfly\n",
    "    best_index = F.index(max(F))\n",
    "    best = X[best_index]\n",
    "    \n",
    "    # Set the initial generation/iteration number G=0\n",
    "    G=0\n",
    "    #for G in range(Maxiter):\n",
    "    while G < Maxiter:\n",
    "        for i in range(Bf):\n",
    "                    \n",
    "            # Generate a random number r from [0,1]\n",
    "            r = random.uniform(0, 1)\n",
    "            if r < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "            else:\n",
    "                j = random.randint(0, n-1)\n",
    "                for k in range(dim):\n",
    "                    X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "            for j in range(dim):\n",
    "                X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "            F[i] = decimal.Decimal(c) / decimal.Decimal(pow((1 + a * G), 1.5))\n",
    "            G=G+1\n",
    "    best = [float(best[i]) for i in range(dim)] # Convert best values back to float\n",
    "    return best, best_fitness\n",
    "\n",
    "# Call the function with the modified input data\n",
    "# Define sensor modality c, power exponent a and switch probability p \n",
    "best_butterfly, best_fitness = butterfly_optimization(X_train_resampled_final_list, y_train_resampled, Maxiter=1000, Bf=10, c=3, a=4, p=0.85)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)\n",
    "\n",
    "# Print the values of the best butterfly to check if they are close to zero\n",
    "print(\"Values of best butterfly:\", [round(x, 5) for x in best_butterfly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "\n",
    "# Objective function: KNN classification with feature selection\n",
    "# def fitness(X, y, k=5, alpha=0.5):\n",
    "#     # X: feature matrix (num_samples x num_features)\n",
    "#     # y: target vector (num_samples,)\n",
    "#     # k: number of nearest neighbors to consider in KNN\n",
    "#     # alpha: importance of classification quality vs subset length (0 <= alpha <= 1)\n",
    "    \n",
    "#     from sklearn.neighbors import KNeighborsClassifier\n",
    "#     from sklearn.metrics import accuracy_score\n",
    "    \n",
    "#     clf = KNeighborsClassifier(n_neighbors=k)\n",
    "#     clf.fit(X, y)\n",
    "    \n",
    "#     # Calculate classification error rate\n",
    "#     y_pred = clf.predict(X)\n",
    "#     error_rate = 1 - accuracy_score(y, y_pred)\n",
    "    \n",
    "#     # Calculate fitness\n",
    "#     num_selected = X.shape[1]\n",
    "#     num_total = X.shape[1]\n",
    "#     fitness_value = alpha * (1 - error_rate) + (1 - alpha) * (num_selected / num_total)\n",
    "    \n",
    "#     return fitness_value\n",
    "\n",
    "def fitness(X, y, k=5, alpha=0.5):\n",
    "    # X: feature matrix (num_samples x num_features)\n",
    "    # y: target vector (num_samples,)\n",
    "    # k: number of nearest neighbors to consider in KNN\n",
    "    # alpha: importance of classification quality vs subset length (0 <= alpha <= 1)\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Calculate classification error rate\n",
    "    y_pred = clf.predict(X)\n",
    "    error_rate = 1 - accuracy_score(y, y_pred)\n",
    "\n",
    "    # Calculate fitness\n",
    "    num_selected = X.shape[1]\n",
    "    num_total = X.shape[1]\n",
    "    fitness_value = alpha * (1 - error_rate) + (1 - alpha) * (num_selected / num_total)\n",
    "\n",
    "    return fitness_value\n",
    "\n",
    "def butterfly_optimization(X, y, Maxiter, Bf, c, a, p):\n",
    "    \n",
    "    dim = X.shape[1]\n",
    "    n = Bf\n",
    "    \n",
    "    decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    \n",
    "    # Initialize butterfly positions and calculate initial stimulus intensity for each butterfly\n",
    "    X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)]\n",
    "    I = [fitness(X[i], y) for i in range(n)]\n",
    "    \n",
    "    # Initialize stimulus strength and find the best butterfly\n",
    "    F = [decimal.Decimal(c) * decimal.Decimal(I[i])**decimal.Decimal(a) for i in range(n)]\n",
    "    best_index = I.index(max(I))\n",
    "    best = X[best_index]\n",
    "    best_fitness = I[best_index]\n",
    "    \n",
    "    # Set the initial generation/iteration number G=0\n",
    "    G=0\n",
    "    while G < Maxiter:\n",
    "        for i in range(Bf):\n",
    "                    \n",
    "            # Generate a random number r from [0,1]\n",
    "            r = random.uniform(0, 1)\n",
    "            if r < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "            else:\n",
    "                j = random.randint(0, n-1)\n",
    "                for k in range(dim):\n",
    "                    X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "            for j in range(dim):\n",
    "                X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "            I[i] = fitness(X[i], y)\n",
    "            if I[i] > best_fitness:\n",
    "                best = X[i]\n",
    "                best_fitness = I[i]\n",
    "            F[i] = decimal.Decimal(c) / decimal.Decimal(pow((1 + a * G), 1.5))\n",
    "            G=G+1\n",
    "    best = [float(best[i]) for i in range(dim)] # Convert best values back to float\n",
    "    return best, best_fitness\n",
    "\n",
    "best_butterfly, best_fitness = butterfly_optimization(X_train_resampled_final, y_train_resampled_final, Maxiter=30, Bf=10, c=3, a=1, p=0.5)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)\n",
    "\n",
    "# Print the values of the best butterfly to check if they are close to zero\n",
    "print(\"Values of best butterfly:\", [round(x, 5) for x in best_butterfly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def butterfly_optimization(X, y, Maxiter, Bf, c, a, p):\n",
    "    \n",
    "    dim = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    \n",
    "    # Load the KNN classifier\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "        \n",
    "    # calculate initial stimulus intensity for each butterfly\n",
    "#     I = np.zeros(Bf)\n",
    "#     for i in range(Bf):\n",
    "#         I[i] = f(X[i])\n",
    "     # In the butterfly optimization function, modify the calculation of initial stimulus intensity for each butterfly to include the fitness function\n",
    "    I = np.zeros(Bf)\n",
    "    for i in range(Bf):\n",
    "         I[i] = fitness(X[i], y, k)\n",
    "    X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)] # Convert all values to Decimal\n",
    "    \n",
    "    #F = [decimal.Decimal(random.uniform(0, 1)) for i in range(n)]\n",
    "    F = [decimal.Decimal(c) * decimal.Decimal(I[i])** decimal.Decimal(a) for i in range(n)]\n",
    "\n",
    "    # Find the best butterfly\n",
    "    best_index = F.index(max(F))\n",
    "    best = X[best_index]\n",
    "    \n",
    "    # Set the initial generation/iteration number G=0\n",
    "    G=0\n",
    "    #for G in range(Maxiter):\n",
    "    while G < Maxiter:\n",
    "        for i in range(Bf):\n",
    "                    \n",
    "            # Generate a random number r from [0,1]\n",
    "            r = random.uniform(0, 1)\n",
    "            if r < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "            else:\n",
    "                j = random.randint(0, n-1)\n",
    "                for k in range(dim):\n",
    "                    X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "            for j in range(dim):\n",
    "                X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "                \n",
    "            # Convert X to numpy array and select the features\n",
    "            X_selected = np.array(X)[i,:]\n",
    "            X_selected = X[:, X_selected > 0]\n",
    "            \n",
    "            # Fit the KNN classifier on the training set\n",
    "            clf.fit(X_train[:, X_selected], y_train)\n",
    "            \n",
    "            # Predict the labels of the validation set\n",
    "            y_pred = clf.predict(X_val[:, X_selected])\n",
    "            \n",
    "            # Calculate the classification error rate\n",
    "            error_rate = sum(y_pred != y_val) / len(y_val)\n",
    "            \n",
    "            # Calculate the number of selected features\n",
    "            num_selected = sum(selected_features)\n",
    "\n",
    "            # Calculate the fitness value\n",
    "            fitness = alpha * (1 - error_rate) + beta * num_selected / num_features\n",
    "\n",
    "            # Append the fitness value to the list of fitness values\n",
    "            fitness_values.append(fitness)\n",
    "\n",
    "            # Return the fitness value\n",
    "            return fitness\n",
    "\n",
    "           \n",
    "\n",
    "            # Modify the while loop to update the best butterfly and its fitness value at each generation\n",
    "            while G < Maxiter:\n",
    "                for i in range(Bf):\n",
    "                        # Generate a random number r from [0,1]\n",
    "                        r = random.uniform(0, 1)\n",
    "                        if r < p:\n",
    "                            for j in range(dim):\n",
    "                                X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "                        else:\n",
    "                            j = random.randint(0, n-1)\n",
    "                            for k in range(dim):\n",
    "                                X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "                        for j in range(dim):\n",
    "                            X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                            X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "                        F[i] = decimal.Decimal(c) / decimal.Decimal(pow((1 + a * G), 1.5))\n",
    "\n",
    "                        # Calculate the fitness value for the updated butterfly\n",
    "                        fitness_val = fitness(X[i], y, k)\n",
    "\n",
    "                        # Update the best butterfly and its fitness value if necessary\n",
    "                        if fitness_val > best_fitness:\n",
    "                            best_fitness = fitness_val\n",
    "                            best = X[i]\n",
    "\n",
    "                        G=G+1\n",
    "    best = [float(best[i]) for i in range(dim)] # Convert best values back to float\n",
    "    return best, best_fitness\n",
    "\n",
    "best_butterfly, best_fitness = butterfly_optimization(X_train_resampled_final, y_train_resampled_final, Maxiter=30, Bf=10, c=3, a=1, p=0.5)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)\n",
    "\n",
    "# Print the values of the best butterfly to check if they are close to zero\n",
    "print(\"Values of best butterfly:\", [round(x, 5) for x in best_butterfly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fitness(X, y, k):\n",
    "    # Split the data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "    # Load the KNN classifier\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # Fit the KNN classifier on the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the validation set\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # Calculate the classification error rate\n",
    "    error_rate = sum(y_pred != y_val) / len(y_val)\n",
    "\n",
    "    # Calculate the number of selected features\n",
    "    num_selected = sum(X > 0)\n",
    "\n",
    "    # Calculate the fitness value\n",
    "    fitness = (1 - error_rate) * (1 + num_selected)\n",
    "\n",
    "    return fitness\n",
    "\n",
    "def butterfly_optimization(X, y, Maxiter, Bf, c, a, p, k):\n",
    "    \n",
    "    dim = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    \n",
    "    # calculate initial stimulus intensity for each butterfly\n",
    "    I = np.zeros(Bf)\n",
    "    for i in range(Bf):\n",
    "         #I[i] = fitness(X[i], y, k)\n",
    "        I[i] = fitness(X[i,:], y, k)\n",
    "\n",
    "    X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)] # Convert all values to Decimal\n",
    "    \n",
    "    F = [decimal.Decimal(c) * decimal.Decimal(I[i])** decimal.Decimal(a) for i in range(n)]\n",
    "\n",
    "    # Find the best butterfly\n",
    "    best_index = F.index(max(F))\n",
    "    best = X[best_index]\n",
    "    best_fitness = fitness(best, y, k)\n",
    "    \n",
    "    # Set the initial generation/iteration number G=0\n",
    "    G=0\n",
    "    while G < Maxiter:\n",
    "        for i in range(Bf):\n",
    "                    \n",
    "            # Generate a random number r from [0,1]\n",
    "            r = random.uniform(0, 1)\n",
    "            if r < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "            else:\n",
    "                j = random.randint(0, n-1)\n",
    "                for k in range(dim):\n",
    "                    X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "            for j in range(dim):\n",
    "                X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "                \n",
    "            # Convert X to numpy array and select the features\n",
    "            X_selected = np.array(X)[i,:]\n",
    "            X_selected = np.where(X_selected > 0)[0]\n",
    "            \n",
    "            if len(X_selected) == 0:\n",
    "                fitness_val = decimal.Decimal('-inf')\n",
    "            else:\n",
    "                fitness_val = fitness(X_selected, y, k)\n",
    "\n",
    "            # Update the stimulus intensity\n",
    "            if fitness_val > I[i]:\n",
    "                I[i] = fitness_val\n",
    "            \n",
    "            # Check if the updated butterfly is better than the previous best\n",
    "            if fitness_val > best_fitness:\n",
    "                best = X[i]\n",
    "                best_fitness = fitness_val\n",
    "            \n",
    "        # Update the generation/iteration number\n",
    "        G += 1\n",
    "        \n",
    "        # Update the functional response F\n",
    "        F = [decimal.Decimal(c) * decimal.Decimal(I[i])**decimal.Decimal(a) for i in range(n)]\n",
    "        \n",
    "        # Print the current best fitness value after every 10 iterations\n",
    "        if G % 10 == 0:\n",
    "            print(\"Iteration {}: Best Fitness Value = {}\".format(G, best_fitness))\n",
    "    \n",
    "    # Convert the best solution to numpy array and select the features\n",
    "    best_selected = np.array(best)\n",
    "    best_selected = np.where(best_selected > 0)[0]\n",
    "    \n",
    "    return best_fitness, best_selected\n",
    "\n",
    "best_butterfly, best_fitness = butterfly_optimization(X_train_resampled_final, y_train_resampled_final, Maxiter=30, Bf=10, c=3, a=1, p=0.5,k=3)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)\n",
    "\n",
    "# Print the values of the best butterfly to check if they are close to zero\n",
    "print(\"Values of best butterfly:\", [round(x, 5) for x in best_butterfly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fitness(X, y, k):\n",
    "#     # Split the data into training and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "#     # Load the KNN classifier\n",
    "#     clf = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "#     # Fit the KNN classifier on the training set\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     # Predict the labels of the validation set\n",
    "#     y_pred = clf.predict(X_val)\n",
    "\n",
    "#     # Calculate the classification error rate\n",
    "#     error_rate = sum(y_pred != y_val) / len(y_val)\n",
    "\n",
    "#     # Calculate the number of selected features\n",
    "#     num_selected = sum(X > 0)\n",
    "\n",
    "#     # Calculate the fitness value\n",
    "#     fitness = (1 - error_rate) * (1 + num_selected)\n",
    "\n",
    "#     return fitness\n",
    "\n",
    "# def butterfly_optimization(X, y, Maxiter, Bf, c, a, p, k):\n",
    "    \n",
    "#     dim = X.shape[1]\n",
    "#     n = X.shape[0]\n",
    "#     decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    \n",
    "#     # calculate initial stimulus intensity for each butterfly\n",
    "#     I = np.zeros(Bf)\n",
    "#     for i in range(Bf):\n",
    "#         X[i] = np.where(X[i] > 0, 1, 0)\n",
    "#         I[i] = fitness(X[i], y, k)\n",
    "\n",
    "#     X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)] # Convert all values to Decimal\n",
    "    \n",
    "#     F = [decimal.Decimal(c) * decimal.Decimal(I[i])** decimal.Decimal(a) for i in range(n)]\n",
    "\n",
    "#     # Find the best butterfly\n",
    "#     best_index = F.index(max(F))\n",
    "#     best = X[best_index]\n",
    "#     best_fitness = fitness(np.where(best > 0, 1, 0), y, k)\n",
    "    \n",
    "#     # Set the initial generation/iteration number G=0\n",
    "#     G=0\n",
    "#     while G < Maxiter:\n",
    "#         for i in range(Bf):\n",
    "                    \n",
    "#             # Generate a random number r from [0,1]\n",
    "#             r = random.uniform(0, 1)\n",
    "            \n",
    "#             if r < p:\n",
    "#                 for j in range(dim):\n",
    "#                     X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "#             else:\n",
    "#                 j = random.randint(0, n-1)\n",
    "#                 for k in range(dim):\n",
    "#                     X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "                    \n",
    "#             for j in range(dim):\n",
    "#                 X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "#                 X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "                \n",
    "#             # Convert X to numpy array and select the features\n",
    "#             X = [[float(X[i][j]) for j in range(dim)] for i in range(n)]\n",
    "#             X = np.array(X)\n",
    "#             X = np.where(X > 0, 1, 0)\n",
    "            \n",
    "#             # Calculate the fitness values of all butterflies\n",
    "# I = [fitness(X[i], y, k) for i in range(Bf)]\n",
    "\n",
    "# # Find the best butterfly\n",
    "# best_index = I.index(max(I))\n",
    "# best = X[best_index]\n",
    "# best_fitness = I[best_index]\n",
    "\n",
    "# # Set the initial generation/iteration number G=0\n",
    "# G = 0\n",
    "\n",
    "# # Create an empty list to store the best fitness value at each generation\n",
    "# best_fitness_list = []\n",
    "\n",
    "# # Run the optimization loop for Maxiter iterations\n",
    "# while G < Maxiter:\n",
    "    \n",
    "#     # Calculate the stimulus intensity of all butterflies\n",
    "#     F = [c * I[i]**a for i in range(Bf)]\n",
    "    \n",
    "#     # Generate a new population of butterflies\n",
    "#     X_new = np.zeros((Bf, dim))\n",
    "#     for i in range(Bf):\n",
    "        \n",
    "#         # Generate a random number r from [0,1]\n",
    "#         r = random.uniform(0, 1)\n",
    "        \n",
    "#         # Calculate the new position of the butterfly based on the random number and the best butterfly\n",
    "#         if r < p:\n",
    "#             for j in range(dim):\n",
    "#                 X_new[i][j] = X[i][j] + (r**2) * abs(best[j] - X[i][j]) * F[i]\n",
    "#         else:\n",
    "#             j = random.randint(0, Bf-1)\n",
    "#             for k in range(dim):\n",
    "#                 X_new[i][k] = X[i][k] + (r**2) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "        \n",
    "#         # Clamp the position of the butterfly to the search space\n",
    "#         for j in range(dim):\n",
    "#             X_new[i][j] = max(X_new[i][j], -Bf)\n",
    "#             X_new[i][j] = min(X_new[i][j], Bf)\n",
    "    \n",
    "#     # Select the features for each butterfly\n",
    "#     X_new = np.where(X_new > 0, 1, 0)\n",
    "    \n",
    "#     # Calculate the fitness values of the new population of butterflies\n",
    "#     I_new = [fitness(X_new[i], y, k) for i in range(Bf)]\n",
    "    \n",
    "#     # Update the best butterfly if there is a better one in the new population\n",
    "#     if max(I_new) > best_fitness:\n",
    "#         best_index = I_new.index(max(I_new))\n",
    "#         best = X_new[best_index]\n",
    "#         best_fitness = I_new[best_index]\n",
    "    \n",
    "#     # Update the population of butterflies if there is a better one in the new population\n",
    "#     if max(I_new) > max(I):\n",
    "#         X = X_new\n",
    "#         I = I_new\n",
    "    \n",
    "#     # Append the best fitness value to the list of best fitness values\n",
    "#     best_fitness_list.append(best_fitness)\n",
    "    \n",
    "#     # Increment the generation/iteration number\n",
    "#     G += 1\n",
    "\n",
    "# return best, best_fitness, best_fitness_list\n",
    "\n",
    "\n",
    "# best_butterfly, best_fitness = butterfly_optimization(X_train_resampled_final, y_train_resampled_final, Maxiter=30, Bf=10, c=3, a=1, p=0.5,k=3)\n",
    "\n",
    "# # Print the best butterfly and its fitness value\n",
    "# print(\"Best butterfly:\", best_butterfly)\n",
    "# print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa33694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import decimal\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def fitness(X, y, k):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "    X_selected = X[:, X.sum(axis=0) > 0]\n",
    "    clf.fit(X_train[:, X_selected], y_train)\n",
    "    y_pred = clf.predict(X_val[:, X_selected])\n",
    "    error_rate = sum(y_pred != y_val) / len(y_val)\n",
    "    num_selected = X_selected.shape[1]\n",
    "    fitness = 1 - error_rate + num_selected / X.shape[1]\n",
    "    return fitness\n",
    "\n",
    "def butterfly_optimization(X, y, Maxiter, Bf, c, a, p, k):\n",
    "    \n",
    "    dim = X.shape[1]\n",
    "    n = X.shape[0]\n",
    "    decimal.getcontext().prec = 28 # Set the precision of the decimal module\n",
    "    \n",
    "    # Calculate initial stimulus intensity for each butterfly\n",
    "    I = np.zeros(Bf)\n",
    "    for i in range(Bf):\n",
    "        I[i] = fitness(X[i], y, k)\n",
    "        \n",
    "    X = [[decimal.Decimal(str(X[i][j])) for j in range(dim)] for i in range(n)] # Convert all values to Decimal\n",
    "    \n",
    "    F = [decimal.Decimal(c) * decimal.Decimal(I[i])** decimal.Decimal(a) for i in range(n)]\n",
    "\n",
    "    # Find the best butterfly\n",
    "    best_index = F.index(max(F))\n",
    "    best = X[best_index]\n",
    "    best_fitness = fitness(best, y, k)\n",
    "    \n",
    "    # Set the initial generation/iteration number G=0\n",
    "    G=0\n",
    "    fitness_values = []\n",
    "    while G < Maxiter:\n",
    "        for i in range(Bf):\n",
    "                    \n",
    "            # Generate a random number r from [0,1]\n",
    "            r = random.uniform(0, 1)\n",
    "            if r < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += decimal.Decimal(pow(float(r), 2)) * abs(best[j] - X[i][j]) * F[i]\n",
    "            else:\n",
    "                j = random.randint(0, n-1)\n",
    "                for k in range(dim):\n",
    "                    X[i][k] += decimal.Decimal(pow(float(r), 2)) * abs(X[j][k] - X[i][k]) * F[i]\n",
    "            for j in range(dim):\n",
    "                X[i][j] = max(X[i][j], decimal.Decimal(-Bf))\n",
    "                X[i][j] = min(X[i][j], decimal.Decimal(Bf))\n",
    "                \n",
    "            # Convert X to numpy array and select the features\n",
    "            X_selected = np.array(X)[i,:]\n",
    "            X_selected = X[:, X.sum(axis=0) > 0]\n",
    "            \n",
    "                    # Evaluate the fitness of the new butterfly\n",
    "        fitness_value = fitness(X_selected, y, k)\n",
    "        \n",
    "        # Update the intensity of the butterfly if its fitness improves\n",
    "        if fitness_value > I[i]:\n",
    "            I[i] = fitness_value\n",
    "            \n",
    "        # Update the global best if the new butterfly is better\n",
    "        if fitness_value > best_fitness:\n",
    "            best = X[i]\n",
    "            best_fitness = fitness_value\n",
    "    \n",
    "    # Update the value of F for the next generation\n",
    "    F = [decimal.Decimal(c) * decimal.Decimal(I[i])** decimal.Decimal(a) for i in range(n)]\n",
    "    \n",
    "    # Add the best fitness value of the current generation to the list of fitness values\n",
    "    fitness_values.append(best_fitness)\n",
    "    \n",
    "    # Increase the generation/iteration number\n",
    "    G += 1\n",
    "\n",
    "    return best, best_fitness, fitness_values\n",
    "\n",
    "best_butterfly, best_fitness = butterfly_optimization(X_train_resampled_final, y_train_resampled_final, Maxiter=30, Bf=10, c=3, a=1, p=0.5,k=3)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)\n",
    "\n",
    "# Print the values of the best butterfly to check if they are close to zero\n",
    "print(\"Values of best butterfly:\", [round(x, 5) for x in best_butterfly])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bd6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# @ Input 1: Objective function\n",
    "def f(X, X_train, y_train):\n",
    "    X_train_subset = X_train[:, X.astype(bool)]\n",
    "    clf = DecisionTreeClassifier(random_state=0)\n",
    "    scores = cross_val_score(clf, X_train_subset, y_train, cv=5)\n",
    "    return -np.mean(scores)\n",
    "\n",
    "# @ Input 2: X = training set\n",
    "# @ Input 3: Maxiter = Maximum number of iterations\n",
    "# @ Input 4: Bf = Butterfly population factor\n",
    "# @ Input 5: c = Constant factor\n",
    "# @ Input 6: a = Exponent factor\n",
    "# @ Input 7: p = Probability factor\n",
    "\n",
    "def butterfly_optimization(X_train, y_train, Maxiter, Bf, c, a, p):\n",
    "    \n",
    "    # Initialization\n",
    "    \n",
    "    n, dim = X.shape\n",
    "    \n",
    "    I = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        I[i] = f(X[i], X_train_resampled_final_array, y_train_resampled)\n",
    "        \n",
    "    best_index = np.argmin(I)\n",
    "    \n",
    "    best = X[best_index]\n",
    "    \n",
    "    G = 0\n",
    "    \n",
    "    while G < Maxiter:\n",
    "        \n",
    "        # Calculate fragrance\n",
    "        F = [c * I[i]**a for i in range(n)]\n",
    "        \n",
    "        # Find the best butterfly\n",
    "        best_index = np.argmax(F)\n",
    "        best = X[best_index]\n",
    "        \n",
    "        # Update each butterfly\n",
    "        for i in range(n):\n",
    "            \n",
    "            # Move towards best butterfly\n",
    "            if np.random.random() < p:\n",
    "                for j in range(dim):\n",
    "                    X[i][j] += np.random.uniform(-1,1) * abs(best[j] - X[i][j])\n",
    "                    \n",
    "            # Mutual relationship\n",
    "            else:\n",
    "                j = np.random.randint(0,n-1)\n",
    "                while j == i:\n",
    "                    j = np.random.randint(0,n-1)\n",
    "                MutVect = np.random.uniform(-1,1,dim)\n",
    "                # Collaboration strategy\n",
    "                X[i] = X[i] + np.random.uniform(0,1) * MutVect * abs(X[j] - X[i])\n",
    "                \n",
    "            # Update fitness value\n",
    "            I[i] = f(X[i], X_train_resampled_final_array, y_train_resampled)\n",
    "        \n",
    "        # Update the best value\n",
    "        if min(I) < f(best, X_train_resampled_final_array, y_train_resampled):\n",
    "            best_index = np.argmin(I)\n",
    "            best = X[best_index]\n",
    "            \n",
    "        G += 1\n",
    "    \n",
    "    # Get the index of the important features in the best butterfly\n",
    "    important_features_index = np.where(best > 0.5)[0]\n",
    "    \n",
    "    # Print the important features\n",
    "    print(\"Important features:\")\n",
    "    for index in important_features_index:\n",
    "        print(\"Feature\", index+1)\n",
    "    \n",
    "    return best, f(best, X_train_resampled_final, y_train_resampled)\n",
    "\n",
    "X_train_resampled_final_array = np.array(X_train_resampled_final)\n",
    "best_butterfly, best_fitness = butterfly_optimization(X_train_resampled_final_array, y_train_resampled, 100, 10, 0.8, 2, 0.5)\n",
    "\n",
    "# Print the best butterfly and its fitness value\n",
    "print(\"Best butterfly:\", best_butterfly)\n",
    "print(\"Fitness value of best butterfly:\", best_fitness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b20db",
   "metadata": {},
   "source": [
    "## Genetic algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "from sklearn_genetic import GASearchCV\n",
    "from genetic_selection import GeneticSelectionCV\n",
    "from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "_, axes = plot.subplots(nrows=1, ncols=4, figsize=(10, 3))\n",
    "for axis, image, label in zip(axes, data.images, data.target):\n",
    "    axis.set_axis_off()\n",
    "    axis.imshow(image, cmap=plot.cm.gray_r, interpolation='nearest')\n",
    "    axis.set_title('Training: %i' % label)\n",
    "    param_grid = {'min_weight_fraction_leaf': Continuous(0.01, 0.5, distribution='log-uniform'),\n",
    "              'bootstrap': Categorical([True, False]),\n",
    "              'max_depth': Integer(2, 30),\n",
    "              'max_leaf_nodes': Integer(2, 35),\n",
    "              'n_estimators': Integer(100, 300)}\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "# The main class from sklearn-genetic-opt\n",
    "evolved_estimator = GASearchCV(estimator=classifier,\n",
    "                              cv=cv,\n",
    "                              scoring='accuracy',\n",
    "                              param_grid=param_grid,\n",
    "                              verbose=True)\n",
    "\n",
    "evolved_estimator.fit(X_train_pca, y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c32f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from genetic_selection import GeneticSelectionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pds\n",
    "import numpy as num\n",
    "\n",
    "estimators = DecisionTreeClassifier()\n",
    "models = GeneticSelectionCV(\n",
    "    estimators, cv=3, verbose=0,\n",
    "    scoring=\"accuracy\", max_features=5,\n",
    "    n_population=70, crossover_proba=0.5,\n",
    "    mutation_proba=0.2, n_generations=50,\n",
    "    crossover_independent_proba=0.5,\n",
    "    mutation_independent_proba=0.04,\n",
    "    tournament_size=3, n_gen_no_change=10,\n",
    "    caching=True, n_jobs=-1)\n",
    "models = models.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "# Convert X_train_resampled_final to a pandas DataFrame\n",
    "X_train_df = pd.DataFrame(X_train_resampled_final, columns=X_train_resampled_final.columns)\n",
    "print('Feature Selection:', X_train_df.columns[models.support_])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
