{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff27a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of column 'A' from float64 to float32\n",
    "df['amount'] = df['amount'].astype('float32')\n",
    "df['oldbalanceOrg'] = df['oldbalanceOrg'].astype('float32')\n",
    "df['oldbalanceDest'] = df['oldbalanceDest'].astype('float32')\n",
    "df['newbalanceOrig'] = df['newbalanceOrig'].astype('float32')\n",
    "df['newbalanceDest'] = df['newbalanceDest'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step'] = df['step'].astype('int32')\n",
    "df['isFlaggedFraud'] = df['isFlaggedFraud'].astype('int32') \n",
    "df['isFraud'] = df['isFraud'].astype('int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LabelEncoder from Sklearn\n",
    "# library from preprocessing Module.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Creating a instance of label Encoder.\n",
    "le = LabelEncoder()\n",
    "# Using .fit_transform function to fit label\n",
    "# encoder and return encoded label\n",
    "label = le.fit_transform(df['nameDest'])\n",
    "# printing label\n",
    "label\n",
    "# removing the column 'type' from df\n",
    "# as it is of no use now.\n",
    "df.drop(\"nameDest\", axis=1, inplace=True)\n",
    "# Appending the array to our dataFrame\n",
    "# with column name 'type'\n",
    "df[\"nameDest\"] = label\n",
    "# printing Dataframe\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23413d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LabelEncoder from Sklearn\n",
    "# library from preprocessing Module.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Creating a instance of label Encoder.\n",
    "le = LabelEncoder()\n",
    "# Using .fit_transform function to fit label\n",
    "# encoder and return encoded label\n",
    "label = le.fit_transform(df['type'])\n",
    "# printing label\n",
    "label\n",
    "# removing the column 'type' from df\n",
    "# as it is of no use now.\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "# Appending the array to our dataFrame\n",
    "# with column name 'type'\n",
    "df[\"type\"] = label\n",
    "# printing Dataframe\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa4890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameOrig'])\n",
    "label\n",
    "df.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df[\"nameOrig\"] = label\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce59670",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24596c67",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=18)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5c966",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "\n",
    "# Assume X_train and y_train are the original training data\n",
    "# resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='auto')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)\n",
    "\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053597f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_train)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6c1278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='auto')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b97588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_train)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d229f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d8e9a",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.14, 'oldbalanceOrg': 0.24, 'newbalanceOrig': 0.25, 'oldbalanceDest': 0.22, 'newbalanceDest': 0.22}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train_resampled_final.loc[X_train_resampled_final[col_name] > np.percentile(X_train_resampled_final[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "\n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your original training data is stored in a pandas DataFrame called X_train\n",
    "# And assuming you have a list of selected feature names called selected_features\n",
    "selected_features = ['oldbalanceOrg', 'type', 'nameDest','amount','step']\n",
    "X_train_selected = X_train_resampled_final[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_selected=X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9db835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Select top features using Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_selected, y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7b6f1",
   "metadata": {},
   "source": [
    "## Hyperparamter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "# Define your hyperparameter search space\n",
    "param_dist = { \n",
    "    'n_estimators': sp_randint(100, 400),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : sp_randint(2,3),\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv  # Required to enable HalvingRandomSearchCV\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "\n",
    "# Set up the HalvingRandomSearchCV with aggressive early stopping\n",
    "search = HalvingRandomSearchCV(rf, param_dist, cv=5,verbose=1, \n",
    "                               factor=2, resource='n_samples', max_resources=100, \n",
    "                               aggressive_elimination=True, random_state=2, \n",
    "                               scoring='accuracy', refit=True)\n",
    "\n",
    "# Fit the HalvingRandomSearchCV object to the data\n",
    "search.fit(X_train_selected, y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e274807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters and evaluate on the test set\n",
    "best_params = search.best_params_\n",
    "best_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494084e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544eb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Test set accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18cae1",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c033fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1:10\n",
    "#3:5\n",
    "#2:8\n",
    "#1:20\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, recall_score, accuracy_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of splits for stratified cross-validation\n",
    "n_splits = 2\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "# Create lists to store evaluation metrics for each fold\n",
    "f1_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Create lists to store ROC curve data for each fold\n",
    "fprs = []\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "# Initialize the OOB error list\n",
    "oob_error = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_selected, y_train_resampled)):\n",
    "    print(f'Fold: {fold+1}')\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    X_fold_train, y_fold_train = X_train_selected.iloc[train_idx], y_train_resampled.iloc[train_idx]\n",
    "    X_val, y_val = X_train_selected.iloc[val_idx], y_train_resampled.iloc[val_idx]\n",
    "    \n",
    "    class_weights = {0: 3, 1: 30}  # Class 0 has weight 1, class 1 has weight 10\n",
    "    \n",
    "    # Create a RandomForestClassifier object with the given hyperparameters\n",
    "    rf_model = RandomForestClassifier(oob_score=True, class_weight=class_weights)\n",
    "   \n",
    "    # Fit the model on the training data\n",
    "    rf_model.fit(X_fold_train, y_fold_train)\n",
    "\n",
    "    # Predict the class labels for the validation set\n",
    "    y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "    # Compute the evaluation metrics for the current fold\n",
    "    conf_mat = confusion_matrix(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "    # Append the evaluation metrics for the current fold to the lists\n",
    "    f1_scores.append(f1)\n",
    "    recall_scores.append(recall)\n",
    "    precision_scores.append(precision)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    \n",
    "    # Compute the ROC curve and AUC for the current fold\n",
    "    fpr, tpr, _ = roc_curve(y_val, rf_model.predict_proba(X_val)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Append the ROC curve data for the current fold to the lists\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    \n",
    "    # Compute the OOB error for the current fold and append to the list\n",
    "    oob_error.append(1 - rf_model.oob_score_)\n",
    "\n",
    "    # Print the evaluation metrics for the current fold\n",
    "    print('Confusion matrix:\\n', conf_mat)\n",
    "    print('Recall:', recall)\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('F1-score:', f1)\n",
    "    print('OOB error:', 1 - rf_model.oob_score_)\n",
    "    print('---------------------')\n",
    "\n",
    "# Create the ROC curve plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# Plot the ROC curve for each fold\n",
    "for i in range(n_splits):\n",
    "    ax.plot(fprs[i], tprs[i], lw=2, label='Fold %d (AUC = %0.2f)' % (i+1, aucs[i]))\n",
    "\n",
    "# Add a dashed line representing the random guess classifier\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='black', label='Random guess')\n",
    "\n",
    "# Add labels and legend to the plot\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver Operating Characteristic')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average F1-score:', sum(f1_scores)/len(f1_scores))\n",
    "print('Average Recall_scores:', sum(recall_scores)/len(recall_scores))\n",
    "print('Average Recall_scores:', sum(precision_scores)/len(precision_scores))\n",
    "print('Average Recall_scores:', sum(accuracy_scores)/len(accuracy_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8dedd",
   "metadata": {},
   "source": [
    "## Evaluate test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320da02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the class labels for the test set\n",
    "# y_test_pred = rf_model.predict(X_test_selected)\n",
    "\n",
    "# # Compute the evaluation metrics for the test set\n",
    "# conf_mat_test = confusion_matrix(y_test, y_test_pred)\n",
    "# recall_test = recall_score(y_test, y_test_pred)\n",
    "# accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "# precision_test = precision_score(y_test, y_test_pred)\n",
    "# f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# # Print the evaluation metrics for the test set\n",
    "# print('Evaluation metrics on test set:')\n",
    "# print('Confusion matrix:\\n', conf_mat_test)\n",
    "# print('Recall:', recall_test)\n",
    "# print('Accuracy:', accuracy_test)\n",
    "# print('Precision:', precision_test)\n",
    "# print('F1-score:', f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the class labels and probabilities for the test set\n",
    "# y_test_pred = rf_model.predict(X_test_selected)\n",
    "# y_test_prob = rf_model.predict_proba(X_test_selected)[:, 1]\n",
    "# # Compute the false positive rate, true positive rate, and AUC for the test set\n",
    "# fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_prob)\n",
    "# roc_auc_test = auc(fpr_test, tpr_test)\n",
    "# # Plot the ROC curve for the test set\n",
    "# plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='Test ROC curve (AUC =%0.2f)' % roc_auc_test)\n",
    "# plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve for Test Set')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79d324",
   "metadata": {},
   "source": [
    "## Evaluate test- NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19f292",
   "metadata": {},
   "source": [
    "Instead, threshold moving can help to optimize the performance of the classifier on the imbalanced test set by adjusting the threshold used to assign the predicted probabilities to the class labels. By setting an appropriate threshold, we can improve the balance between precision and recall and reduce the impact of the class imbalance on the performance of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3645b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "# Get predicted probabilities for the test data\n",
    "y_prob = rf_model.predict_proba(X_test_selected)[:,1]\n",
    "\n",
    "# Set different thresholds and compute precision, recall, and F1-score for each threshold\n",
    "thresholds = np.arange(0.1,10,0.01)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    precision_scores.append(precision[1])\n",
    "    recall_scores.append(recall[1])\n",
    "    f1_scores.append(f1[1])\n",
    "\n",
    "# Find the optimal threshold that maximizes the F1-score\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Assign the class labels based on the optimal threshold\n",
    "y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate the performance of the classifier for the optimal threshold\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeffcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get predicted probabilities for the test data\n",
    "y_prob = rf_model.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Set different thresholds and compute precision, recall, and F1-score for each threshold\n",
    "thresholds = np.arange(0.1, 10, 0.01)\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    precision_scores.append(precision[1])\n",
    "    recall_scores.append(recall[1])\n",
    "    f1_scores.append(f1[1])\n",
    "\n",
    "# Find the optimal threshold that maximizes the F1-score\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Assign the class labels based on the optimal threshold\n",
    "y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate the performance of the classifier for the optimal threshold\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print('Average F1-score:', np.mean(f1_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
