{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "623bc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e04005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b331399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c6cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b2d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666703a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c572aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3595ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032e705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0        236    4680.870000   42991.040000             0.0            0.00   \n",
      "1        138   44423.330000   51412.000000             0.0        67584.28   \n",
      "2        325  138522.570000    4564.000000             0.0        67584.28   \n",
      "3        308  300712.340000   51474.000000             0.0        67584.28   \n",
      "4        331   65301.220000     389.000000             0.0        67584.28   \n",
      "...      ...            ...            ...             ...             ...   \n",
      "413442   277  111168.880136  111168.880136             0.0        67584.28   \n",
      "413443   274  138522.570000   51412.000000             0.0        67584.28   \n",
      "413444    60  138522.570000   51412.000000             0.0            0.00   \n",
      "413445   449   44882.356239   44882.356239             0.0            0.00   \n",
      "413446   220   39953.091459   29059.334627             0.0        67584.28   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0             0.000000               0     3    159150     74387  \n",
      "1        238394.800000               0     0     92881    180374  \n",
      "2        238394.800000               0     1     80756    482539  \n",
      "3        654217.020000               0     1    175711    597630  \n",
      "4        238394.800000               0     1    110818    514027  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "413442   238394.800000               0     1     90379    472585  \n",
      "413443   238394.800000               0     1    112071    494845  \n",
      "413444        0.000000               0     1    154830    240268  \n",
      "413445    36237.626509               0     1    122579     88980  \n",
      "413446   238394.800000               0     1     93537    130866  \n",
      "\n",
      "[413447 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "485376  278.0   22928.58            0.0             0.0           0.000   \n",
      "642214   45.0    8606.90         5764.0             0.0           0.000   \n",
      "192982  237.0  220046.83            0.0             0.0      130797.505   \n",
      "99091   328.0   83938.53        13653.5             0.0      130797.505   \n",
      "203398  307.0   74636.86            0.0             0.0      130797.505   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "230877  154.0  195805.05        31725.0             0.0           0.000   \n",
      "315026  301.0   36352.03        13653.5             0.0           0.000   \n",
      "661254  238.5  163969.90        13653.5             0.0      130797.505   \n",
      "688112  280.0    3092.79            0.0             0.0           0.000   \n",
      "642560   35.0   74636.86        30807.0             0.0      130797.505   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "485376           0.000               0     3    291184    424837  \n",
      "642214           0.000               0     3    363649    442961  \n",
      "192982      214326.245               0     1      1853    410946  \n",
      "99091       537297.070               0     0    252825    347652  \n",
      "203398      214326.245               0     1    201182    417173  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "230877      195805.050               0     1    181881    192704  \n",
      "315026           0.000               0     3    458861    630843  \n",
      "661254      706564.020               0     0     37270    676511  \n",
      "688112           0.000               0     3    455345    152073  \n",
      "642560      214326.245               0     1    214954    689599  \n",
      "\n",
      "[70000 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbaee3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19032385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "660fad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DA5FD010D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001DA5FD010D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "6453/6461 [============================>.] - ETA: 0s - loss: 0.4629WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DA647DC8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001DA647DC8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "6461/6461 [==============================] - 18s 3ms/step - loss: 0.4628 - val_loss: 0.4879\n",
      "Epoch 2/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4132 - val_loss: 0.4794\n",
      "Epoch 3/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4094 - val_loss: 0.4776\n",
      "Epoch 4/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4082 - val_loss: 0.4769\n",
      "Epoch 5/10\n",
      "6461/6461 [==============================] - 18s 3ms/step - loss: 0.4078 - val_loss: 0.4765\n",
      "Epoch 6/10\n",
      "6461/6461 [==============================] - 18s 3ms/step - loss: 0.4075 - val_loss: 0.4764\n",
      "Epoch 7/10\n",
      "6461/6461 [==============================] - 18s 3ms/step - loss: 0.4074 - val_loss: 0.4762\n",
      "Epoch 8/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4073 - val_loss: 0.4762\n",
      "Epoch 9/10\n",
      "6461/6461 [==============================] - 17s 3ms/step - loss: 0.4072 - val_loss: 0.4761\n",
      "Epoch 10/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4072 - val_loss: 0.4761\n",
      "Test MSE: 0.43020\n",
      "Epoch 1/10\n",
      "6461/6461 [==============================] - 18s 3ms/step - loss: 0.4760 - val_loss: 0.4072\n",
      "Epoch 2/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4760 - val_loss: 0.4071\n",
      "Epoch 3/10\n",
      "6461/6461 [==============================] - 20s 3ms/step - loss: 0.4759 - val_loss: 0.4071\n",
      "Epoch 4/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4759 - val_loss: 0.4070\n",
      "Epoch 5/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4759 - val_loss: 0.4071\n",
      "Epoch 6/10\n",
      "6461/6461 [==============================] - 18s 3ms/step - loss: 0.4759 - val_loss: 0.4071\n",
      "Epoch 7/10\n",
      "6461/6461 [==============================] - 18s 3ms/step - loss: 0.4759 - val_loss: 0.4070\n",
      "Epoch 8/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4758 - val_loss: 0.4070\n",
      "Epoch 9/10\n",
      "6461/6461 [==============================] - 20s 3ms/step - loss: 0.4758 - val_loss: 0.4070\n",
      "Epoch 10/10\n",
      "6461/6461 [==============================] - 19s 3ms/step - loss: 0.4758 - val_loss: 0.4070\n",
      "Test MSE: 0.43008\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 15\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='tanh', kernel_regularizer=regularizers.l1(0.000391))(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "opt = Adam(lr= 0.00087)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder model with the specified optimizer and loss function\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    # Define early stopping to prevent overfitting and improve efficiency\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10,batch_size=32, verbose=1, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26098e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.models import Model\n",
    "# from keras import regularizers\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.layers import Input, Dense\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Define the autoencoder architecture\n",
    "# input_dim = X_train_resampled_final.shape[1]\n",
    "# encoding_dim = 32\n",
    "# decoding_dim = 10\n",
    "\n",
    "# input_layer = Input(shape=(input_dim,))\n",
    "# hidden_layer = Dense(encoding_dim, activation='tanh', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "# output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# # Define the optimizer with the desired learning rate\n",
    "# opt = Adam(lr= 0.00087)\n",
    "\n",
    "# # Define the autoencoder model\n",
    "# autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# # Compile the autoencoder model with the specified optimizer and loss function\n",
    "# autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# # Define the number of folds for cross-validation\n",
    "# n_splits = 5\n",
    "# kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# # Define lists to store the MSE of training and validation sets for each fold\n",
    "# train_mse = []\n",
    "# val_mse = []\n",
    "# test_mse = []\n",
    "# recon_errors = []\n",
    "\n",
    "# # Loop over each fold\n",
    "# for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "#     # Split the data into training and validation sets for the current fold\n",
    "#     X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "#     # Define early stopping to prevent overfitting and improve efficiency\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#     # Fit the autoencoder on the training set for the current fold\n",
    "#     history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10,batch_size=32, verbose=1, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "#     # Append the MSE of training and validation sets for the current fold to the lists\n",
    "#     train_mse.append(history.history['loss'])\n",
    "#     val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "#     # compute the reconstruction error for the test data\n",
    "#     recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     recon_errors.append(recon_error)\n",
    "    \n",
    "#     # Calculate the MSE for the test set\n",
    "#     test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     test_mse.append(test_error)\n",
    "#     print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# # Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "# mean_train_mse = np.mean(train_mse, axis=0)\n",
    "# std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "# mean_val_mse = np.mean(val_mse, axis=0)\n",
    "# std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# # Plot the MSE of training and validation sets against the number of epochs\n",
    "# epochs = range(1, len(mean_train_mse)+1)\n",
    "# plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "# plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "# plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "# plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Mean Squared Error (MSE)')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7b871a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DA5F475438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DA5F475438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2188/2188 [==============================] - 3s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate reconstructed outputs for the test set\n",
    "reconstructed = autoencoder.predict(X_test)\n",
    "\n",
    "# Calculate the MSE between the input and the reconstructed output for each data point\n",
    "recon_errors = np.mean(np.power(X_test - reconstructed, 2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98f92d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse_fold = np.mean(recon_errors)\n",
    "test_mse.append(test_mse_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "687edc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reconstruction Error: 0.42993\n"
     ]
    }
   ],
   "source": [
    "# Print the final mean and standard deviation of reconstruction error across all folds\n",
    "print(f\"Mean Reconstruction Error: {np.mean(test_mse):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b62211a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188/2188 [==============================] - 3s 1ms/step - loss: 0.4301\n",
      "Mean squared error on test data: 0.43007633090019226\n"
     ]
    }
   ],
   "source": [
    "mse = autoencoder.evaluate(X_test, X_test)\n",
    "print('Mean squared error on test data:', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18bdc029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train MSE (Average): 0.44484832137823105\n"
     ]
    }
   ],
   "source": [
    "mean_train_mse_avg = np.mean(mean_train_mse)\n",
    "print(\"Mean Train MSE (Average):\", mean_train_mse_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea539278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4301a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(autoencoder, to_file='autoencoder_last.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa66b3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DA5F4CB828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DA5F4CB828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12921/12921 [==============================] - 16s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input = Input(shape=(hidden_layer_output_train.shape[1],))\n",
    "x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.00111))(new_model_input)\n",
    "output = Dense(2, activation='sigmoid')(x)\n",
    "#output = Dense(1, activation='softmax')(x)\n",
    "mediator_network = Model(inputs=new_model_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68b82758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(mediator_network , to_file='mediator_network .png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76dd86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(hidden_layer_model, to_file='autoencoder_last.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a28c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert target labels to one-hot encoded format\n",
    "y_train_resampled_final_onehot = to_categorical(y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40ae75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_new = Adam(lr= 0.00392)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a908a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1980\\796160800.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Compile the new model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmediator_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Train the new model on the activations of the hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt_new' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "#Compile the new model\n",
    "mediator_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "history = mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot,\n",
    "                               epochs=10, batch_size=32, validation_split=0.3,\n",
    "                               callbacks=[early_stopping])\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "#history=mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot, epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d72c5466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_mean = sum(history.history['loss']) / len(history.history['loss'])\n",
    "val_loss_mean = sum(history.history['val_loss']) / len(history.history['val_loss'])\n",
    "\n",
    "print(\"Train Loss Mean:\", train_loss_mean)\n",
    "print(\"Validation Loss Mean:\", val_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2e9f498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n",
      "         ..\n",
      "437415    1\n",
      "437416    1\n",
      "437417    1\n",
      "437418    1\n",
      "437419    1\n",
      "Name: isFraud, Length: 437420, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0fd12",
   "metadata": {},
   "source": [
    "## Create the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fa72b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DA5FF7B9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001DA5FF7B9D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12921/12921 [==============================] - 18s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = Model(inputs=mediator_network .input, outputs=mediator_network .layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.0000611))(new_model_input_med)\n",
    "# x = Dense(32, activation='relu')(x)\n",
    "output_med = Dense(2, activation='sigmoid')(x)\n",
    "#output = Dense(1, activation='softmax')(x)\n",
    "agent_network = Model(inputs=new_model_input_med, outputs=output_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66634a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the input layer for the agent network\n",
    "# agent_input = Input(shape=(hidden_layer_output_train.shape[1],))\n",
    "\n",
    "# # Define the new Dense layer for the agent network\n",
    "# x = Dense(4, activation='relu')(agent_input)\n",
    "\n",
    "# output_agent= Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# # Define the agent network model\n",
    "# agent_network = Model(inputs=agent_input, outputs=output_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network , to_file='agent_network .png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db656ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(hidden_layer_model_med , to_file='agent_network .png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fde33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ef889bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt_new = Adam(lr= 0.007312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fd3b79a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_resampled_final_onehot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1980\\3371380380.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Train the new model on the activations of the hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n\u001b[0m\u001b[0;32m     13\u001b[0m                                \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                                callbacks=[early_stopping],verbose=0)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_resampled_final_onehot' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "#Compile the new model\n",
    "#agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "# Compile the new model\n",
    "agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n",
    "                               epochs=15, batch_size=32, validation_split=0.2,\n",
    "                               callbacks=[early_stopping],verbose=0)\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "#history=mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot, epochs=10, batch_size=32,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49059cf",
   "metadata": {},
   "source": [
    "## New agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the hidden layer model\n",
    "# hidden_layer_model_med = Model(inputs=mediator_network .input, outputs=mediator_network .layers[1].output)\n",
    "\n",
    "# # Get the activations of the hidden layer for the training data\n",
    "# hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.00211))(new_model_input_med)\n",
    "# x1 = Dense(8, activation='tanh')(x)\n",
    "# output_med = Dense(2, activation='sigmoid')(x1)\n",
    "# agent_network = Model(inputs=new_model_input_med, outputs=output_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82e4c7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000157575BC318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000157575BC318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "8841/8864 [============================>.] - ETA: 0s - loss: 0.0803WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001575C560C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001575C560C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "8864/8864 [==============================] - 21s 2ms/step - loss: 0.0802 - val_loss: 3.9815\n",
      "Epoch 2/10\n",
      "8864/8864 [==============================] - 20s 2ms/step - loss: 0.0787 - val_loss: 4.4451\n",
      "Epoch 3/10\n",
      "8864/8864 [==============================] - 22s 2ms/step - loss: 0.0787 - val_loss: 4.3215\n",
      "Epoch 4/10\n",
      "8864/8864 [==============================] - 21s 2ms/step - loss: 0.0787 - val_loss: 4.3674\n",
      "Epoch 5/10\n",
      "8864/8864 [==============================] - 22s 2ms/step - loss: 0.0787 - val_loss: 4.2668\n",
      "Epoch 6/10\n",
      "8864/8864 [==============================] - 22s 2ms/step - loss: 0.0787 - val_loss: 4.1611\n"
     ]
    }
   ],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the early stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# # Train the new model on the activations of the hidden layer\n",
    "# history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n",
    "#                                epochs=10, batch_size=32, validation_split=0.3,\n",
    "#                                callbacks=[early_stopping],verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "160f9862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation loss curves\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e050d1e",
   "metadata": {},
   "source": [
    "## 21/04/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c48710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 41\n",
    "max_steps = 7\n",
    "learning_rate=0.5\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "num_features=10\n",
    "state_input = Input(shape=(num_features,))\n",
    "action_input = Input(shape=(num_actions,))\n",
    "merged = Concatenate()([state_input, action_input])\n",
    "x = Dense(10, activation='tanh')(merged)\n",
    "q_values = Dense(num_actions)(x)\n",
    "\n",
    "agent_network = Model(inputs=[state_input, action_input], outputs=q_values)\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "def Q(states, agent_network):\n",
    "    num_states = states.shape[0]\n",
    "    num_actions = agent_network.output_shape[-1]\n",
    "    Q_values = np.zeros((num_states, num_actions))\n",
    "    for i, state in enumerate(states):\n",
    "        for j in range(num_actions):\n",
    "            action_one_hot = np.zeros(num_actions)\n",
    "            action_one_hot[j] = 1\n",
    "            Q_values[i][j] = agent_network.predict([state.reshape(1, -1), action_one_hot.reshape(1, -1)])[0][j]\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    #state = D[0][0]\n",
    "    #state= hidden_layer_output_train_med[0, 0]\n",
    "    state = hidden_layer_output_train_med[0, :]\n",
    "    \n",
    "    # Initialize index counter\n",
    "    idx = 0\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        #next_state = D[step+1][0]\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[idx, 0]\n",
    "        idx += 1\n",
    "        \n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c45b07",
   "metadata": {},
   "source": [
    "## 24/03/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19fc0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.5\n",
      "Accuracy: 0.5217391304347826\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.21\n",
    "batch_size = 32\n",
    "num_episodes = 10\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.009\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 2000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loss_mean = sum(history.history['loss']) / len(history.history['loss'])\n",
    "# val_loss_mean = sum(history.history['val_loss']) / len(history.history['val_loss'])\n",
    "\n",
    "# print(\"Train Loss Mean:\", train_loss_mean)\n",
    "# print(\"Validation Loss Mean:\", val_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the reward function\n",
    "# def reward_fn(action, label):\n",
    "#     DF = [0,1,2]  # indices of fraud class\n",
    "#     DN = [81,    787,   2392,   3121,   3449]  # indices of non-fraud class\n",
    "#     terminal = 0  # initialize terminal flag to 0\n",
    "#     if label in DF:\n",
    "#         if action == label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     else:\n",
    "#         if action == label:\n",
    "#             reward = 0.5  # set  to 0.5\n",
    "#         else:\n",
    "#             reward = -0.5  # set  to -0.5\n",
    "#     return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc280510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "# import random\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # # Define the reward function\n",
    "# # def reward_fn(action, label):\n",
    "# #     # Replace with your own reward function\n",
    "# #     if action == label:\n",
    "# #         return 1\n",
    "# #     else:\n",
    "# #         return -1\n",
    "\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 2  # Number of episodes\n",
    "# T = 2  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # mediator_network = keras.models.Sequential([\n",
    "# #     keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "# #     keras.layers.Dense(10, activation='softmax')\n",
    "# # ])\n",
    "# # mediator_network.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=learning_rate_val))\n",
    "\n",
    "# # Generate dataset D\n",
    "# # X_train_resampled_final_20 = np.hstack((X_train_resampled_final, np.zeros((X_train_resampled_final.shape[0], 10))))\n",
    "# # hidden_layer_output = [mediator_network.predict(np.array([x]*2).reshape(2, -1))[0] for x in X_train_resampled_final]\n",
    "# # D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# hidden_layer_output = mediator_network.predict(X_train_resampled_final)\n",
    "# D = [(hidden_layer_output[i], y_train_resampled_final[i]) for i in range(len(hidden_layer_output))]\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch - CONCEPT OF GRADIENT DESCENT\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cf50d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "# import random\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 2  # Number of episodes\n",
    "# T = 2  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # # Define agent network\n",
    "# # agent_network = keras.models.Sequential([\n",
    "# #     keras.layers.Dense(20, activation='relu', input_shape=(10,)),\n",
    "# #     keras.layers.Dense(10, activation='softmax')\n",
    "# # ])\n",
    "# # agent_network.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(learning_rate=learning_rate_val))\n",
    "\n",
    "# # Define reward function\n",
    "# def reward_fn(action, label):\n",
    "#     # Replace with your own reward function\n",
    "#     if action == label:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return -1\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     #np.random.shuffle(D)\n",
    "#     # Shuffle the training data\n",
    "#     D=np.random.shuffle(X_train_resampled_final)\n",
    "\n",
    "    \n",
    "#     # Initialize state\n",
    "#     #state = D[0][0]\n",
    "#     # Initialize state\n",
    "\n",
    "#     #state = X_train_resampled_final[0]\n",
    "#     state = X_train_resampled_final[0][:7]\n",
    "\n",
    "#     #state = X_train_resampled_final[0].reshape(1, -1)\n",
    "\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         #action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d5e65c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# K=25\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0][:7]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 7))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# KC=10\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(KC):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0][:7]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 7))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdea785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# KC7=2\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(KC7):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0][:7]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, 7)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 7))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# K1=5\n",
    "# batch_size = 32\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K1):\n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, state.shape[0]))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcf98c",
   "metadata": {},
   "source": [
    "## New trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551457ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reward function\n",
    "def reward_fn(action, label):\n",
    "    terminal = 0  # initialize terminal flag to 0\n",
    "    if label in [0, 1, 2]:\n",
    "        if action == label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    else:\n",
    "        if action == label:\n",
    "            reward = 0.5  # set  to 0.5\n",
    "        else:\n",
    "            reward = -0.5  # set  to -0.5\n",
    "    return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27c3472f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Reward: -1\n",
      "True label in dataset is: 1 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 1\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 0\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Reward: 1\n",
      "True label in dataset is: 0 , agent has predicted 0\n",
      "Terminal: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import models, layers\n",
    "import random\n",
    "\n",
    "# Shuffle the training data\n",
    "X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# Initialize replay memory with M capacity\n",
    "M = 10000\n",
    "replay_memory = []\n",
    "\n",
    "# Initialize simulation environment\n",
    "env = None  # Replace with your own simulation environment\n",
    "\n",
    "# Define hyperparameters\n",
    "K2 = 10  # Number of episodes\n",
    "T = 3  # Number of timesteps per episode\n",
    "gamma = 0.9  # Discount factor\n",
    "batch_size = 32\n",
    "learning_rate_val = 0.001\n",
    "\n",
    "# Train agent\n",
    "for k in range(K2):\n",
    "    # Shuffle dataset D\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Choose action\n",
    "        action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward = reward_fn(action, D[t][1])\n",
    "        terminal = 1 if t == T - 1 else 0\n",
    "#         print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "        # Calculate reward and terminal flag\n",
    "        true_label = D[t][1]\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"True label in dataset is:\", true_label, \", agent has predicted\", action)\n",
    "        print(\"Terminal:\", terminal)\n",
    "\n",
    "        \n",
    "        # Update state\n",
    "        state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "        # Store transition in replay memory\n",
    "        replay_memory.append((state, action, reward, state_next, terminal))\n",
    "        if len(replay_memory) > M:\n",
    "            replay_memory.pop(0)\n",
    "        \n",
    "        # Sample minibatch from replay memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            minibatch = random.sample(replay_memory, batch_size)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        X = np.zeros((batch_size, 10))\n",
    "        y = np.zeros((batch_size, 10))\n",
    "        for i in range(batch_size):\n",
    "            state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "            X[i] = state_i\n",
    "            y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "            if terminal_i:\n",
    "                y[i][action_i] = reward_i\n",
    "            else:\n",
    "                y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "        # Train agent network on minibatch - CONCEPT OF GRADIENT DESCENT\n",
    "        agent_network.train_on_batch(X, y)\n",
    "        \n",
    "        # Update state\n",
    "        state = state_next\n",
    "        \n",
    "        # Check if episode is over\n",
    "        if terminal:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import keras\n",
    "# from keras import models, layers\n",
    "# import random\n",
    "\n",
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize an empty list to store the cumulative reward obtained by the agent over time\n",
    "# cumulative_rewards = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K2 = 5  # Number of episodes\n",
    "# T = 5  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K2):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "#         # Choose action\n",
    "#         action = agent_network.predict(state.reshape(1, -1)).argmax()\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward = reward_fn(action, D[t][1])\n",
    "#         terminal = 1 if t == T - 1 else 0\n",
    "#         # print(\"Reward:\", reward) # Add this line to print the reward\n",
    "#         # print(\"Terminal:\", terminal) # Add this line to print the terminal flag\n",
    "#         # Calculate reward and terminal flag\n",
    "#         true_label = D[t][1]\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"True label in dataset is:\", true_label, \", agent has predicted\", action)\n",
    "#         print(\"Terminal:\", terminal)\n",
    "        \n",
    "#         # Compute the cumulative reward obtained by the agent for this episode\n",
    "#         episode_reward = 0\n",
    "#         if len(replay_memory) >= T:\n",
    "#             for t_ in range(T):\n",
    "#                 episode_reward += replay_memory[-T+t_][2][0]\n",
    "#             cumulative_rewards.append(episode_reward)\n",
    "\n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "        \n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "        \n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 10))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 y[i][action_i] = reward_i + gamma * np.max(agent_network.predict(state_next_i.reshape(1, -1)))\n",
    "        \n",
    "#         # Train agent network on minibatch - CONCEPT OF GRADIENT DESCENT\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "        \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "        \n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break\n",
    "            \n",
    "# # Plot the cumulative reward obtained by the agent over time\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(cumulative_rewards)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Cumulative Reward')\n",
    "# plt.title('Cumulative Reward over Time')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e09a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    #Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    #Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    \n",
    "    return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # Shuffle the training data\n",
    "# X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "# D = list(zip(X_train_shuffled, y_train_resampled_final))\n",
    "\n",
    "# # Initialize replay memory with M capacity\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# env = None  # Replace with your own simulation environment\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 10  # Number of episodes\n",
    "# T = 4  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # Randomly initialize parameters \n",
    "# theta = np.random.rand(10, 10)\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # Shuffle dataset D\n",
    "#     np.random.shuffle(D)\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = D[0][0]\n",
    "    \n",
    "# for t in range(T):\n",
    "#     # Choose action based on the policy  (s,a)\n",
    "#     logits = np.dot(state, theta)\n",
    "#     action_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "#     action = np.random.choice(range(len(action_probs)), p=action_probs)\n",
    "\n",
    "#     # Calculate reward and terminal flag\n",
    "#     true_label = D[t][1]\n",
    "#     reward, terminal = reward_fn(action, true_label, action, lambda_val=0.1)\n",
    "#     print(\"Step:\", t, \"True Label:\", true_label, \"Action:\", action, \"Reward:\", reward)\n",
    "\n",
    "#     # Update state\n",
    "#     state_next = D[t+1][0] if t < T - 1 else state\n",
    "\n",
    "#     # Store transition in replay memory\n",
    "#     replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#     if len(replay_memory) > M:\n",
    "#         replay_memory.pop(0)\n",
    "\n",
    "#     # Sample minibatch from replay memory\n",
    "#     if len(replay_memory) >= batch_size:\n",
    "#         minibatch = random.sample(replay_memory, batch_size)\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "#     X = np.zeros((batch_size, 10))\n",
    "#     y = np.zeros((batch_size, 2))\n",
    "#     for i in range(batch_size):\n",
    "#         state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#         X[i] = state_i\n",
    "#         y[i] = agent_network.predict(state_i.reshape(1, -1))[0]\n",
    "\n",
    "#         if terminal_i:\n",
    "#             y[i][action_i] = reward_i\n",
    "#         else:\n",
    "#             # Choose action based on the policy  (s,a) for the next state\n",
    "#             logits_next = np.dot(state_next_i, theta)\n",
    "#             action_probs_next = np.exp(logits_next) / np.sum(np.exp(logits_next))\n",
    "#             next_action = np.random.choice(range(2), p=action_probs_next)\n",
    "#             y[i][action_i] = reward_i + gamma * y[i][next_action]\n",
    "\n",
    "#     # Train agent network on minibatch using gradient descent\n",
    "#     agent_network.train_on_batch(X, y)\n",
    "\n",
    "#     # Update state\n",
    "#     state = state_next\n",
    "\n",
    "#     # Check if episode is over\n",
    "#     if terminal:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01187701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "\n",
    "# # # Shuffle the training data\n",
    "# # X_train_shuffled = np.random.permutation(X_train_resampled_final)\n",
    "\n",
    "# #zip() function is used to pair up each element of X_train_shuffled (training data) with its \n",
    "# #corresponding element of y_train_resampled_final (training labels), creating a list of tuples\n",
    "# D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# # Initialize replay memory with capacity, M\n",
    "# M = 10000\n",
    "# replay_memory = []\n",
    "\n",
    "# # Define hyperparameters\n",
    "# K = 10  # Number of episodes\n",
    "# T = 3  # Number of timesteps per episode\n",
    "# gamma = 0.9  # Discount factor\n",
    "# batch_size = 32\n",
    "# learning_rate_val = 0.001\n",
    "\n",
    "# # Randomly initialize parameters .  denotes the parameters of the policy function used to choose actions. \n",
    "# # The 1st dimension of  corresponds to the number of features in the state representation,and the 2nd\n",
    "# # dimension corresponds to the number of possible actions (The 2 possible actions: Decline transation if 'Fraud' \n",
    "# # or Allow transaction if 'Non-fraud').\n",
    "# theta = np.random.rand(10, 2)\n",
    "\n",
    "# # Agent training\n",
    "# for k in range(K):\n",
    "    \n",
    "#     # Shuffle the dataset D\n",
    "#     np.random.shuffle(D)\n",
    "   \n",
    "#     print(\"Episode \", k)\n",
    "#     print(\"--------------------------------------------\")\n",
    "    \n",
    "#     # Initialize state to D[0][0], which is the 1st element of the list of tuples D, where each tuple\n",
    "#     # contains a shuffled training sample as its first element and its corresponding label as the 2nd element.\n",
    "#     # Thus, variable 'state' stores the feature values of the first training sample.\n",
    "#     state = D[0][0]\n",
    "    \n",
    "#     for t in range(T):\n",
    "        \n",
    "#         # Choose an action based on the policy  (s,a)\n",
    "#         # Dot product of the state and  is calculated, resulting in a 2-dimensional vector of scores (logits) for each action.\n",
    "#         # The logit is the natural logarithm of the odds that a binary outcome (e.g., 0 or 1) will occur. These scores represent \n",
    "#         # how likely each action is to be chosen given the current state.\n",
    "#         logits = np.dot(state, theta)\n",
    "#         action_probs = np.exp(logits) / np.sum(np.exp(logits))\n",
    "#         action = 1 if action_probs[1] > 0.5 else 0\n",
    "\n",
    "#         # Calculate reward and terminal flag\n",
    "#         true_label = D[t][1]\n",
    "#         reward, terminal = reward_fn(action, true_label, action, lambda_val=0.1)\n",
    "#         print(\"Step:\", t)\n",
    "#         print(\"True label is\", true_label, \". Agent has predicted:\", action)\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"\")\n",
    "        \n",
    "#         # Update state\n",
    "#         state_next = D[t+1][0] if t < T - 1 else state\n",
    "\n",
    "#         # Store transition in replay memory\n",
    "#         replay_memory.append((state, action, reward, state_next, terminal))\n",
    "#         if len(replay_memory) > M:\n",
    "#             replay_memory.pop(0)\n",
    "\n",
    "#         # Sample minibatch from replay memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             minibatch = random.sample(replay_memory, batch_size)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "#         X = np.zeros((batch_size, 10))\n",
    "#         y = np.zeros((batch_size, 2))\n",
    "#         for i in range(batch_size):\n",
    "#             state_i, action_i, reward_i, state_next_i, terminal_i = minibatch[i]\n",
    "#             X[i] = state_i\n",
    "#             y[i] = agent_network.predict(state_i.reshape(1, -1))[0]\n",
    "\n",
    "#             if terminal_i:\n",
    "#                 y[i][action_i] = reward_i\n",
    "#             else:\n",
    "#                 # Choose action based on the policy  (s,a) for the next state\n",
    "#                 logits_next = np.dot(state_next_i, theta)\n",
    "#                 action_probs_next = np.exp(logits_next) / np.sum(np.exp(logits_next))\n",
    "#                 next_action = 1 if action_probs_next[1] > 0.5 else 0\n",
    "#                 y[i][action_i] = reward_i + gamma * y[i][next_action]\n",
    "\n",
    "#         # Train agent network on minibatch using gradient descent\n",
    "#         agent_network.train_on_batch(X, y)\n",
    "      \n",
    "#         # Update state\n",
    "#         state = state_next\n",
    "\n",
    "#         # Check if episode is over\n",
    "#         if terminal:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27c89d",
   "metadata": {},
   "source": [
    "## NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9bc044a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -1.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -1.1\n",
      "\n",
      "Precision: 0.3333333333333333\n",
      "Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.7\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 5\n",
    "max_steps = 5\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.85\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "            if predicted_label == 1:\n",
    "                reward -= 1\n",
    "    return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "39c6e2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 1.0\n",
      "Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 5\n",
    "max_steps = 5\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.99\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "            if predicted_label == 1:\n",
    "                reward -= 1\n",
    "    return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9768f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.5714285714285714\n",
      "Accuracy: 0.7560975609756098\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.75\n",
    "epsilon = 0.3\n",
    "batch_size = 64\n",
    "num_episodes = 10\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.2\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c51d4cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  10\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  11\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  12\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  13\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.4\n",
      "Accuracy: 0.6458333333333334\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.75\n",
    "epsilon = 0.3\n",
    "batch_size = 64\n",
    "num_episodes = 14\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.2\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e789d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  10\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  11\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  12\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  13\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.6\n",
      "Accuracy: 0.7358490566037735\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.55\n",
    "epsilon = 0.3\n",
    "batch_size = 64\n",
    "num_episodes = 14\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.2\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "68565a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.5\n",
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 7\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.87\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62dc68a",
   "metadata": {},
   "source": [
    "## New agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "564f4cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.5\n",
      "Accuracy: 0.6818181818181818\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.3\n",
    "batch_size = 32\n",
    "num_episodes = 7\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.275\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#             if predicted_label == 1:\n",
    "#                 reward -= 1\n",
    "#     return reward, terminal\n",
    "\n",
    "\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "da61b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.8\n",
      "Accuracy: 0.7857142857142857\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.8\n",
    "epsilon = 0.3\n",
    "batch_size = 32\n",
    "num_episodes = 10\n",
    "max_steps = 4\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.0375\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 100000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce743a",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f22b0a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.6\n",
      "Accuracy: 0.5238095238095238\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.8\n",
    "epsilon = 0.7\n",
    "batch_size = 32\n",
    "num_episodes = 10\n",
    "max_steps = 3\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.75\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 2000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5c676d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 4\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 5\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 6\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 4\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "Step: 3\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Precision: 0.6666666666666666\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 10\n",
    "max_steps = 7\n",
    "lambda_val = 0.1\n",
    "learning_rate=0.7\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(X_train_resampled_final, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 2000\n",
    "replay_memory = []\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = Q(state, theta)\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return np.sum(np.square(y - Q_values))\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing Q-values\n",
    "def Q(states, theta):\n",
    "    # Compute Q-values for all states and actions in the batch\n",
    "    Q_values = np.dot(states, theta)\n",
    "    return Q_values\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    # Initialize state\n",
    "    state = D[0][0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "        next_state = D[step+1][0]\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=0)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "            \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fac5f",
   "metadata": {},
   "source": [
    "## New agent + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cc8114e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Iterate over actions in replay_memory and count how many times each action was taken\n",
    "# actions = [memory[1] for memory in replay_memory]\n",
    "# action_counts = np.zeros(10)\n",
    "# for action in actions:\n",
    "#     action_counts[action] += 1\n",
    "\n",
    "# # Plot action distribution\n",
    "# plt.bar(range(10), action_counts)\n",
    "# plt.xlabel('Action')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Action Distribution')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "50528983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to store the success rate obtained by the agent over time\n",
    "# success_rates = []\n",
    "\n",
    "# # Train agent\n",
    "# for k in range(K):\n",
    "#     # ... (code for training the agent)\n",
    "    \n",
    "#     # Calculate success rate for this episode\n",
    "#     num_successes = 0\n",
    "#     for t in range(T):\n",
    "#         if D[t][1] == 1 and agent_network.predict(D[t][0].reshape(1, -1)).argmax() == 1:\n",
    "#             num_successes += 1\n",
    "#         elif D[t][1] == 0 and agent_network.predict(D[t][0].reshape(1, -1)).argmax() == 0:\n",
    "#             num_successes += 1\n",
    "#     success_rate = num_successes / T\n",
    "#     success_rates.append(success_rate)\n",
    "\n",
    "# # Plot the success rate obtained by the agent over time\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(success_rates)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Success Rate')\n",
    "# plt.title('Success Rate over Time')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
