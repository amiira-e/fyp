{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bbe5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b8d8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2625d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample[\"type\"])\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "label = le.fit_transform(df_sample[\"nameDest\"])\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "label = le.fit_transform(df_sample[\"nameOrig\"])\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faee02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.55, random_state=0)\n",
    "\n",
    "# Fit and apply the resampler to the entire dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y_resampled.value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0094790",
   "metadata": {},
   "source": [
    "## Reservoir sampling without replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3833542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "# number of data points to sample from original dataset\n",
    "k= 1500000\n",
    "\n",
    "def reservoir_sampling(iterable, k, header=True):\n",
    "    reservoir = []\n",
    "    for i, item in enumerate(iterable):\n",
    "        if i < k:\n",
    "            reservoir.append(item)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < k:\n",
    "                reservoir[j] = item\n",
    "    return reservoir\n",
    "\n",
    "# Open the input CSV file\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\") as f:\n",
    "    # Check if header line exists\n",
    "    header = True\n",
    "    first_line = f.readline()\n",
    "    if not first_line.startswith('step,type,amount,nameOrig,oldbalanceOrg,newbalanceOrig,nameDest,oldbalanceDest,newbalanceDest,isFraud,isFlaggedFraud'):\n",
    "        header = False\n",
    "        f.seek(0)  # Rewind file pointer to beginning\n",
    "\n",
    "    # Sample from remaining lines\n",
    "    sampled_lines = []\n",
    "    for i, line in enumerate(f):\n",
    "        if i < k:\n",
    "            sampled_lines.append(line)\n",
    "        else:\n",
    "            j = random.randint(0, i)\n",
    "            if j < k:\n",
    "                sampled_lines[j] = line\n",
    "\n",
    "# Open the output CSV file and write the subsample to it\n",
    "with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\", mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    if header:\n",
    "        writer.writerow(first_line.strip().split(','))\n",
    "    for line in sampled_lines:\n",
    "        writer.writerow(line.strip().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa00f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_sample_big['isFraud'].isnull().sum()\n",
    "print(\"Number of missing values in 'isFraud':\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23377b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import random\n",
    "\n",
    "# def reservoir_sampling(iterable, k, header=True):\n",
    "#     reservoir = []\n",
    "#     for i, item in enumerate(iterable):\n",
    "#         if i < k:\n",
    "#             reservoir.append(item)\n",
    "#         else:\n",
    "#             j = random.randint(0, i)\n",
    "#             if j < k:\n",
    "#                 reservoir[j] = item\n",
    "#     return reservoir\n",
    "\n",
    "# # Open the input CSV file\n",
    "# with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\") as f:\n",
    "#     # Check if header line exists\n",
    "#     header = True\n",
    "#     first_line = f.readline()\n",
    "#     if not first_line.startswith('step,type,amount,nameOrig,oldbalanceOrg,newbalanceOrig,nameDest,oldbalanceDest,newbalanceDest,isFraud,isFlaggedFraud'):\n",
    "#         header = False\n",
    "#         f.seek(0)  # Rewind file pointer to beginning\n",
    "\n",
    "#     # Sample from remaining lines\n",
    "#     sampled_lines = reservoir_sampling(f, k=2500000, header=header)\n",
    "\n",
    "# # Open the output CSV file and write the subsample to it\n",
    "# with open(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\", mode='w', newline='') as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     if header:\n",
    "#         writer.writerow(first_line.strip().split(','))\n",
    "#     for line in sampled_lines:\n",
    "#         writer.writerow(line.strip().split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5944eec7",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/generating-expanding-your-datasets-with-synthetic-data-4e27716be218"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329a5aa",
   "metadata": {},
   "source": [
    "## Pre-process larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed453d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\transfer_learning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['type'])\n",
    "label\n",
    "df_sample_big.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample_big[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameDest'])\n",
    "label\n",
    "df_sample_big.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample_big['nameOrig'])\n",
    "label\n",
    "df_sample_big.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample_big[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f7a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "# #Downsample via RandomUnderSampler\n",
    "# rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "# #Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "# X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample_big.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample_big['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, stratify=y_resampled, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca43564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd34545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da522fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_big = df_sample_big.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c1c38e",
   "metadata": {},
   "source": [
    "### Big dataset: Pre-train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384fea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.regularizers import l1\n",
    "# from keras import regularizers\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "# from sklearn.model_selection import KFold\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "# from tensorflow import keras\n",
    "\n",
    "\n",
    "# # define number of folds for cross-validation\n",
    "# num_folds = 2\n",
    "\n",
    "# # create KFold cross-validation object\n",
    "# kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# # create arrays to store training and validation loss for each epoch\n",
    "# train_losses = np.zeros((num_folds, 5))\n",
    "# val_losses = np.zeros((num_folds, 5))\n",
    "\n",
    "# import time\n",
    "\n",
    "# # loop over the folds\n",
    "# fold_no = 1\n",
    "# for train, val in kfold.split(X_train_resampled_final, y_train_resampled_final):\n",
    "    \n",
    "#     # To add window\n",
    "#     # pre_trained_model.add(Conv1D(filters=32, (3,3)), input_shape=)\n",
    "    \n",
    "#     # Can add Activation layer before Pooling layer\n",
    "#     # pre_trained_model.add(Activation(\"relu\"))\n",
    "    \n",
    "#     # create model\n",
    "#     pre_trained_model = Sequential()\n",
    "#     # add convolutional layer\n",
    "#     # pre_trained_model.add(Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=(10, 1)))\n",
    "#     pre_trained_model.add(Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=(10, 1)))\n",
    "#     # add pooling layer\n",
    "#     pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "#     # flatten output to feed into fully connected layer\n",
    "#     pre_trained_model.add(Flatten()) \n",
    "#     # add fully connected layer\n",
    "#     pre_trained_model.add(Dense(20, activation='relu', kernel_regularizer=l1(0.0001)))\n",
    "#     # add dropout layer\n",
    "#     layer = Dropout(0.1)\n",
    "#     pre_trained_model.add(layer)\n",
    "#     # add output layer\n",
    "#     pre_trained_model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     # opt_new = keras.optimizers.RMSprop(learning_rate=0.000871)\n",
    "#     opt_new = keras.optimizers.Adam(learning_rate=0.000871)\n",
    "\n",
    "#     # compile model\n",
    "#     pre_trained_model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "    \n",
    "#     # record start time\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # train model for each fold\n",
    "#     history = pre_trained_model.fit(X_train_resampled_final[train], y_train_resampled_final[train],\n",
    "#                                      epochs=5, batch_size=32, validation_data=(X_train_resampled_final[val], y_train_resampled_final[val]))\n",
    "    \n",
    "#     # record end time and calculate elapsed time\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "    \n",
    "#     print(f'Fold {fold_no} elapsed time: {elapsed_time:.2f} seconds')\n",
    "    \n",
    "#     # store training and validation loss for each epoch\n",
    "#     train_losses[fold_no-1] = history.history['loss']\n",
    "#     val_losses[fold_no-1] = history.history['val_loss']\n",
    "    \n",
    "#     # increment fold number\n",
    "#     fold_no += 1\n",
    "    \n",
    "# # calculate mean training and validation loss across all folds for each epoch\n",
    "# mean_train_loss = np.mean(train_losses, axis=0)\n",
    "# mean_val_loss = np.mean(val_losses, axis=0)\n",
    "\n",
    "# # plot training and validation loss curves for each epoch\n",
    "# plt.plot(mean_train_loss, label='Training Loss')\n",
    "# plt.plot(mean_val_loss, label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4501b",
   "metadata": {},
   "source": [
    "## Another CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# define number of folds for cross-validation\n",
    "num_folds = 2\n",
    "\n",
    "# create KFold cross-validation object\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# create arrays to store training and validation loss for each epoch\n",
    "train_losses = np.zeros((num_folds, 4))\n",
    "val_losses = np.zeros((num_folds, 4))\n",
    "\n",
    "# loop over the folds\n",
    "fold_no = 1\n",
    "for train, val in kfold.split(X_train_resampled_final, y_train_resampled_final): \n",
    "\n",
    "    # create model\n",
    "    pre_trained_model = Sequential()\n",
    "    # add convolutional layer\n",
    "    pre_trained_model.add(Conv1D(filters=16, kernel_size=2, input_shape=(10, 1)))\n",
    "    # add batch normalization layer\n",
    "    pre_trained_model.add(BatchNormalization())\n",
    "    # add activation function\n",
    "    pre_trained_model.add(Conv1D(8, 2, activation='tanh'))\n",
    "    pre_trained_model.add(BatchNormalization())\n",
    "    # add pooling layer\n",
    "    pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "    # flatten output to feed into fully connected layer\n",
    "    pre_trained_model.add(Flatten()) \n",
    "    # add fully connected layer\n",
    "    pre_trained_model.add(Dense(5, activation='tanh', kernel_regularizer=l1(0.0001714)))\n",
    "    # add dropout layer\n",
    "    layer = Dropout(0.1)\n",
    "    pre_trained_model.add(layer)\n",
    "    # add output layer\n",
    "    pre_trained_model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt_new = keras.optimizers.RMSprop(learning_rate=0.000871)\n",
    "    # opt_new = keras.optimizers.Adam(learning_rate=0.0171)\n",
    "\n",
    "    # compile model\n",
    "    pre_trained_model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "    \n",
    "    # record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "    # train model for each fold\n",
    "    history = pre_trained_model.fit(X_train_resampled_final[train], y_train_resampled_final[train],\n",
    "                                     epochs=4, batch_size=32, validation_data=(X_train_resampled_final[val], y_train_resampled_final[val]))\n",
    "    \n",
    "    # record end time and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    print(f'Fold {fold_no} elapsed time: {elapsed_time:.2f} seconds')\n",
    "    \n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # make predictions on validation set\n",
    "    y_pred = pre_trained_model.predict(X_train_resampled_final[val])\n",
    "\n",
    "    # convert predictions to binary labels\n",
    "    y_pred = np.round(y_pred)\n",
    "\n",
    "    # print classification report\n",
    "    print(classification_report(y_train_resampled_final[val], y_pred))\n",
    "    \n",
    "    # store training and validation loss for each epoch\n",
    "    train_losses[fold_no-1] = history.history['loss']\n",
    "    val_losses[fold_no-1] = history.history['val_loss']\n",
    "    \n",
    "    # increment fold number\n",
    "    fold_no += 1\n",
    "    \n",
    "# calculate mean training and validation loss across all folds for each epoch\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "\n",
    "# plot training and validation loss curves for each epoch\n",
    "plt.plot(mean_train_loss, label='Training Loss')\n",
    "plt.plot(mean_val_loss, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff10939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model.save('pre_trained_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f73a8",
   "metadata": {},
   "source": [
    "## Go back to smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df5fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac218053",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90c151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.55, random_state=0)\n",
    "\n",
    "# Fit and apply the resampler to the entire dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036edbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = df_sample.drop('isFraud', axis=1)\n",
    "# # Separate the target variable\n",
    "# y = df_sample['isFraud']\n",
    "\n",
    "# # Print class distribution before split\n",
    "# print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled,  y_resampled, test_size=0.1, stratify= y_resampled, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39800044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# #Upsampling via SMOTE\n",
    "# smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "# #Downsample via RandomUnderSampler\n",
    "# rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "# #Application of the resampling methods\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "# X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbaa832",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3ef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bdbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29beb78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "     X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "   # X_train_resampled_final[:, i] = replace_outliers_with_mad(X_train_resampled_final[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88603bde",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a7ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "# # Define your LSTM model architecture\n",
    "# def create_model(activation='relu', optimizer='adam'):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(15, activation=activation, input_shape=(10,1)))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(loss='mse', optimizer=optimizer)\n",
    "#     return model\n",
    "\n",
    "# # Wrap the Keras model with scikit-learn's KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "# # Define the hyperparameters and their possible values to search over\n",
    "# param_grid = {'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "#               'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "#               'epochs': [10,20,30],\n",
    "#               'batch_size': [16, 32, 64]}\n",
    "\n",
    "# # Use RandomizedSearchCV to search for the best hyperparameters\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=4, cv=3,verbose=1)\n",
    "# random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "\n",
    "# # Print the best hyperparameters and corresponding mean score\n",
    "# print('Best hyperparameters:', random_search.best_params_)\n",
    "# print('Best score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b45cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM\n",
    "# from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "# # Define your LSTM model architecture\n",
    "# def create_model(units=15, activation='relu', optimizer='adam', lr=0.001):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(units, activation=activation, input_shape=(10,1)))\n",
    "#     model.add(Dense(1))\n",
    "#     optimizer = get_optimizer(optimizer, lr)\n",
    "#     model.compile(loss='mse', optimizer=optimizer)\n",
    "#     return model\n",
    "\n",
    "# # Define a function to get the optimizer based on its name and learning rate\n",
    "# def get_optimizer(name, lr):\n",
    "#     if name == 'adam':\n",
    "#         return Adam(lr=lr)\n",
    "#     elif name == 'rmsprop':\n",
    "#         return RMSprop(lr=lr)\n",
    "#     elif name == 'sgd':\n",
    "#         return SGD(lr=lr)\n",
    "\n",
    "# # Wrap the Keras model with scikit-learn's KerasRegressor\n",
    "# model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "# # Define the hyperparameters and their possible values to search over\n",
    "# param_grid = {'units': [10, 20, 30],\n",
    "#               'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "#               'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "#               'lr': [0.001, 0.01, 0.1],\n",
    "#               'epochs': [10, 20, 30],\n",
    "#               'batch_size': [16, 32, 64]}\n",
    "\n",
    "# # Use RandomizedSearchCV to search for the best hyperparameters\n",
    "# random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=2, cv=2)\n",
    "# random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "\n",
    "# # Print the best hyperparameters and corresponding mean score\n",
    "# print('Best hyperparameters:', random_search.best_params_)\n",
    "# print('Best score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eabecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "\n",
    "# Define your LSTM model architecture\n",
    "def create_model(units=15, activation='relu', optimizer='adam', lr=0.001, dropout=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, activation=activation, input_shape=(10,1)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = get_optimizer(optimizer, lr)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Define a function to get the optimizer based on its name and learning rate\n",
    "def get_optimizer(name, lr):\n",
    "    if name == 'adam':\n",
    "        return Adam(lr=lr)\n",
    "    elif name == 'rmsprop':\n",
    "        return RMSprop(lr=lr)\n",
    "    elif name == 'sgd':\n",
    "        return SGD(lr=lr)\n",
    "\n",
    "# Wrap the Keras model with scikit-learn's KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Define the hyperparameters and their possible values to search over\n",
    "param_grid = {'units': [10, 20, 30],\n",
    "              'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "              'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "              'lr': [0.001, 0.01, 0.1],\n",
    "              'dropout': [0.0, 0.1, 0.3],\n",
    "              'epochs': [10, 20, 30],\n",
    "              'batch_size': [16, 32, 64]}\n",
    "\n",
    "# Use RandomizedSearchCV to search for the best hyperparameters\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=1, cv=2)\n",
    "random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "\n",
    "# Print the best hyperparameters and corresponding mean score\n",
    "print('Best hyperparameters:', random_search.best_params_)\n",
    "print('Best score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd58ee6",
   "metadata": {},
   "source": [
    "## GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200157d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Definition of the LSTM's architecture\n",
    "def create_model(units=15, activation='relu', optimizer='adam', lr=0.001, dropout=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units, activation=activation, input_shape=(10,1)))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    optimizer = get_optimizer(optimizer, lr)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Function to get the optimizer's name and the associated learning rate\n",
    "def get_optimizer(name, lr):\n",
    "    if name == 'adam':\n",
    "        return Adam(lr=lr)\n",
    "    elif name == 'rmsprop':\n",
    "        return RMSprop(lr=lr)\n",
    "    elif name == 'sgd':\n",
    "        return SGD(lr=lr)\n",
    "\n",
    "# Wrap the Keras model with KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Definition of hyperparameters' search space\n",
    "param_grid = {'units': [10, 20, 30],\n",
    "              'activation': ['relu', 'tanh', 'sigmoid'],\n",
    "              'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
    "              'lr': [0.001, 0.01, 0.1],\n",
    "              'dropout': [0.0, 0.1, 0.3],\n",
    "              'epochs': [10, 20, 30],\n",
    "              'batch_size': [16, 32, 64]}\n",
    "\n",
    "# Using RandomizedSearchCV to search for the best hyperparameters\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=1, cv=2)\n",
    "random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "\n",
    "# Printing the best hyperparameters and corresponding mean score\n",
    "print('Best hyperparameters:', random_search.best_params_)\n",
    "print('Best score:', random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# encoding_dim = 10\n",
    "# num_hidden = 10\n",
    "# num_features = 10\n",
    "# num_filters = 16\n",
    "# num_units = 32\n",
    "\n",
    "# # Define the autoencoder to encode each feature\n",
    "# input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "# encoder = tf.keras.layers.Dense(units=10, activation='relu', input_shape=input_shape)\n",
    "# first_hidden_layer = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "# decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "# autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# # Load the pre-trained CNN\n",
    "# CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# # Freeze all layers in CNN\n",
    "# CNN.trainable = False\n",
    "\n",
    "# V = np.zeros((num_features, num_hidden))\n",
    "# # V =np.zeros ((32, 10, 10))\n",
    "\n",
    "# for i in range(num_features):\n",
    "#     #V[i] = encoder.predict(X_train_resampled_final[i:i+1])[0]\n",
    "#     V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "#     print(\"value:\", V[i])\n",
    "# # V = np.expand_dims(V, axis=-1)  # Add a new dimension to V\n",
    "# C = CNN(V)\n",
    "# C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "# C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "# print(\"C:\" , C.shape) # C: (10, 1)\n",
    "\n",
    "# # Define the LSTM layer with the appropriate input shape\n",
    "# LSTM = tf.keras.layers.LSTM(units=num_units, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, num_hidden))\n",
    "\n",
    "# # Reshape V to have shape (num_features, 1, num_hidden) to add the timesteps dimension\n",
    "# V = np.reshape(V, (num_features, 1, num_hidden))\n",
    "\n",
    "# # Pass the vector representations through the LSTM layer\n",
    "# O = LSTM(V)\n",
    "\n",
    "# # Define the final classification layer\n",
    "# FL = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# # Connect it to the output of the LSTM layer\n",
    "# O = FL(O)\n",
    "\n",
    "\n",
    "# # # Define the model and compile it\n",
    "# # model = tf.keras.models.Model(inputs=encoder.input, outputs=O)\n",
    "# # model.build((None, num_features, num_hidden))\n",
    "# # model.compile(optimizer=optimizer, loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# encoding_dim = 10\n",
    "# num_hidden = 10\n",
    "# num_features = 10\n",
    "# num_filters = 16\n",
    "# num_units = 32\n",
    "\n",
    "# # Define the autoencoder to encode each feature\n",
    "# input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "# encoder = tf.keras.layers.Dense(units=10, activation='relu', input_shape=input_shape)\n",
    "# first_hidden_layer = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "# decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "# autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# # Load the pre-trained CNN\n",
    "# CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# # Freeze all layers in CNN\n",
    "# CNN.trainable = False\n",
    "\n",
    "# V = np.zeros((num_features, num_hidden))\n",
    "# # V =np.zeros ((32, 10, 10))\n",
    "\n",
    "# for i in range(num_features):\n",
    "#     #V[i] = encoder.predict(X_train_resampled_final[i:i+1])[0]\n",
    "#     V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "\n",
    "# C = CNN(V)\n",
    "# C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "# C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "\n",
    "# # Define the LSTM layer with the appropriate input shape\n",
    "# LSTM = tf.keras.layers.LSTM(units=num_units, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, num_hidden))\n",
    "\n",
    "# # Reshape V to have shape (num_features, 1, num_hidden) to add the timesteps dimension\n",
    "# V = np.reshape(V, (num_features, 1, num_hidden))\n",
    "\n",
    "# # Pass the vector representations through the LSTM layer\n",
    "# O = LSTM(V)\n",
    "\n",
    "# # Define the final classification layer\n",
    "# FL = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# # Connect it to the output of the LSTM layer\n",
    "# O = FL(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fb9ae",
   "metadata": {},
   "source": [
    "## Final model is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afac096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "encoding_dim = 10 # number of neurons in hidden layer\n",
    "num_features = 10 # number of input variables\n",
    "num_units= 32\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "encoder = tf.keras.layers.Dense(units=10, activation='relu', input_shape=input_shape)\n",
    "first_hidden_layer = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# Freeze all trainable layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "# The reason why V takes two arguments is because it represents a matrix where each row corresponds to an input feature \n",
    "# and each column corresponds to a hidden layer neuron. In other words,V is a matrix that will store the encoded representations \n",
    "# of each input feature in the hidden layer of the autoencoder.\n",
    "V = np.zeros((num_features, encoding_dim))\n",
    "\n",
    "# 1st for loop: Encoding\n",
    "for i in range(num_features):\n",
    "    V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "\n",
    "# CNN model is applied to all the encoded features in a single step, by passing the matrix V \n",
    "# with shape (num_features, encoding_dim) to the CNN model. This is achieved by calling\n",
    "# the CNN function on V, which applies the CNN model to all the encoded features in V at once.\n",
    "C = CNN(V)\n",
    "C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "\n",
    "# # Define the LSTM layer with the appropriate input shape\n",
    "# # (timestep, feature_dim): Input shape of LSTM\n",
    "# LSTM = tf.keras.layers.LSTM(units=num_units, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, encoding_dim))\n",
    "# LSTM = tf.keras.layers.LSTM(units=num_units, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, encoding_dim))\n",
    "\n",
    "# # Reshape V to have shape (num_features, 1, encoding_dim) to add the timesteps dimension\n",
    "# V = np.reshape(V, (num_features, 1, encoding_dim))\n",
    "\n",
    "# # Pass the vector representations through the LSTM layer\n",
    "# O = LSTM(V)\n",
    "\n",
    "# # Define the final classification layer\n",
    "# FL = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# # Connect it to the output of the LSTM layer\n",
    "# # Based on the values of the probabilities, these values (O-values) are the output of a binary classification model. Since the probabilities are all between 0.49 and 0.50, this suggests that the model is uncertain about the classification of the \n",
    "# # input data, and the probabilities are close to the threshold of 0.5 for deciding between the two classes.\n",
    "# O = FL(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1277a8",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c432d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "encoding_dim = 10 # number of neurons in hidden layer\n",
    "num_features = 10 # number of input variables\n",
    "num_units= 32\n",
    "num_hidden=10\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "encoder = tf.keras.layers.Dense(units=10, activation='relu', input_shape=input_shape)\n",
    "first_hidden_layer = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# Freeze all trainable layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "# The reason why V takes two arguments is because it represents a matrix where each row corresponds to an input feature \n",
    "# and each column corresponds to a hidden layer neuron. In other words,V is a matrix that will store the encoded representations \n",
    "# of each input feature in the hidden layer of the autoencoder.\n",
    "V = np.zeros((num_features, encoding_dim))\n",
    "\n",
    "# 1st for loop: Encoding\n",
    "for i in range(num_features):\n",
    "    V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "\n",
    "# CNN model is applied to all the encoded features in a single step, by passing the matrix V \n",
    "# with shape (num_features, encoding_dim) to the CNN model. This is achieved by calling\n",
    "# the CNN function on V, which applies the CNN model to all the encoded features in V at once.\n",
    "C = CNN(V)\n",
    "C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "\n",
    "# Extract convolutional,batch normalization, maxpooling, and flatten layers from pre-trained model\n",
    "conv_layer = pre_trained_model.layers[0]\n",
    "batch_layer = pre_trained_model.layers[1]\n",
    "conv2_layer = pre_trained_model.layers[2]\n",
    "batch2_layer = pre_trained_model.layers[3]\n",
    "maxpooling_layer = pre_trained_model.layers[4]\n",
    "flatten_layer = pre_trained_model.layers[5]\n",
    "\n",
    "# reshape output to feed into LSTM layers\n",
    "# pre_trained_model.add(Reshape((1, -1)))\n",
    "\n",
    "# Define the LSTM layers\n",
    "LSTM1 = tf.keras.layers.LSTM(units=64, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, encoding_dim))\n",
    "\n",
    "# Add the LSTM layers to the pre-trained model\n",
    "pre_trained_model.add(LSTM1)\n",
    "\n",
    "# Reshape V to have shape (num_features, 1, num_hidden) to add the timesteps dimension\n",
    "V = np.reshape(V, (num_features, 1, num_hidden))\n",
    "\n",
    "# Pass the vector representations through the LSTM layer\n",
    "O = LSTM1(V)\n",
    "\n",
    "# add fully connected layer\n",
    "pre_trained_model.add(Dense(20, activation='tanh'))\n",
    "\n",
    "# add dropout layer\n",
    "layer = Dropout(0.1)\n",
    "pre_trained_model.add(layer)\n",
    "\n",
    "# add output layer\n",
    "\n",
    "# Define the final classification layer\n",
    "FL = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# Connect it to the output of the LSTM layer\n",
    "# Based on the values of the probabilities, these values (O-values) are the output of a binary classification model. Since the probabilities are all between 0.49 and 0.50, this suggests that the model is uncertain about the classification of the \n",
    "# input data, and the probabilities are close to the threshold of 0.5 for deciding between the two classes.\n",
    "O = FL(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bef73",
   "metadata": {},
   "source": [
    "## Working new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "encoding_dim = 10 # number of neurons in hidden layer (TO REVIEW)\n",
    "num_features = 10 # number of input variables\n",
    "num_units= 32\n",
    "num_hidden=10\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "encoder = tf.keras.layers.Dense(units=10, activation='relu', input_shape=input_shape)\n",
    "first_hidden_layer = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# Freeze all trainable layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "# The reason why V takes two arguments is because it represents a matrix where each row corresponds to an input feature \n",
    "# and each column corresponds to a hidden layer neuron. In other words,V is a matrix that will store the encoded representations \n",
    "# of each input feature in the hidden layer of the autoencoder.\n",
    "V = np.zeros((num_features, encoding_dim))\n",
    "\n",
    "# 1st for loop: Encoding\n",
    "for i in range(num_features):\n",
    "    V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "\n",
    "# CNN model is applied to all the encoded features in a single step, by passing the matrix V \n",
    "# with shape (num_features, encoding_dim) to the CNN model. This is achieved by calling\n",
    "# the CNN model on V, which applies the CNN model to all the encoded features in V at once.\n",
    "C = CNN(V)\n",
    "C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "\n",
    "# Extract convolutional,batch normalization, maxpooling, and flatten layers from pre-trained model\n",
    "conv_layer = pre_trained_model.layers[0]\n",
    "batch_layer = pre_trained_model.layers[1]\n",
    "conv2_layer = pre_trained_model.layers[2]\n",
    "batch2_layer = pre_trained_model.layers[3]\n",
    "maxpooling_layer = pre_trained_model.layers[4]\n",
    "flatten_layer = pre_trained_model.layers[5]\n",
    "\n",
    "# reshape output to feed into LSTM layers\n",
    "# pre_trained_model.add(Reshape((1, -1)))\n",
    "\n",
    "# Define the LSTM layers\n",
    "LSTM1 = tf.keras.layers.LSTM(units=64, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, encoding_dim))\n",
    "\n",
    "# Add the LSTM layers to the model\n",
    "LSTM_model = tf.keras.Sequential()\n",
    "LSTM_model.add(LSTM1)\n",
    "\n",
    "# Add fully connected layer\n",
    "LSTM_model.add(Dense(20, activation='tanh'))\n",
    "\n",
    "# Add dropout layer\n",
    "LSTM_model.add(Dropout(0.1))\n",
    "\n",
    "# Add output layer\n",
    "LSTM_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "LSTM_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Reshape V to have shape (num_features, 1, encoding_dim) to add the timesteps dimension\n",
    "# V = np.reshape(V, (num_features, 1, encoding_dim))\n",
    "\n",
    "X_train_reshaped = np.reshape(X_train_resampled_final, (-1, 1, num_features))\n",
    "# assuming X_train_resampled_final has shape (num_samples, num_features, encoding_dim)\n",
    "# X_train_resampled_reshaped = tf.expand_dims(X_train_resampled_final, axis=1)\n",
    "\n",
    "# Train the model\n",
    "LSTM_model.fit(X_train_reshaped, y_train_resampled_final, epochs=10, batch_size=32)\n",
    "\n",
    "# Unfreeze the CNN\n",
    "CNN.trainable = True\n",
    "\n",
    "# Train the CNN-LSTM model\n",
    "# model_CNN_LSTM = tf.keras.Sequential()\n",
    "# model_CNN_LSTM.add(CNN)\n",
    "# model_CNN_LSTM.add(LSTM_model)\n",
    "# model_CNN_LSTM.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model_CNN_LSTM.fit(X_train_resampled_final, y_train_resampled_final, epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ca6c8",
   "metadata": {},
   "source": [
    "## Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf00dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_LSTM = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b058ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c5fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12356030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_resampled_final, y_train_resampled_final, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1f72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(autoencoder, to_file='a_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(pre_trained_model, to_file='a_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5fa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(LSTM_model, to_file='a_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a33111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(CNN_LSTM, to_file='a_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6c579",
   "metadata": {},
   "source": [
    "## Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03229603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "encoding_dim = 6\n",
    "num_features = 10 # number of input variables\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "encoder = tf.keras.layers.Dense(units=10, activation='elu', input_shape=input_shape)\n",
    "first_hidden_layer = tf.keras.layers.Dense(units=encoding_dim, activation='elu')\n",
    "decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# Freeze all trainable layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "V = np.zeros((num_features, encoding_dim))\n",
    "\n",
    "# 1st for loop: Encoding\n",
    "for i in range(num_features):\n",
    "    V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "\n",
    "V = np.reshape(V, (-1, num_features, 1))\n",
    "\n",
    "C = CNN(V)\n",
    "C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "\n",
    "# Extract convolutional,batch normalization, maxpooling, and flatten layers from pre-trained model\n",
    "conv_layer = pre_trained_model.layers[0]\n",
    "batch_layer = pre_trained_model.layers[1]\n",
    "conv2_layer = pre_trained_model.layers[2]\n",
    "batch2_layer = pre_trained_model.layers[3]\n",
    "maxpooling_layer = pre_trained_model.layers[4]\n",
    "flatten_layer = pre_trained_model.layers[5]\n",
    "\n",
    "# Define the LSTM layers\n",
    "LSTM1 = tf.keras.layers.LSTM(units=64, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, num_features))\n",
    "\n",
    "# Add the LSTM layers to the model\n",
    "LSTM_model = tf.keras.Sequential()\n",
    "LSTM_model.add(LSTM1)\n",
    "\n",
    "# # Add fully connected layer\n",
    "# LSTM_model.add(Dense(20, activation='tanh'))\n",
    "\n",
    "# # Add dropout layer\n",
    "# LSTM_model.add(Dropout(0.1))\n",
    "\n",
    "# # Add output layer\n",
    "# LSTM_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Add a fully connected layer\n",
    "LSTM_model.add(tf.keras.layers.Dense(units=20, activation='tanh', kernel_regularizer=l1(0.00114)))\n",
    "\n",
    "# Add a dropout layer\n",
    "LSTM_model.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "# Add the output layer\n",
    "LSTM_model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.0015\n",
    "\n",
    "# Create the Adam optimizer with the defined learning rate\n",
    "opt_LSTM = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "LSTM_model.compile(optimizer=opt_LSTM, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train_reshaped = np.reshape(X_train_resampled_final, (-1, 1, num_features))\n",
    "\n",
    "# Train the model\n",
    "LSTM_model.fit(X_train_reshaped, y_train_resampled_final, epochs=8, batch_size=32)\n",
    "\n",
    "# Unfreeze the CNN\n",
    "CNN.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09214b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (10, 1)\n",
    "\n",
    "# Define CNN layers\n",
    "# conv_layer = Conv1D(filters=32, kernel_size=3, activation='tanh', input_shape=input_shape)\n",
    "# batch_layer = BatchNormalization()\n",
    "# conv2_layer = Conv1D(filters=16, kernel_size=3, activation='tanh')\n",
    "# batch2_layer = BatchNormalization()\n",
    "# maxpooling_layer = MaxPooling1D(pool_size=2)\n",
    "# flatten_layer = Flatten()\n",
    "\n",
    "conv_layer = pre_trained_model.layers[0]\n",
    "batch_layer = pre_trained_model.layers[1]\n",
    "conv2_layer = pre_trained_model.layers[2]\n",
    "batch2_layer = pre_trained_model.layers[3]\n",
    "maxpooling_layer = pre_trained_model.layers[4]\n",
    "flatten_layer = pre_trained_model.layers[5]\n",
    "\n",
    "# Define LSTM layers\n",
    "LSTM_layer = LSTM(units=64, use_bias=True, kernel_initializer=\"glorot_uniform\")\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Pass input through CNN layers\n",
    "x = conv_layer(input_layer)\n",
    "x = batch_layer(x)\n",
    "x = conv2_layer(x)\n",
    "x = batch2_layer(x)\n",
    "x = maxpooling_layer(x)\n",
    "x = flatten_layer(x)\n",
    "\n",
    "# Reshape output of CNN to feed into LSTM layers\n",
    "x = Reshape((1, -1))(x)\n",
    "\n",
    "# Pass output of CNN through LSTM layers\n",
    "x = LSTM_layer(x)\n",
    "\n",
    "# Add fully connected layer\n",
    "x = Dense(20, activation='tanh')(x)\n",
    "\n",
    "# Add dropout layer\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Add output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fbdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (10, 1)\n",
    "\n",
    "conv_layer = pre_trained_model.layers[0]\n",
    "batch_layer = pre_trained_model.layers[1]\n",
    "conv2_layer = pre_trained_model.layers[2]\n",
    "batch2_layer = pre_trained_model.layers[3]\n",
    "maxpooling_layer = pre_trained_model.layers[4]\n",
    "flatten_layer = pre_trained_model.layers[5]\n",
    "\n",
    "# Define the LSTM layers\n",
    "LSTM1 = tf.keras.layers.LSTM(units=64, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, num_features))\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Pass input through CNN layers\n",
    "x = conv_layer(input_layer)\n",
    "x = batch_layer(x)\n",
    "x = conv2_layer(x)\n",
    "x = batch2_layer(x)\n",
    "x = maxpooling_layer(x)\n",
    "x = flatten_layer(x)\n",
    "\n",
    "# Reshape output of CNN to feed into the LSTM\n",
    "x = Reshape((1, -1))(x)\n",
    "\n",
    "# Pass output of CNN through LSTM layer\n",
    "x = LSTM1(x)\n",
    "\n",
    "# Add fully connected layer\n",
    "x = Dense(20, activation='tanh')(x)\n",
    "\n",
    "# Add dropout layer\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Add output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed01e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract convolutional,batch normalization, maxpooling, and flatten layers from pre-trained model\n",
    "# conv_layer = pre_trained_model.layers[0]\n",
    "# batch_layer = pre_trained_model.layers[1]\n",
    "# conv2_layer = pre_trained_model.layers[2]\n",
    "# batch2_layer = pre_trained_model.layers[3]\n",
    "# maxpooling_layer = pre_trained_model.layers[4]\n",
    "# flatten_layer = pre_trained_model.layers[5]\n",
    "\n",
    "# # reshape output to be in 3D shape (batch_size, timesteps, input_dim)\n",
    "# pre_trained_model.add(Reshape((-1,1)))\n",
    "\n",
    "# # add LSTM layer\n",
    "# num_units = 32\n",
    "# pre_trained_model.add(LSTM(units=num_units, use_bias=True, kernel_initializer=\"glorot_uniform\"))\n",
    "\n",
    "# # add dropout layer\n",
    "# pre_trained_model.add(Dropout(0.2))\n",
    "\n",
    "# # add fully connected layer\n",
    "# pre_trained_model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# # add output layer\n",
    "# pre_trained_model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# # compile model\n",
    "# pre_trained_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "# # create model\n",
    "# pre_trained_model = Sequential()\n",
    "# # add convolutional layer\n",
    "# pre_trained_model.add(Conv1D(filters=16, kernel_size=2, input_shape=(10, 1)))\n",
    "# # add batch normalization layer\n",
    "# pre_trained_model.add(BatchNormalization())\n",
    "# # add activation function\n",
    "# pre_trained_model.add(Conv1D(8, 2, activation='tanh'))\n",
    "# pre_trained_model.add(BatchNormalization())\n",
    "# # add pooling layer\n",
    "# pre_trained_model.add(MaxPooling1D(pool_size=2))\n",
    "# # flatten output to feed into fully connected layer\n",
    "# pre_trained_model.add(Flatten()) \n",
    "\n",
    "# # reshape output to be in 3D shape (batch_size, timesteps, input_dim)\n",
    "# pre_trained_model.add(Reshape((-1,1)))\n",
    "\n",
    "# # add LSTM layer\n",
    "# num_units = 32\n",
    "# pre_trained_model.add(LSTM(units=num_units, use_bias=True, kernel_initializer=\"glorot_uniform\"))\n",
    "\n",
    "# # add dropout layer\n",
    "# pre_trained_model.add(Dropout(0.2))\n",
    "\n",
    "# # add fully connected layer\n",
    "# pre_trained_model.add(Dense(units=16, activation='relu'))\n",
    "\n",
    "# # add output layer\n",
    "# pre_trained_model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# # compile model\n",
    "# pre_trained_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32062928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "opt_new = keras.optimizers.Adam(learning_rate=0.000271)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt_new, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_resampled_final, y_train_resampled_final, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Generate predicted probabilities for the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Set the threshold for converting probabilities to class labels\n",
    "threshold = 0.71\n",
    "\n",
    "# Convert predicted probabilities to class labels using the threshold\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# # Convert the predictions to binary values\n",
    "# y_pred_binary = np.round(y_pred)\n",
    "\n",
    "# Calculate the f1 score, precision, and recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('Test Loss:', loss)\n",
    "# print('Test Accuracy:', accuracy)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Generate predicted probabilities for the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Set the threshold for converting probabilities to class labels\n",
    "threshold = 0.25\n",
    "\n",
    "# Convert predicted probabilities to class labels using the threshold\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# # Convert the predictions to binary values\n",
    "# y_pred_binary = np.round(y_pred)\n",
    "\n",
    "# Calculate the f1 score, precision, and recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('Test Loss:', loss)\n",
    "# print('Test Accuracy:', accuracy)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Generate predicted probabilities for the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Set the threshold for converting probabilities to class labels\n",
    "threshold = 0.49\n",
    "\n",
    "# Convert predicted probabilities to class labels using the threshold\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# # Convert the predictions to binary values\n",
    "# y_pred_binary = np.round(y_pred)\n",
    "\n",
    "# Calculate the f1 score, precision, and recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('Test Loss:', loss)\n",
    "# print('Test Accuracy:', accuracy)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9ee8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y_test.value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0640c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Generate predicted probabilities for the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Set the threshold for converting probabilities to class labels\n",
    "threshold = 0.25\n",
    "\n",
    "# Convert predicted probabilities to class labels using the threshold\n",
    "y_pred = (y_pred_prob > threshold).astype(int)\n",
    "\n",
    "# # Convert the predictions to binary values\n",
    "# y_pred_binary = np.round(y_pred)\n",
    "\n",
    "# Calculate the f1 score, precision, and recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('Test Loss:', loss)\n",
    "# print('Test Accuracy:', accuracy)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072bd87",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d811cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate predicted probabilities for the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a9cc78",
   "metadata": {},
   "source": [
    "## True Vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c850950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = (y_pred > 0.25).astype(int)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# assuming y_true and y_pred are your true and predicted labels respectively\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# assuming y_test and y_pred are your true and predicted labels respectively\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set(xticks=np.arange(conf_mat.shape[1]),\n",
    "       yticks=np.arange(conf_mat.shape[0]),\n",
    "       xticklabels=class_names, yticklabels=class_names,\n",
    "       ylabel='True label',\n",
    "       xlabel='Predicted label')\n",
    "\n",
    "# Rotate the tick labels and set their alignment\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations\n",
    "for i in range(conf_mat.shape[0]):\n",
    "    for j in range(conf_mat.shape[1]):\n",
    "        ax.text(j, i, format(conf_mat[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if conf_mat[i, j] > conf_mat.max() / 2. else \"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfd5418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# assuming y_test and y_pred are your true and predicted labels respectively\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat,\n",
    "                              display_labels=class_names)\n",
    "\n",
    "# Set the color map to be used in the display\n",
    "# You can customize this to your preference\n",
    "cmap = plt.cm.Blues\n",
    "disp = disp.plot(cmap=cmap)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# assuming y_test and y_pred are your true and predicted labels respectively\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "class_names = np.unique(y_test)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat,\n",
    "                              display_labels=class_names)\n",
    "\n",
    "# Set the color map to be used in the display\n",
    "# You can customize this to your preference\n",
    "cmap = plt.cm.Blues\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp = disp.plot(cmap=cmap, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f1e4d",
   "metadata": {},
   "source": [
    "## Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b661b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Generate model predictions for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate precision and recall for each probability threshold\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "# Calculate area under the precision-recall curve\n",
    "auc_pr = auc(recall, precision)\n",
    "\n",
    "print(\"AUC-PR:\", auc_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182805f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate predicted probabilities for the test set\n",
    "y_proba = model.predict(X_test)\n",
    "\n",
    "# Extract probabilities of the positive class\n",
    "y_proba = y_proba[:, 0]\n",
    "\n",
    "# Calculate precision and recall for different probability thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be59889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "lr_probs = model.predict(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 0]\n",
    "# predict class labels using a threshold of 0.5\n",
    "y_pred = np.where(lr_probs > 0.7, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the f1 score, precision, and recall\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e0a9c",
   "metadata": {},
   "source": [
    "## Writing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e701492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import LSTM\n",
    "from keras.regularizers import l1\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "encoding_dim = 6  # number of neurons in hidden layer of autoencoder\n",
    "num_features = 10  # number of input variables\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "\n",
    "input_shape = (10,)  # 10 refers to the 10 features\n",
    "\n",
    "encoder = tf.keras.layers.Dense(units=10, activation=\"elu\", input_shape=input_shape)\n",
    "first_hidden_layer = tf.keras.layers.Dense(units=encoding_dim, activation=\"elu\")\n",
    "decoder = tf.keras.layers.Dense(units=10, activation=\"sigmoid\")\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# Load the saved pre-trained CNN\n",
    "CNN = tf.keras.models.load_model(\"pre_trained_model.h5\")\n",
    "\n",
    "# Freeze all trainable layers in CNN before training LSTM\n",
    "CNN.trainable = False\n",
    "\n",
    "# V takes 2 arguments as it represents a matrix where each row corresponds to an input feature\n",
    "# and each column corresponds to a hidden layer neuron. In other words,V is a matrix that will store the encoded representations  of each input feature in the hidden layer of the autoencoder.\n",
    "V = np.zeros((num_features, encoding_dim))\n",
    "\n",
    "# 1st for loop: Encoding the raw features into hidden representations\n",
    "for i in range(num_features):\n",
    "    V[i] = autoencoder.predict(X_train_resampled_final[i : i + 1])[0][0:encoding_dim]\n",
    "\n",
    "# Reshape the hidden representations so that it can be fed to the CNN\n",
    "V = np.reshape(V, (-1, num_features, 1))\n",
    "\n",
    "# The CNN model is designed to process all the encoded features in V at once. This is done by passing V as input to the CNN model.\n",
    "\n",
    "C = CNN(V)\n",
    "C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "\n",
    "# Define the LSTM layers\n",
    "LSTM1 = tf.keras.layers.LSTM(\n",
    "    units=64,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    input_shape=(1, num_features),\n",
    ")\n",
    "\n",
    "# Add the LSTM layers to the model\n",
    "LSTM_model = tf.keras.Sequential()\n",
    "LSTM_model.add(LSTM1)\n",
    "\n",
    "# Add a fully connected layer\n",
    "LSTM_model.add(\n",
    "    tf.keras.layers.Dense(units=20, activation=\"tanh\", kernel_regularizer=l1(0.00114))\n",
    ")\n",
    "\n",
    "# Add a dropout layer\n",
    "LSTM_model.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "# Add the output layer\n",
    "LSTM_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Define the learning rate\n",
    "learning_rate = 0.0015\n",
    "\n",
    "# Create the Adam optimizer with the defined learning rate\n",
    "opt_LSTM = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "LSTM_model.compile(optimizer=opt_LSTM, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "X_train_reshaped = np.reshape(X_train_resampled_final, (-1, 1, num_features))\n",
    "\n",
    "# Train the model\n",
    "LSTM_model.fit(X_train_reshaped, y_train_resampled_final, epochs=8, batch_size=32)\n",
    "\n",
    "# Unfreeze the CNN before training the hybrid CNN-LSTM\n",
    "CNN.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2db20",
   "metadata": {},
   "source": [
    "## Writing part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input shape\n",
    "input_shape = (10, 1)\n",
    "\n",
    "# Extract convolutional,batch normalization, maxpooling, and flatten layers from pre-trained model\n",
    "conv_layer = pre_trained_model.layers[0]\n",
    "batch_layer = pre_trained_model.layers[1]\n",
    "conv2_layer = pre_trained_model.layers[2]\n",
    "batch2_layer = pre_trained_model.layers[3]\n",
    "maxpooling_layer = pre_trained_model.layers[4]\n",
    "flatten_layer = pre_trained_model.layers[5]\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Pass input through CNN layers\n",
    "x = conv_layer(input_layer)\n",
    "x = batch_layer(x)\n",
    "x = conv2_layer(x)\n",
    "x = batch2_layer(x)\n",
    "x = maxpooling_layer(x)\n",
    "x = flatten_layer(x)\n",
    "\n",
    "# Reshape output of CNN to feed into the LSTM\n",
    "x = Reshape((1, -1))(x)\n",
    "\n",
    "# Pass output of CNN through LSTM layer\n",
    "x = LSTM1(x)\n",
    "\n",
    "# Add fully connected layer\n",
    "x = Dense(20, activation='tanh')(x)\n",
    "\n",
    "# Add dropout layer\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Add output layer\n",
    "output_layer = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524f14f9",
   "metadata": {},
   "source": [
    "## Model-fine tuning: Time Distributed Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Flatten, Dense, Input, Reshape\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate, LSTM\n",
    "\n",
    "# Extract convolutional, maxpooling, and flatten layers from pre-trained model\n",
    "conv_layer = pre_trained_model.layers[0]\n",
    "maxpool_layer = pre_trained_model.layers[1]\n",
    "flatten_layer = pre_trained_model.layers[2]\n",
    "\n",
    "# create LSTM layers\n",
    "num_units_1 = 32\n",
    "num_units_2 = 64\n",
    "LSTM1 = LSTM(units=num_units_1, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, encoding_dim), return_sequences=True)\n",
    "LSTM2 = LSTM(units=num_units_2, use_bias=True, kernel_initializer=\"glorot_uniform\")\n",
    "\n",
    "# wrap the CNN in TimeDistributed layers\n",
    "time_distributed_conv_layer = TimeDistributed(conv_layer)\n",
    "time_distributed_maxpool_layer = TimeDistributed(maxpool_layer)\n",
    "time_distributed_flatten_layer = TimeDistributed(flatten_layer)\n",
    "\n",
    "# create the CNN model\n",
    "cnn_input = Input(shape=(10,))\n",
    "x = time_distributed_conv_layer(cnn_input)\n",
    "x = time_distributed_maxpool_layer(x)\n",
    "x = time_distributed_flatten_layer(x)\n",
    "cnn_output = Dense(units=10, activation='relu')(x)\n",
    "\n",
    "# connect the output of the CNN to the LSTM layers\n",
    "lstm_input = Reshape((sequence_length, 10))(cnn_output)\n",
    "lstm_output_1 = LSTM1(lstm_input)\n",
    "lstm_output_2 = LSTM2(lstm_output_1)\n",
    "\n",
    "# add a fully connected layer with 17 neurons\n",
    "fc_layer = Dense(units=17, activation='relu')(lstm_output_2)\n",
    "\n",
    "# concatenate the output of the CNN-LSTM and fully connected layers\n",
    "merge_layer = concatenate([cnn_output, lstm_output_2, fc_layer])\n",
    "\n",
    "# add the final classification layer\n",
    "output_layer = Dense(units=1, activation='sigmoid')(merge_layer)\n",
    "\n",
    "# create the hybrid model\n",
    "time_distributed_model = Model(inputs=[cnn_input], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "encoding_dim = 10\n",
    "num_hidden = 10\n",
    "num_features = 10\n",
    "num_filters = 16\n",
    "num_units = 32\n",
    "\n",
    "# Define the autoencoder to encode each feature\n",
    "input_shape = (10,)  # N is the number of input features\n",
    "\n",
    "encoder = tf.keras.layers.Dense(units=10, activation='relu', input_shape=input_shape)\n",
    "first_hidden_layer = tf.keras.layers.Dense(units=6, activation='relu')\n",
    "decoder = tf.keras.layers.Dense(units=10, activation='sigmoid')\n",
    "\n",
    "autoencoder = tf.keras.Sequential([encoder, first_hidden_layer, decoder])\n",
    "\n",
    "# Load the pre-trained CNN\n",
    "CNN = tf.keras.models.load_model('pre_trained_model.h5')\n",
    "\n",
    "# Freeze all layers in CNN\n",
    "CNN.trainable = False\n",
    "\n",
    "V = np.zeros((num_features, num_hidden))\n",
    "\n",
    "# 1st for loop: Encoding\n",
    "for i in range(num_features):\n",
    "    V[i] = autoencoder.predict(X_train_resampled_final[i:i+1])[0][0:encoding_dim]\n",
    "\n",
    "C = CNN(V)\n",
    "C = np.expand_dims(C, axis=0)  # Add a new dimension to C\n",
    "C = np.repeat(C, num_features, axis=0)  # Repeat C num_features times\n",
    "\n",
    "# Define the LSTM layer with the appropriate input shape\n",
    "LSTM = tf.keras.layers.LSTM(units=num_units, use_bias=True, kernel_initializer=\"glorot_uniform\", input_shape=(1, num_hidden))\n",
    "\n",
    "# Reshape V to have shape (num_features, 1, num_hidden) to add the timesteps dimension\n",
    "V = np.reshape(V, (num_features, 1, num_hidden))\n",
    "\n",
    "# Pass the vector representations through the LSTM layer\n",
    "O = LSTM(V)\n",
    "\n",
    "# Define the final classification layer\n",
    "FL = tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# Connect it to the output of the LSTM layer\n",
    "O = FL(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5debe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# # Build the model\n",
    "# model = tf.keras.Sequential([LSTM])\n",
    "# model.build(input_shape=(None, 1, num_hidden))\n",
    "\n",
    "# # Plot the model architecture\n",
    "# plot_model(model, to_file='lstm.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f231a4",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Create a function to build the Keras model\n",
    "def create_model(filters=32, kernel_size=3, activation='tanh', dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create a KerasClassifier from the build function\n",
    "keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "param_dist = {\n",
    "    'filters': randint(16, 64),\n",
    "    'kernel_size': [3, 5],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Define the number of iterations for random search\n",
    "n_iter_search = 1\n",
    "\n",
    "# Create a RandomizedSearchCV object and fit it to the data\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=keras_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=n_iter_search,\n",
    "    cv=3,\n",
    "    verbose=2\n",
    ")\n",
    "random_search_result = random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "\n",
    "# Print the best hyperparameters found by random search\n",
    "print(\"Best hyperparameters: \", random_search_result.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f445aeb",
   "metadata": {},
   "source": [
    "## Here: Add in thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "def create_model(conv_layers=1, filters=32, kernel_size=3, activation='tanh', pool_layers=1, pool_size=2, dropout_rate=0.2, learning_rate=0.001, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "    \n",
    "    for i in range(conv_layers-1):\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "        \n",
    "    for i in range(pool_layers):\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(lr=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(lr=learning_rate)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "param_dist = {\n",
    "    'conv_layers': [1, 2, 3],\n",
    "    'filters': randint(16, 32),\n",
    "    'kernel_size': [1,2,3],\n",
    "    'pool_layers': [0, 1, 2,5],\n",
    "    'pool_size': [2, 3],\n",
    "    'activation': ['tanh', 'relu','elu','selu'],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'learning_rate': [0.01, 0.03, 0.1],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop']\n",
    "}\n",
    "\n",
    "n_iter_search = 5\n",
    "random_search = RandomizedSearchCV(estimator=keras_model, param_distributions=param_dist, n_iter=n_iter_search, cv=3)\n",
    "random_search_result = random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "print(\"Best hyperparameters: \", random_search_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37596da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint\n",
    "# import time\n",
    "\n",
    "# def create_model(conv_layers=1, filters=32, kernel_size=3, activation='tanh', pool_layers=1, pool_size=2, \n",
    "#                  dense_neurons=16, dropout_rate=0.2, learning_rate=0.001, optimizer='adam', last_activation='sigmoid'):\n",
    "    \n",
    "#     model = Sequential()\n",
    "    \n",
    "#     model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "    \n",
    "#     for i in range(conv_layers-1):\n",
    "#         model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "        \n",
    "#     for i in range(pool_layers):\n",
    "#         model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "#     model.add(Flatten())\n",
    "    \n",
    "#     model.add(Dense(dense_neurons, activation=activation))\n",
    "    \n",
    "#     model.add(Dropout(dropout_rate))\n",
    "    \n",
    "#     model.add(Dense(1, activation=last_activation))\n",
    "    \n",
    "#     if optimizer == 'adam':\n",
    "#         opt = Adam(lr=learning_rate)\n",
    "#     elif optimizer == 'sgd':\n",
    "#         opt = SGD(lr=learning_rate)\n",
    "#     else:\n",
    "#         opt = optimizer\n",
    "    \n",
    "#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# param_dist = {\n",
    "#     'conv_layers': [1, 2, 3],\n",
    "#     'filters': randint(16, 64),\n",
    "#     'kernel_size': [3, 5],\n",
    "#     'pool_layers': [0, 1, 2],\n",
    "#     'pool_size': [2, 3],\n",
    "#     'dense_neurons': [8, 16, 32],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'dropout_rate': [0.2, 0.5],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "#     'last_activation': ['sigmoid', 'softmax']\n",
    "# }\n",
    "\n",
    "# n_iter_search = 5\n",
    "\n",
    "# start_time = time.time()\n",
    "# random_search = RandomizedSearchCV(estimator=keras_model, param_distributions=param_dist, n_iter=n_iter_search, cv=3)\n",
    "# random_search_result = random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "# end_time = time.time()\n",
    "\n",
    "# print(\"Best hyperparameters: \", random_search_result.best_params_)\n",
    "# print(\"Time taken: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae4bb8",
   "metadata": {},
   "source": [
    "## Good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9600eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import time\n",
    "\n",
    "def create_model(conv_layers=1, filters=32, kernel_size=3, activation='tanh', pool_layers=1, pool_size=2, dropout_rate=0.2, learning_rate=0.001, optimizer='adam', fc_neurons=32, last_activation='sigmoid'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "    \n",
    "    for i in range(conv_layers-1):\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "        \n",
    "    for i in range(pool_layers):\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(fc_neurons, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation=last_activation))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(lr=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(lr=learning_rate)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "param_dist = {\n",
    "    'conv_layers': [1, 3, 4, 5],\n",
    "    'filters': randint(16, 100),\n",
    "    'kernel_size': [3, 4, 5,7],\n",
    "    'pool_layers': [0, 1, 2, 3],\n",
    "    'pool_size': [2, 3,4],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'dropout_rate': [0.2, 0.7],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'fc_neurons': randint(16, 64),\n",
    "    'last_activation': ['sigmoid', 'softmax']\n",
    "}\n",
    "\n",
    "n_iter_search = 5\n",
    "cv = 3\n",
    "random_search = RandomizedSearchCV(estimator=keras_model, param_distributions=param_dist, n_iter=n_iter_search, cv=cv)\n",
    "start_time = time.time()\n",
    "random_search_result = random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "end_time = time.time()\n",
    "print(\"Time taken: \", end_time - start_time, \"seconds\")\n",
    "print(\"Best hyperparameters: \", random_search_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e744b28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import time\n",
    "\n",
    "def create_model(conv_layers=1, filters=32, kernel_size=3, activation='tanh', pool_layers=1, pool_size=2, dropout_rate=0.2, learning_rate=0.001, optimizer='adam', fc_neurons=32, last_activation='sigmoid'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "    \n",
    "    for i in range(conv_layers-1):\n",
    "        model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "        \n",
    "    for i in range(pool_layers):\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(fc_neurons, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation=last_activation))\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(lr=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(lr=learning_rate)\n",
    "    else:\n",
    "        opt = optimizer\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "param_dist = {\n",
    "    'conv_layers': [1, 2, 3],\n",
    "    'filters': randint(16, 64),\n",
    "    'kernel_size': [3, 5],\n",
    "    'pool_layers': [0, 1, 2],\n",
    "    'pool_size': [2, 3],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "    'fc_neurons': randint(16, 64),\n",
    "    'last_activation': ['sigmoid', 'softmax']\n",
    "}\n",
    "\n",
    "n_iter_search = 8\n",
    "cv = 5\n",
    "random_search = RandomizedSearchCV(estimator=keras_model, param_distributions=param_dist, n_iter=n_iter_search, cv=cv)\n",
    "start_time = time.time()\n",
    "random_search_result = random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "end_time = time.time()\n",
    "print(\"Time taken: \", end_time - start_time, \"seconds\")\n",
    "print(\"Best hyperparameters: \", random_search_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from scipy.stats import randint\n",
    "# import time\n",
    "\n",
    "# def create_model(conv_layers=1, filters=32, kernel_size=3, activation='tanh', pool_layers=1, pool_size=2, dropout_rate=0.2, learning_rate=0.001, optimizer='adam', fc_neurons=32, last_activation='sigmoid'):\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "    \n",
    "#     for i in range(conv_layers-1):\n",
    "#         model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "        \n",
    "#     for i in range(pool_layers):\n",
    "#         model.add(MaxPooling1D(pool_size=pool_size))\n",
    "        \n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(fc_neurons, activation=activation))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(1, activation=last_activation))\n",
    "    \n",
    "#     if optimizer == 'adam':\n",
    "#         opt = Adam(lr=learning_rate)\n",
    "#     elif optimizer == 'sgd':\n",
    "#         opt = SGD(lr=learning_rate)\n",
    "#     else:\n",
    "#         opt = optimizer\n",
    "    \n",
    "#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# param_dist = {\n",
    "#     'conv_layers': [1, 2, 3.4],\n",
    "#     'filters': randint(16, 128),\n",
    "#     'kernel_size': [2, 3, 4, 5],\n",
    "#     'pool_layers': [0, 2, 4, 6],\n",
    "#     'pool_size': [2, 3 , 4],\n",
    "#     'activation': ['tanh', 'relu','elu',],\n",
    "#     'dropout_rate': [0.2, 0.5],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "#     'fc_neurons': randint(16, 128),\n",
    "#     'last_activation': ['sigmoid', 'softmax']\n",
    "# }\n",
    "\n",
    "# n_iter_search = 8\n",
    "# cv = 5\n",
    "# random_search = RandomizedSearchCV(estimator=keras_model, param_distributions=param_dist, n_iter=n_iter_search, cv=cv)\n",
    "# start_time = time.time()\n",
    "# random_search_result = random_search.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "# end_time = time.time()\n",
    "# print(\"Time taken: \", end_time - start_time, \"seconds\")\n",
    "# print(\"Best hyperparameters: \", random_search_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ba45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model(filters=32, kernel_size=3, activation='tanh', dropout_rate=0.2, loss='binary_crossentropy', opt='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(10, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation=activation))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "keras_model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "param_grid = {\n",
    "    'filters': [16, 32, 64],\n",
    "    'kernel_size': [3, 5],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'dropout_rate': [0.2, 0.5],\n",
    "    'loss': ['binary_crossentropy', 'mse'],\n",
    "    'opt': ['adam', 'sgd']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(X_train_resampled_final, y_train_resampled_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
