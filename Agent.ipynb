{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f374f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90396195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1ac7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700000, 11)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17bda14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd852dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95128f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.55, random_state=0)\n",
    "\n",
    "# Fit and apply the resampler to the entire dataset\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41793035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.645162\n",
      "1    0.354838\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.645159\n",
      "1    0.354841\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = df_sample.drop('isFraud', axis=1)\n",
    "# # Separate the target variable\n",
    "# y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, stratify=y_resampled, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(\"Count of 0s: \", counts[0])\n",
    "print(\"Count of 1s: \", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f83c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# #Upsampling via SMOTE\n",
    "# smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "# #Downsample via RandomUnderSampler\n",
    "# rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "# #Application of the resampling methods\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "# X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5f220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a3c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ced06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5184944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0        230   21730.530000   21069.000000             0.0        0.000000   \n",
      "1        299  153287.571152       0.000000             0.0    48032.732913   \n",
      "2        214   40082.480000   75685.200439             0.0    54532.410000   \n",
      "3        229   72769.580000   43166.000000             0.0        0.000000   \n",
      "4        284   19306.770000       0.000000             0.0        0.000000   \n",
      "...      ...            ...            ...             ...             ...   \n",
      "914981   124  153287.571152   75685.200439             0.0        0.000000   \n",
      "914982   282  141996.285548  141996.285548             0.0   128838.062662   \n",
      "914983   305  153287.571152   75685.200439             0.0    28465.618106   \n",
      "914984   435  341707.785915   75685.200439             0.0    48032.732913   \n",
      "914985   486  153287.571152   75685.200439             0.0        0.000000   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0             0.000000               0     3  154180.5    492371  \n",
      "1        229158.563284               0     1  202934.0    627621  \n",
      "2         14449.930000               0     0   22974.0    677921  \n",
      "3         72769.580000               0     1  154491.0    670050  \n",
      "4             0.000000               0     3  154180.5    106403  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "914981   229158.563284               0     1   84744.0    604050  \n",
      "914982   270834.349422               0     1  164586.0    142360  \n",
      "914983   229158.563284               0     1  118470.0    411760  \n",
      "914984   727531.448120               0     1  170412.0    158105  \n",
      "914985   761542.624447               0     1  197990.0    192587  \n",
      "\n",
      "[914986 rows x 10 columns]\n",
      "         step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "162757    328  230117.470000       0.000000             0.0    10713.613233   \n",
      "476897    395   15010.570000       0.000000             0.0    10713.613233   \n",
      "305893     16  244173.720000   20167.000000             0.0    10266.000000   \n",
      "11003     280    5335.170000   61909.500000             0.0        0.000000   \n",
      "920032    461   63680.344537   63680.344537             0.0    28242.349065   \n",
      "...       ...            ...            ...             ...             ...   \n",
      "684283    428    7598.630000   82228.000000             0.0        0.000000   \n",
      "530816    188  324406.230000       0.000000             0.0    10713.613233   \n",
      "1014827   280  134603.595449   61909.500000             0.0    10713.613233   \n",
      "980792    440   91052.149907   91052.149907             0.0        0.000000   \n",
      "600909    352    3570.040000   19871.000000             0.0    24685.130000   \n",
      "\n",
      "         newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "162757     159337.38000               0     1  183207.0     96649  \n",
      "476897     459896.08000               0     1  208691.0     48663  \n",
      "305893     254439.72000               0     1   59417.0    581824  \n",
      "11003           0.00000               0     3  292933.0    435630  \n",
      "920032      91922.70334               0     1   52464.0     17748  \n",
      "...                 ...             ...   ...       ...       ...  \n",
      "684283          0.00000               0     3  163726.5    426109  \n",
      "530816     159337.38000               0     1  102344.0    267358  \n",
      "1014827    159337.38000               0     1    8010.0    130708  \n",
      "980792          0.00000               0     1  229356.0    645224  \n",
      "600909      28255.17000               0     2  140420.0    211274  \n",
      "\n",
      "[108364 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0385cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc22192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b82113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy as np\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the autoencoder model\n",
    "# def create_autoencoder(encoding_dim, batch_size, learning_rate, patience, l1_reg):\n",
    "#     input_layer = Input(shape=(10,))\n",
    "#     hidden_layer = Dense(encoding_dim, activation='relu', activity_regularizer='l1', kernel_regularizer='l1')(input_layer)\n",
    "#     output_layer = Dense(10, activation='sigmoid')(hidden_layer)\n",
    "#     autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#     autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "#     autoencoder.fit(X_train_resampled_final, X_train_resampled_final, epochs=1, batch_size=int(batch_size), validation_data=(X_test, X_test), callbacks=[EarlyStopping(monitor='val_loss', patience=int(patience), mode='min')])\n",
    "#     reconstruction_loss = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     return -reconstruction_loss\n",
    "\n",
    "# # Define the search space for the hyperparameters\n",
    "# search_space = {\n",
    "#     'encoding_dim': (10, 50),\n",
    "#     'batch_size': (32, 128),\n",
    "#     'learning_rate': (-5, -1),\n",
    "#     'patience': (1, 10),\n",
    "#     'l1_reg': (0, 0.1),\n",
    "# }\n",
    "\n",
    "# # Create a BayesianOptimization optimizer and optimize the function defined above\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=create_autoencoder,\n",
    "#     pbounds=search_space,\n",
    "#     random_state=42,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# optimizer.maximize(n_iter=10, init_points=2)\n",
    "\n",
    "# # Get the best hyperparameters\n",
    "# best_encoding_dim = int(optimizer.max['params']['encoding_dim'])\n",
    "# best_batch_size = int(optimizer.max['params']['batch_size'])\n",
    "# best_learning_rate = optimizer.max['params']['learning_rate']\n",
    "# best_patience = int(optimizer.max['params']['patience'])\n",
    "# best_l1_reg = optimizer.max['params']['l1_reg']\n",
    "\n",
    "# # Train the autoencoder model using the best hyperparameters\n",
    "# best_autoencoder = create_autoencoder(best_encoding_dim, best_batch_size, best_learning_rate, best_patience, best_l1_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa203a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "\n",
    "# Define the autoencoder model\n",
    "def create_autoencoder(encoding_dim, batch_size, learning_rate, patience, l1_reg):\n",
    "    input_layer = Input(shape=(10,))\n",
    "    hidden_layer = Dense(encoding_dim, activation='relu', activity_regularizer='l1', kernel_regularizer='l1')(input_layer)\n",
    "    output_layer = Dense(10, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "    for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "        X_train, X_val = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "        es = EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "        autoencoder.fit(X_train, X_train, epochs=5, batch_size=int(batch_size), validation_data=(X_val, X_val), callbacks=[es])\n",
    "        val_losses.append(autoencoder.evaluate(X_val, X_val))\n",
    "    return np.mean(val_losses)\n",
    "\n",
    "# Define the search space for the hyperparameters\n",
    "search_space = {\n",
    "    'encoding_dim': (10, 50),\n",
    "    'batch_size': (32, 128),\n",
    "    'learning_rate': (-5, -1),\n",
    "    'patience': (1, 10),\n",
    "    'l1_reg': (0, 0.1),\n",
    "}\n",
    "\n",
    "# Create a BayesianOptimization optimizer and optimize the function defined above\n",
    "optimizer = BayesianOptimization(\n",
    "    f=create_autoencoder,\n",
    "    pbounds=search_space,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "optimizer.maximize(n_iter=3, init_points=2)\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_encoding_dim = int(optimizer.max['params']['encoding_dim'])\n",
    "best_batch_size = int(optimizer.max['params']['batch_size'])\n",
    "best_learning_rate = optimizer.max['params']['learning_rate']\n",
    "best_patience = int(optimizer.max['params']['patience'])\n",
    "best_l1_reg = optimizer.max['params']['l1_reg']\n",
    "\n",
    "# Print the best hyperparameters and the time taken by the algorithm\n",
    "print('Best hyperparameters:')\n",
    "print(f'Encoding dim: {best_encoding_dim}')\n",
    "print(f'Batch size: {best_batch_size}')\n",
    "print(f'Learning rate: {best_learning_rate}')\n",
    "print(f'Patience: {best_patience}')\n",
    "print(f'L1 reg: {best_l1_reg}')\n",
    "print(f'Time taken: {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ce6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best hyperparameters: Encoding dim: {best_encoding_dim}, Batch size: {best_batch_size}, Learning rate: {best_learning_rate}, Patience: {best_patience}, L1 reg: {best_l1_reg}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba3d6140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001D72B7AAE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001D72B7AAE58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "7862/7900 [============================>.] - ETA: 0s - loss: 0.4089WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001D7329F1318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001D7329F1318> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "7900/7900 [==============================] - 20s 2ms/step - loss: 0.4088 - val_loss: 0.5089\n",
      "Epoch 2/5\n",
      "7900/7900 [==============================] - 19s 2ms/step - loss: 0.3926 - val_loss: 0.5064\n",
      "Epoch 3/5\n",
      "7900/7900 [==============================] - 19s 2ms/step - loss: 0.3912 - val_loss: 0.5055\n",
      "Epoch 4/5\n",
      "7900/7900 [==============================] - 19s 2ms/step - loss: 0.3907 - val_loss: 0.5059\n",
      "Epoch 5/5\n",
      "7900/7900 [==============================] - 19s 2ms/step - loss: 0.3905 - val_loss: 0.5052\n",
      "Test MSE: 0.44224\n",
      "Epoch 1/5\n",
      "7900/7900 [==============================] - 20s 3ms/step - loss: 0.5050 - val_loss: 0.3901\n",
      "Epoch 2/5\n",
      "7900/7900 [==============================] - 20s 2ms/step - loss: 0.5049 - val_loss: 0.3911\n",
      "Epoch 3/5\n",
      "7900/7900 [==============================] - 20s 2ms/step - loss: 0.5048 - val_loss: 0.3899\n",
      "Epoch 4/5\n",
      "7900/7900 [==============================] - 19s 2ms/step - loss: 0.5048 - val_loss: 0.3917\n",
      "Epoch 5/5\n",
      "7900/7900 [==============================] - 19s 2ms/step - loss: 0.5048 - val_loss: 0.3900\n",
      "Test MSE: 0.44164\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 15\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.0033827404209811787))(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "opt = Adam(lr= 0.00818356134505909)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder model with the specified optimizer and loss function\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    # Define early stopping to prevent overfitting and improve efficiency\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=5,batch_size=64, verbose=1, validation_data=(X_val_fold, X_val_fold),callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "# plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.fill_between(range(1, len(mean_train_mse)+1), mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(range(1, len(mean_val_mse)+1), mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047e8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 42\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.1))(input_layer)\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "opt = Adam(lr= 0.1)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the autoencoder model with the specified optimizer and loss function\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "test_mse = []\n",
    "recon_errors = []\n",
    "\n",
    "# Loop over each fold\n",
    "for train_index, val_index in kf.split(X_train_resampled_final):\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    # Define early stopping to prevent overfitting and improve efficiency\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=30, batch_size=64, verbose=1, validation_data=(X_val_fold, X_val_fold), callbacks=[early_stopping])\n",
    "    \n",
    "    # Append the MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(history.history['loss'])\n",
    "    val_mse.append(history.history['val_loss'])\n",
    "    \n",
    "    # compute the reconstruction error for the test data\n",
    "    recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    recon_errors.append(recon_error)\n",
    "    \n",
    "    # Calculate the MSE for the test set\n",
    "    test_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "    test_mse.append(test_error)\n",
    "    print(f\"Test MSE: {test_error:.5f}\")\n",
    "\n",
    "# Calculate the mean and standard deviation of MSE for training and validation sets across all folds\n",
    "mean_train_mse = np.mean(train_mse, axis=0)\n",
    "std_train_mse = np.std(np.concatenate(train_mse), axis=0)\n",
    "mean_val_mse = np.mean(val_mse, axis=0)\n",
    "std_val_mse = np.std(np.concatenate(val_mse), axis=0)\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(range(1, len(mean_train_mse)+1), mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(range(1, len(mean_val_mse)+1), mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.xlim([1,30]) # set the x-axis limit to show only up to 30 epochs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a7075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have two lists, one for the training MSE and one for the test MSE, for each epoch\n",
    "train_mse = [0.1, 0.05, 0.03, 0.02, 0.015, 0.012, 0.01, 0.008, 0.007, 0.006, 0.005, 0.004, 0.0035, 0.003, 0.0028, 0.0026, 0.0025, 0.0023, 0.0022, 0.0021]\n",
    "test_mse = [0.15, 0.12, 0.1, 0.09, 0.08, 0.075, 0.072, 0.07, 0.068, 0.065, 0.063, 0.062, 0.061, 0.06, 0.059, 0.058, 0.057, 0.056, 0.055, 0.054]\n",
    "epochs = range(1, len(train_mse)+1)\n",
    "\n",
    "plt.plot(epochs, train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(range(1, len(train_mse)+1), [x - 0.01 for x in train_mse], [x + 0.01 for x in train_mse], alpha=0.2, color='b')\n",
    "plt.plot(epochs, test_mse, 'r', label='Test MSE')\n",
    "plt.fill_between(range(1, len(test_mse)+1), [x - 0.01 for x in test_mse], [x + 0.01 for x in test_mse], alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.xlim([1,20]) # set the x-axis limit to show only up to 20 epochs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "\n",
    "space = {\n",
    "    'encoding_dim': hp.quniform('encoding_dim', 10, 50, 1),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -1),\n",
    "    'patience': hp.quniform('patience', 1, 10, 1),\n",
    "    'l1_reg': hp.uniform('l1_reg', 0, 0.1),\n",
    "    'activation': hp.choice('activation', ['relu', 'sigmoid', 'tanh']),\n",
    "    'epochs': hp.quniform('epochs', 10, 30, 1)\n",
    "}\n",
    "\n",
    "def optimize(params):\n",
    "    encoding_dim = int(params['encoding_dim'])\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    patience = int(params['patience'])\n",
    "    l1_reg = params['l1_reg']\n",
    "    activation = params['activation']\n",
    "    epochs=int(params['epochs'])\n",
    "   \n",
    "    # Define autoencoder architecture\n",
    "    input_dim = X_train_resampled_final.shape[1]\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    hidden_layer = Dense(encoding_dim, activation=activation, activity_regularizer=regularizers.l1(l1_reg))(input_layer)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(hidden_layer)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=Adam(lr=learning_rate), loss='mse')\n",
    "\n",
    "    # Define cross-validation parameters\n",
    "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # Train and evaluate model using cross-validation\n",
    "    for train_idx, val_idx in kf.split(X_train_resampled_final):\n",
    "        # Split data into training and validation sets\n",
    "        train_data, val_data = X_train_resampled_final[train_idx], X_train_resampled_final[val_idx]\n",
    "\n",
    "        # Train model\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, mode='min', restore_best_weights=True)\n",
    "        autoencoder.fit(train_data, train_data, epochs=epochs, batch_size=batch_size, validation_data=(val_data, val_data), verbose=0, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluate model\n",
    "        val_loss = autoencoder.evaluate(val_data, val_data, verbose=0)\n",
    "        losses.append(val_loss)\n",
    "\n",
    "    # Calculate mean validation loss across folds\n",
    "    mean_loss = np.mean(losses)\n",
    "\n",
    "    return {'loss': mean_loss, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "best = fmin(fn=optimize, space=space, algo=tpe.suggest, max_evals=5)\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50466ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D72BEDB438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D72BEDB438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "31600/31600 [==============================] - 30s 948us/step\n"
     ]
    }
   ],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model = Model(inputs=autoencoder.input, outputs=autoencoder.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train = hidden_layer_model.predict(X_train_resampled_final)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input = Input(shape=(hidden_layer_output_train.shape[1],))\n",
    "x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.000111))(new_model_input)\n",
    "# x = Dense(32, activation='relu')(x)\n",
    "output = Dense(2, activation='sigmoid')(x)\n",
    "#output = Dense(1, activation='softmax')(x)\n",
    "mediator_network = Model(inputs=new_model_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db0c2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert target labels to one-hot encoded format\n",
    "y_train_resampled_final_onehot = to_categorical(y_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0dcd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt_new = Adam(lr= 0.000992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34b3c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001D732AEC8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001D732AEC8B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "28413/28440 [============================>.] - ETA: 0s - loss: 0.4423WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001D7324E5678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000001D7324E5678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "28440/28440 [==============================] - 43s 1ms/step - loss: 0.4422 - val_loss: 0.5038\n",
      "Epoch 2/15\n",
      "28440/28440 [==============================] - 43s 2ms/step - loss: 0.2519 - val_loss: 0.3093\n",
      "Epoch 3/15\n",
      "28440/28440 [==============================] - 46s 2ms/step - loss: 0.1972 - val_loss: 0.2930\n",
      "Epoch 4/15\n",
      "28440/28440 [==============================] - 42s 1ms/step - loss: 0.1854 - val_loss: 0.2232\n",
      "Epoch 5/15\n",
      "28440/28440 [==============================] - 42s 1ms/step - loss: 0.1785 - val_loss: 0.2591\n",
      "Epoch 6/15\n",
      "28440/28440 [==============================] - 43s 2ms/step - loss: 0.1733 - val_loss: 0.2167\n",
      "Epoch 7/15\n",
      "28440/28440 [==============================] - 42s 1ms/step - loss: 0.1692 - val_loss: 0.1914\n",
      "Epoch 8/15\n",
      "28440/28440 [==============================] - 42s 1ms/step - loss: 0.1663 - val_loss: 0.3393\n",
      "Epoch 9/15\n",
      "28440/28440 [==============================] - 43s 2ms/step - loss: 0.1637 - val_loss: 0.2655\n",
      "Epoch 10/15\n",
      "28440/28440 [==============================] - 42s 1ms/step - loss: 0.1613 - val_loss: 0.1936\n",
      "Epoch 11/15\n",
      "28440/28440 [==============================] - 43s 1ms/step - loss: 0.1588 - val_loss: 0.1987\n",
      "Epoch 12/15\n",
      "28440/28440 [==============================] - 43s 2ms/step - loss: 0.1564 - val_loss: 0.2043\n",
      "Epoch 13/15\n",
      "28440/28440 [==============================] - 43s 1ms/step - loss: 0.1541 - val_loss: 0.1785\n",
      "Epoch 14/15\n",
      "28440/28440 [==============================] - 42s 1ms/step - loss: 0.1516 - val_loss: 0.2210\n",
      "Epoch 15/15\n",
      "28440/28440 [==============================] - 43s 1ms/step - loss: 0.1495 - val_loss: 0.1612\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "#Compile the new model\n",
    "mediator_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# Train the new model on the activations of the hidden layer\n",
    "history = mediator_network.fit(hidden_layer_output_train, y_train_resampled_final_onehot,\n",
    "                               epochs=15, batch_size=32, validation_split=0.1,\n",
    "                               callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = Model(inputs=mediator_network .input, outputs=mediator_network .layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "\n",
    "# x = Dense(10, activation='tanh',kernel_regularizer=regularizers.l1(0.0000611))(new_model_input_med)\n",
    "\n",
    "# output_med = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# agent_network = Model(inputs=new_model_input_med, outputs=output_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define the early stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# agent_network.compile(optimizer=opt_new, loss='binary_crossentropy')\n",
    "\n",
    "# # Train the new model on the activations of the hidden layer\n",
    "# history = agent_network.fit(hidden_layer_output_train_med, y_train_resampled_final_onehot,\n",
    "#                                epochs=10, batch_size=32, validation_split=0.2,\n",
    "#                                callbacks=[early_stopping],verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a088bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow import keras\n",
    "\n",
    "# Import necessary libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 32\n",
    "num_episodes = 15\n",
    "max_steps = 7\n",
    "learning_rate=0.5\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize experience replay memory\n",
    "M = 20000\n",
    "replay_memory = []\n",
    "\n",
    "# Define epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon, theta):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, 1)\n",
    "    else:\n",
    "        Q_values = Q(state, theta)\n",
    "        return np.argmax(Q_values)\n",
    "\n",
    "# Define Q-network\n",
    "# def agent_network(state_shape, num_actions):\n",
    "#     inputs = keras.layers.Input(shape=state_shape)\n",
    "#     x = keras.layers.Dense(32, activation='relu')(inputs)\n",
    "#     x = keras.layers.Dense(32, activation='relu')(x)\n",
    "#     outputs = keras.layers.Dense(num_actions)(x)\n",
    "#     model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "def agent_network(state_shape, num_actions):\n",
    "    inputs = keras.layers.Input(shape=(1,))\n",
    "    x = keras.layers.Dense(10, activation='tanh')(inputs)\n",
    "    outputs = keras.layers.Dense(num_actions)(x)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Define Q function\n",
    "def Q(state, theta):\n",
    "    return agent_network(state.shape, 2)(state.reshape(1, -1)).numpy()[0]\n",
    "\n",
    "\n",
    "# Define loss function\n",
    "def compute_loss(target_Q_values, predicted_Q_values):\n",
    "    return np.mean(np.square(target_Q_values - predicted_Q_values))\n",
    "\n",
    "# Define reward function\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    if action == 1:\n",
    "        reward = true_label * (predicted_label - lambda_val) - (1 - true_label) * (predicted_label + lambda_val)\n",
    "    else:\n",
    "        reward = (1 - true_label) * (predicted_label - lambda_val) - true_label * (predicted_label + lambda_val)\n",
    "    return reward, int(predicted_label == true_label)\n",
    "\n",
    "# Define hyperparameters\n",
    "num_episodes = 10\n",
    "max_steps = 20\n",
    "epsilon = 0.1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize simulation environments\n",
    "environments = [epsilon for i in range(num_episodes)]\n",
    "\n",
    "# Initialize Q-network parameters\n",
    "num_features = D[0][0].shape[0]\n",
    "num_actions = 2\n",
    "model = agent_network(num_features, num_actions)\n",
    "theta = model.get_weights()\n",
    "\n",
    "# Initialize index counter for hidden_layer_output_train_med\n",
    "idx = 0\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle training data\n",
    "    random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state= hidden_layer_output_train_med[0, 0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        action = epsilon_greedy_policy(state, epsilon, theta)\n",
    "        true_label = D[step][1]\n",
    "        predicted_label = action\n",
    "\n",
    "        # Get next state from hidden_layer_output_train_med\n",
    "        next_state = hidden_layer_output_train_med[idx, 0]\n",
    "        idx += 1\n",
    "\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label, lambda_val=0.1)\n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], model)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Convert states and next_states tuples into numpy arrays\n",
    "            states = np.array(states)\n",
    "            next_states = np.array(next_states)\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, model)[np.arange(batch_size), actions.astype(int)]\n",
    "            target_Q_values = np.array(target_Q_values)\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Backpropagation\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(model.trainable_variables)\n",
    "                predictions = Q(states, model)\n",
    "                loss = compute_loss(target_Q_values, predictions[np.arange(batch_size), actions.astype(int)])\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "        \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd89f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "num_episodes = 15\n",
    "max_steps = 7\n",
    "learning_rate = 0.5\n",
    "\n",
    "replay_memory_size = 20000\n",
    "num_features = D[0][0].shape[0]\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = epsilon\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "num_actions = 2\n",
    "\n",
    "# Define Q-network\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# Compile your Keras model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = model(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = model.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Reshape the state array to match the expected size\n",
    "    #state = np.reshape(state, (batch_size, num_features))\n",
    "    \n",
    "    #state = np.stack(states, axis=0)\n",
    "    #state = np.array(state)\n",
    "    #state = np.reshape(state, (batch_size, num_features))\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute predicted Q-values and loss\n",
    "            predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "            loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "            # Compute gradients\n",
    "            grad = np.gradient(loss, np.ravel(theta.T), axis=2)\n",
    "\n",
    "            # Reshape gradients to match the shape of theta\n",
    "            grad = grad.reshape(theta.shape)\n",
    "\n",
    "            # Update parameters using gradient descent\n",
    "            theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # Define hyperparameters\n",
    "# gamma = 0.85\n",
    "# epsilon = 0.1\n",
    "# batch_size = 128\n",
    "# num_episodes = 30\n",
    "# max_steps = 7\n",
    "# learning_rate = 0.5\n",
    "\n",
    "# replay_memory_size = 20000\n",
    "# num_features = D[0][0].shape[0]\n",
    "\n",
    "# D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# # Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "# tp = 0\n",
    "# tn = 0\n",
    "# fp = 0\n",
    "# fn = 0\n",
    "\n",
    "# # Initialize simulation environment\n",
    "# environment = epsilon\n",
    "\n",
    "# theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# # Define Q-network\n",
    "# input_shape = hidden_layer_output_train_med[0].shape\n",
    "# num_actions = 2\n",
    "\n",
    "# # Define Q-network\n",
    "\n",
    "# # Define the hidden layer model\n",
    "# hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "#                                             outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# # Get the activations of the hidden layer for the training data\n",
    "# hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# # Define a new model that takes the output of the hidden layer as input\n",
    "# new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "# x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "# output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "# agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# opt_new=Adam(lr=0.00517)\n",
    "\n",
    "# # Compile your Keras model\n",
    "# agent_network.compile(optimizer='opt_new',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Initialize replay memory\n",
    "# replay_memory = []\n",
    "\n",
    "# def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Reshape state to (1, num_features)\n",
    "#     state = np.reshape(state, (1, -1))\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    "#     return Q_values\n",
    "\n",
    "\n",
    "# def epsilon_greedy_policy(state, epsilon, model):\n",
    "#     if np.random.uniform() < epsilon:\n",
    "#         # Choose a random action\n",
    "#         action = np.random.randint(num_actions)\n",
    "#     else:\n",
    "#         # Choose the action with the highest Q-value\n",
    "#         Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "#         action = np.argmax(Q_values)\n",
    "#     return action\n",
    "\n",
    "# def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "#     # Initialize terminal flag\n",
    "#     terminal = 0\n",
    "#     # Fraud class\n",
    "#     if true_label == 1:\n",
    "#         if action == true_label:\n",
    "#             reward = 1\n",
    "#         else:\n",
    "#             reward = -1\n",
    "#             terminal = 1\n",
    "#     # Not fraud class\n",
    "#     else:\n",
    "#         if action == true_label:\n",
    "#             reward = lambda_val\n",
    "#         else:\n",
    "#             reward = -lambda_val\n",
    "#     return reward, terminal\n",
    "\n",
    "# # Define function for computing loss\n",
    "# def compute_loss(y, Q_values):\n",
    "#     return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# # Start training\n",
    "# for episode in range(num_episodes):\n",
    "#     # Shuffle training data\n",
    "#     np.random.shuffle(D)\n",
    "#     print(\"Episode \", episode)\n",
    "#     print(\"--------------------------------------------\")\n",
    "    \n",
    "#     # Initialize state\n",
    "#     state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "#     # Start episode\n",
    "#     for step in range(max_steps):\n",
    "#         # Choose action\n",
    "#         action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "#         # Get true label\n",
    "#         true_label = D[step][1]\n",
    "        \n",
    "#         # Predict label\n",
    "#         predicted_label = action\n",
    "        \n",
    "#         # Get next state\n",
    "#         next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "#         # Calculate reward and terminal flag\n",
    "#         reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "#         print(\"Step:\", step)\n",
    "#         print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "#         print(\"Reward:\", reward)\n",
    "#         print(\"\")\n",
    "        \n",
    "#         # Update counters for precision and accuracy\n",
    "#         if true_label == 1:\n",
    "#             if predicted_label == 1:\n",
    "#                 tp += 1\n",
    "#             else:\n",
    "#                 fn += 1\n",
    "#         else:\n",
    "#             if predicted_label == 1:\n",
    "#                 fp += 1\n",
    "#             else:\n",
    "#                 tn += 1\n",
    "        \n",
    "\n",
    "#         # Store experience in memory\n",
    "#         replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "#         # Sample a batch of experiences from memory\n",
    "#         if len(replay_memory) >= batch_size:\n",
    "#             batch = random.sample(replay_memory, batch_size)\n",
    "#             states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "#             # Convert actions tuple into numpy array\n",
    "#             actions = np.array(actions)\n",
    "\n",
    "#             # Compute target Q-values\n",
    "#             target_Q_values = []\n",
    "#             for i in range(batch_size):\n",
    "#                 if terminals[i]:\n",
    "#                     target_Q_values.append(rewards[i])\n",
    "#                 else:\n",
    "#                     next_Q_values = Q(next_states[i], theta)\n",
    "#                     target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "#             # Compute predicted Q-values and loss\n",
    "#             predicted_Q_values = Q(states, theta)[np.arange(batch_size), actions.astype(int)]\n",
    "#             loss = compute_loss(target_Q_values, predicted_Q_values)\n",
    "\n",
    "#             # Compute gradients\n",
    "#             grad = np.gradient(loss, np.ravel(theta.T), axis=2)\n",
    "\n",
    "#             # Reshape gradients to match the shape of theta\n",
    "#             grad = grad.reshape(theta.shape)\n",
    "\n",
    "#             # Update parameters using gradient descent\n",
    "#             theta -= grad * learning_rate\n",
    "        \n",
    "      \n",
    "#         # Update state\n",
    "#         state = next_state\n",
    "        \n",
    "#         # Check if episode is finished\n",
    "#         if terminal==1:\n",
    "#             break\n",
    "            \n",
    "# # Calculate precision and accuracy\n",
    "# precision = tp / (tp + fp)\n",
    "# accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.85\n",
    "epsilon = 0.1\n",
    "#batch_size = 128\n",
    "replay_memory_size = 1000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 5\n",
    "learning_rate = 0.5\n",
    "\n",
    "replay_memory_size = 20000\n",
    "num_features = D[0][0].shape[0]\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = epsilon\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "num_actions = 2\n",
    "\n",
    "# Define Q-network\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "model= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "# Compile your Keras model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = model(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = model.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac057377",
   "metadata": {},
   "source": [
    "# FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d275b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "#Extraction of hidden layer\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.0011)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all data points in the training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Randomly initialize parameters θ\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d1d34",
   "metadata": {},
   "source": [
    "## FInal New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3337e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "# Randomly initialize parameters θ\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "# new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "# reshaped_input_med = keras.layers.Lambda(lambda x: x, output_shape=(1, 10))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "    \n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    \n",
    "    # state = np.reshape(state, (batch_size, -1))\n",
    "    \n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "       # Initialize parameters\n",
    "        theta = agent_network.trainable_variables\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acef758",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_output_train_med.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b83c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.75\n",
    "epsilon = 0.1\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.41\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.425\n",
    "epsilon = 0.2\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.91\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddaae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.5\n",
    "epsilon = 0.1\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 35\n",
    "max_steps = 4\n",
    "learning_rate = 0.13\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "        print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa2419",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.90\n",
    "epsilon = 0.3\n",
    "batch_size = 128\n",
    "learning_rate = 0.61\n",
    "\n",
    "max_steps = 5\n",
    "num_episodes = 20\n",
    "replay_memory_size = 20000\n",
    "\n",
    "# \n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.425\n",
    "epsilon = 0.2\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.91\n",
    "\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions=2\n",
    "\n",
    "# Initialize counters for true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# Initialize simulation environment\n",
    "environment = None\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "theta = np.zeros((num_features, num_actions))\n",
    "\n",
    "# Define Q-network\n",
    "input_shape = hidden_layer_output_train_med[0].shape\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define a new model that takes the output of the hidden layer as input\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],))\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_regularizer=keras.regularizers.l1(0.0000611))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= Adam(lr=0.00061)\n",
    "\n",
    "# Compile your Keras model\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = []\n",
    "\n",
    "def Q(state, theta):\n",
    "    # Convert state to numpy array\n",
    "    state = np.array(state)\n",
    "    # Reshape state to (1, num_features)\n",
    "    state = np.reshape(state, (1, -1))\n",
    "    # Compute Q-values using the network\n",
    "    Q_values = agent_network(state).numpy()[0]\n",
    "    return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, model):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Define function for computing loss\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start training\n",
    "for episode in range(num_episodes):\n",
    "    # Shuffle training data\n",
    "    np.random.shuffle(D)\n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action\n",
    "        action = epsilon_greedy_policy(state, epsilon, model)\n",
    "        \n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        \n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "        \n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "                    \n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = model(states)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "            gradients = tape.gradient(loss, agent_network.trainable_variables)\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, agent_network.trainable_variables))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "    \n",
    "      \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb09f80",
   "metadata": {},
   "source": [
    "# Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15570cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the values for the pie chart\n",
    "sizes = [44,47,3,6]\n",
    "\n",
    "# Define the labels for each value\n",
    "labels = ['True label: 1, Prediction:1', 'True label: 0, Prediction:0', 'True label: 1, Prediction:0', 'True label: 0, Prediction:1']\n",
    "\n",
    "# Define the colors for each value\n",
    "colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\n",
    "\n",
    "# Create the pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Draw a circle at the center of the pie chart\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Add a legend to the pie chart\n",
    "ax1.legend(labels, loc=\"best\")\n",
    "\n",
    "# Set the title of the pie chart\n",
    "plt.title(\"My Pie Chart\")\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network, to_file='a_new.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8acefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 20\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "\n",
    "def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    " # Set the weights and biases of the network to the values in theta\n",
    "    agent_network.set_weights(theta)\n",
    "    \n",
    "    # Pass the state through the network to get the Q-values for each action\n",
    "    q_values = agent_network.predict(np.array([state]))\n",
    "    \n",
    "    # Return the Q-values as a numpy array\n",
    "    return q_values\n",
    "    #return Q_values\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                if terminals[i]:\n",
    "                    target_Q_values.append(rewards[i])\n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    # target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "                    # target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "                    target_Q_values.append(np.mean((reward + gamma * np.max(Q_star[next_state, :]))))\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # loss = compute_loss(target_Q_values, selected_Q_values)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fa7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Q(state, theta):\n",
    "#     # Convert state to numpy array\n",
    "#     state = np.array(state)\n",
    "    \n",
    "#     # Compute Q-values using the network\n",
    "#     Q_values = agent_network(state).numpy()[0]\n",
    "#     return Q_values\n",
    "\n",
    "# def compute_optimal_Q(next_states, rewards, terminals, gamma, Q_star):\n",
    "#     target_Q_values = []\n",
    "#     for i in range(batch_size):\n",
    "#         if terminals[i]:\n",
    "#             target_Q_values.append(rewards[i])\n",
    "#         else:\n",
    "#             next_Q_values = Q_star[next_states[i]]\n",
    "#             target_Q_values.append(rewards[i] + gamma * np.max(next_Q_values))\n",
    "#     return target_Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2265f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.8 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 5\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )\n",
    "# Policy function\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Rearding the agent\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "\n",
    "            # Compute target Q-values: This code block is necessary to update the Q-network so that \n",
    "            # it accurately predicts the Q-values for each state-action pair.\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                # if the next state is a terminal state (i.e., the episode is over), then the value of the \n",
    "                # state-action pair is simply the immediate reward received.\n",
    "                if terminals[i]:\n",
    "                    \n",
    "                    target_Q_values.append(rewards[i])\n",
    "                    \n",
    "                # If the next state is not a terminal state, then the value of the state-action pair is the immediate reward received plus the discounted value of the best action in the next state,\n",
    "                # which is computed using the deep neural network with the previous set of parameters, denoted as θk-1.\n",
    "                \n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values\n",
    "                    target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "                \n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision and accuracy\n",
    "precision = tp / (tp + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c423cc",
   "metadata": {},
   "outputs": [],
   "source": [
    " theta = agent_network.trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f7c34",
   "metadata": {},
   "source": [
    "# Working in 3 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf46f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D72E7FF0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001D72E7FF0D8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "31600/31600 [==============================] - 31s 986us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Define the hidden layer model\n",
    "hidden_layer_model_med = keras.models.Model(inputs=mediator_network.input,\n",
    "                                            outputs=mediator_network.layers[1].output)\n",
    "\n",
    "# Get the activations of the hidden layer for the training data\n",
    "hidden_layer_output_train_med = hidden_layer_model_med.predict(hidden_layer_output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bc3bf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "\n",
    "# Define the Q-network called 'agent_network'(to approximate the Q-function)\n",
    "new_model_input_med = keras.layers.Input(shape=(hidden_layer_output_train_med.shape[1],)) # shape (batch_size, number features)= (32,10)\n",
    "\n",
    "# The Reshape layer is needed to reshape the input to a format that can be fed into the next layer of the network.\n",
    "# Reshape((1, -1))  is used to reshape the input from shape (batch_size, 10) to (batch_size, 1, 10)\n",
    "reshaped_input_med = keras.layers.Reshape((1, -1))(new_model_input_med)\n",
    "\n",
    "x = keras.layers.Dense(10, activation='tanh',kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05), kernel_regularizer=keras.regularizers.l1(0.000811))(reshaped_input_med)\n",
    "\n",
    "output_med = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "agent_network= keras.Model(inputs=new_model_input_med, outputs=output_med)\n",
    "\n",
    "opt_new= keras.optimizers.Adam(lr=0.00061)\n",
    "\n",
    "# Compile the agent_network\n",
    "agent_network.compile(optimizer=opt_new,\n",
    "              loss='mse'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fd6057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  1\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  2\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  3\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  4\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  5\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  6\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  7\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  8\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  9\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  10\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  11\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  12\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  13\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  14\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  15\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  16\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  17\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  18\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 0\n",
      "Reward: -1\n",
      "\n",
      "Episode  19\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  20\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  21\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  22\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  23\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  24\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  25\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Episode  26\n",
      "--------------------------------------------\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 0\n",
      "Reward: 0.1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  27\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  28\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Step: 0\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Step: 1\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 1 . Agent has predicted: 1\n",
      "Reward: 1\n",
      "\n",
      "Episode  29\n",
      "--------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 0\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 1\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Step: 2\n",
      "True label is 0 . Agent has predicted: 1\n",
      "Reward: -0.1\n",
      "\n",
      "Precision: 0.41379310344827586\n",
      "Recall: 0.9473684210526315\n",
      "F1-score: 0.576\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 36\n",
      "fp: 51\n",
      "tn: 1\n",
      "fn: 2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set hyperparameters\n",
    "gamma = 0.5 # discount factor\n",
    "epsilon = 0.1 #exploration rate\n",
    "replay_memory_size = 20000\n",
    "batch_size = 128\n",
    "num_episodes = 30\n",
    "max_steps = 3\n",
    "learning_rate = 0.44\n",
    "\n",
    "# Counter initialization: true positives, true negatives, false positives, and false negatives\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "# hidden_layer_output_train_med contains the output of the hidden layer of the mediator network for all points in training set. \n",
    "# y_train_resampled_final contains the corresponding labels of the training set.\n",
    "D = list(zip(hidden_layer_output_train_med, y_train_resampled_final))\n",
    "\n",
    "num_features = 10\n",
    "num_actions = 2 #number of possible actions (either fraud or non-fraud)\n",
    "\n",
    "# Randomly initialize paramter θ: theta represents the weights and biases of the network.\n",
    "theta = np.random.randn(num_features, num_actions)\n",
    "\n",
    "# Initialize replay memory\n",
    "replay_memory = [replay_memory_size]\n",
    "\n",
    "# Initialize a list to store the actions taken\n",
    "actions = []\n",
    "\n",
    "# Aprroximation Q-function via Neural Network\n",
    "def Q(state, theta):\n",
    "\n",
    "    # Set the weights and biases of the network to the values in theta\n",
    "    agent_network.set_weights(theta)\n",
    "    \n",
    "    # Pass the state through the network to get the Q-values for each action\n",
    "    q_values = agent_network.predict(np.array([state]))\n",
    "    \n",
    "    # Return the Q-values as a numpy array\n",
    "    return q_values\n",
    "\n",
    "# Policy function: Action selection\n",
    "def epsilon_greedy_policy(state, epsilon, agent_network):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        # Choose a random action\n",
    "        action = np.random.randint(num_actions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        Q_values = agent_network.predict(state[np.newaxis])[0]\n",
    "        action = np.argmax(Q_values)\n",
    "    return action\n",
    "\n",
    "# Rearding the agent\n",
    "def reward_fn(action, true_label, predicted_label, lambda_val=0.1):\n",
    "    # Initialize terminal flag\n",
    "    terminal = 0\n",
    "    # Fraud class\n",
    "    if true_label == 1:\n",
    "        if action == true_label:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            terminal = 1\n",
    "    # Not fraud class\n",
    "    else:\n",
    "        if action == true_label:\n",
    "            reward = lambda_val\n",
    "        else:\n",
    "            reward = -lambda_val\n",
    "    return reward, terminal\n",
    "\n",
    "# Function for computing loss: Compute the mean of the squared difference between the \n",
    "# target Q-values and the predicted Q-values for a batch of training samples.\n",
    "# Target Q-values: Computed using Bellman euqation, Predicted Q-values: Q-values predicted by the Q-network for the current state & action.\n",
    "def compute_loss(y, Q_values):\n",
    "    return tf.reduce_mean(tf.square(y - Q_values))\n",
    "\n",
    "# Start agent training\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # Shuffle the input data\n",
    "    np.random.shuffle(D)\n",
    "    \n",
    "    print(\"Episode \", episode)\n",
    "    print(\"--------------------------------------------\")\n",
    "    \n",
    "    # Initialize state\n",
    "    state = hidden_layer_output_train_med[0]\n",
    "    \n",
    "    # Start episode\n",
    "    for step in range(max_steps):\n",
    "        # Choose action based on epsilon greedy algorithm\n",
    "        action = epsilon_greedy_policy(state, epsilon, agent_network)\n",
    "        \n",
    "        # Append the action to the 'actions' list\n",
    "        actions.append(action)\n",
    "        \n",
    "        # Get true label y_train_resampled_final (This is present in D[step][1])\n",
    "        true_label = D[step][1]\n",
    "        \n",
    "        # Predict label\n",
    "        predicted_label = action\n",
    "        \n",
    "        # Calculate reward and terminal flag\n",
    "        reward, terminal = reward_fn(action, true_label, predicted_label)\n",
    "        \n",
    "        print(\"Step:\", step)\n",
    "        print(\"True label is\", true_label, \". Agent has predicted:\", predicted_label)\n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Get next state\n",
    "        next_state = hidden_layer_output_train_med[step+1] if step < max_steps - 1 else state\n",
    "        \n",
    "        # Update counters for precision and accuracy\n",
    "        if true_label == 1:\n",
    "            if predicted_label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted_label == 1:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "        # Store experience in memory\n",
    "        replay_memory.append((state, action, reward, next_state, terminal))\n",
    "\n",
    "        # Sample a batch of experiences from memory\n",
    "        if len(replay_memory) >= batch_size:\n",
    "            batch = random.sample(replay_memory, batch_size)\n",
    "            #if all(isinstance(item, tuple) for item in batch): # Check if all items in the batch are tuples\n",
    "            states, actions, rewards, next_states, terminals = zip(*batch)\n",
    "\n",
    "            # Convert actions tuple into numpy array\n",
    "            actions = np.array(actions)\n",
    "            #else:\n",
    "                #continue # Skip this iteration if batch contains non-tuple items\n",
    "\n",
    "            # Compute target Q-values: This code block is necessary to update the Q-network so that \n",
    "            # it accurately predicts the Q-values for each state-action pair.\n",
    "            target_Q_values = []\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                # if the next state is a terminal state (i.e., the episode is over), then the value of the \n",
    "                # state-action pair is simply the immediate reward received.\n",
    "                if terminals[i]:\n",
    "                    \n",
    "                    target_Q_values.append(rewards[i])\n",
    "                    \n",
    "                # If the next state is not a terminal state, then the value of the state-action pair is the immediate reward received plus the discounted value of the best action in the next state,\n",
    "                # which is computed using the deep neural network with the previous set of parameters, denoted as θk-1.\n",
    "                \n",
    "                else:\n",
    "                    next_Q_values = Q(next_states[i], theta)\n",
    "\n",
    "                    # Update Q-values via the optimal Bellman\n",
    "                    target_Q_values.append (np.mean(rewards + gamma * np.max(next_Q_values, axis=1)))\n",
    "            \n",
    "            # Train the agent_network on the batch of data\n",
    "            hist = agent_network.train_on_batch(np.array(states), np.array(target_Q_values))\n",
    "            \n",
    "            # Compute loss and gradients\n",
    "            with tf.GradientTape() as tape:\n",
    "                Q_values = agent_network(states, theta)\n",
    "                selected_Q_values = tf.reduce_sum(Q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                loss = compute_loss(tf.constant(target_Q_values, dtype=tf.float32), selected_Q_values)\n",
    "            gradients = tape.gradient(loss, theta)\n",
    "\n",
    "            # Update parameters theta\n",
    "            for i in range(len(theta)):\n",
    "                theta[i].assign_sub(learning_rate * gradients[i])\n",
    "\n",
    "            # Apply gradients to update weights\n",
    "            optimizer.apply_gradients(zip(gradients, theta))\n",
    "\n",
    "            # Clear replay memory\n",
    "            replay_memory.clear()    \n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if episode is finished\n",
    "        if terminal==1:\n",
    "            break\n",
    "            \n",
    "# Calculate precision, recall, F1-score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "F1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", F1_score)\n",
    "\n",
    "# Plot the distribution of actions\n",
    "plt.hist(actions, bins=range(num_actions+1), align='left', rwidth=0.8)\n",
    "plt.xticks(range(num_actions))\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Distribution')\n",
    "plt.show()\n",
    "\n",
    "print(\"tp:\", tp)\n",
    "print(\"fp:\", fp)\n",
    "print(\"tn:\", tn)\n",
    "print(\"fn:\", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(agent_network , to_file='a_network .png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the values for the pie chart\n",
    "sizes = [44.7,47.4,3.48,4.42]\n",
    "\n",
    "# Define the labels for each value\n",
    "labels = ['True label: 1, Prediction:1','True label: 0, Prediction:0', 'True label: 1, Prediction:0', 'True label: 0, Prediction:1']\n",
    "\n",
    "# Define the colors for each value\n",
    "colors = ['#ffcc99','#40e0d0','#66b3ff','#ff9999']\n",
    "\n",
    "# Create the pie chart\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, colors=colors, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "\n",
    "# Draw a circle at the center of the pie chart\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Move the legend to the right of the pie chart\n",
    "ax1.legend(labels, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set the title of the pie chart\n",
    "plt.title(\"Performance of the Q-Fraud Detection System\")\n",
    "\n",
    "# Show the pie chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0855b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_network.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
