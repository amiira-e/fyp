{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb27b30e",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddd607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61dd7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e620b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of column 'A' from float64 to float32\n",
    "df['amount'] = df['amount'].astype('float32')\n",
    "df['oldbalanceOrg'] = df['oldbalanceOrg'].astype('float32')\n",
    "df['oldbalanceDest'] = df['oldbalanceDest'].astype('float32')\n",
    "df['newbalanceOrig'] = df['newbalanceOrig'].astype('float32')\n",
    "df['newbalanceDest'] = df['newbalanceDest'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806044b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step'] = df['step'].astype('int32')\n",
    "df['isFlaggedFraud'] = df['isFlaggedFraud'].astype('int32') \n",
    "df['isFraud'] = df['isFraud'].astype('int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a814c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LabelEncoder from Sklearn\n",
    "# library from preprocessing Module.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Creating a instance of label Encoder.\n",
    "le = LabelEncoder()\n",
    "# Using .fit_transform function to fit label\n",
    "# encoder and return encoded label\n",
    "label = le.fit_transform(df['type'])\n",
    "# printing label\n",
    "label\n",
    "# removing the column 'type' from df\n",
    "# as it is of no use now.\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "# Appending the array to our dataFrame\n",
    "# with column name 'type'\n",
    "df[\"type\"] = label\n",
    "# printing Dataframe\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1820dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameDest'])\n",
    "label\n",
    "df.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df[\"nameDest\"] = label\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e9a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameOrig'])\n",
    "label\n",
    "df.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df[\"nameOrig\"] = label\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f87e3",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=18)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0329e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of column 'A' from float64 to float32\n",
    "X_train['amount'] = X_train['amount'].astype('float32')\n",
    "X_train['oldbalanceOrg'] =X_train['oldbalanceOrg'].astype('float32')\n",
    "X_train['oldbalanceDest'] = X_train['oldbalanceDest'].astype('float32')\n",
    "X_train['newbalanceOrig'] = X_train['newbalanceOrig'].astype('float32')\n",
    "X_train['newbalanceDest'] = X_train['newbalanceDest'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['step'] = X_train['step'].astype('int32')\n",
    "X_train['isFlaggedFraud'] = X_train['isFlaggedFraud'].astype('int32') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6669395",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set up the resampling methods\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=42)\n",
    "rus = RandomUnderSampler(sampling_strategy=0.7, random_state=42)\n",
    "\n",
    "# Apply the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_resampled)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "\n",
    "# Assume X_train and y_train are the original training data\n",
    "# resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_train_resampled)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985706cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a235616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_train_resampled_new)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f201086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = np.bincount(y_train_resampled_final)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cd300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assuming y_train is a numpy array or a pandas series\n",
    "counts = np.bincount(y_test)\n",
    "print(\"Class 0 count:\", counts[0])\n",
    "print(\"Class 1 count:\", counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5613db3",
   "metadata": {},
   "source": [
    "## Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663c31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.14, 'oldbalanceOrg': 0.24, 'newbalanceOrig': 0.21, 'oldbalanceDest': 0.2, 'newbalanceDest': 0.22}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train_resampled_final.index, size=len(X_train_resampled_final), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train_resampled_final.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train_resampled_final.loc[X_train_resampled_final[col_name] > np.percentile(X_train_resampled_final[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "\n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119cad68",
   "metadata": {},
   "source": [
    "## Linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Create a Perceptron object\n",
    "clf = Perceptron(random_state=0)\n",
    "\n",
    "# Train the Perceptron on the data\n",
    "clf.fit(X_train_resampled_final, y_train_resampled_final)\n",
    "\n",
    "# Predict the output classes for the data points\n",
    "y_pred = clf.predict(X_train_resampled_final)\n",
    "\n",
    "# Check if the Perceptron correctly classified all the data points\n",
    "if np.all(y_pred == y_train_resampled_final):\n",
    "    print(\"Data is linearly separable\")\n",
    "else:\n",
    "    print(\"Data is not linearly separable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your original training data is stored in a pandas DataFrame called X_train\n",
    "# And assuming you have a list of selected feature names called selected_features\n",
    "selected_features = ['oldbalanceOrg', 'newbalanceOrig', 'type', 'amount']\n",
    "X_train_selected = X_train_resampled_final[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_selected=X_test[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf5ef1",
   "metadata": {},
   "source": [
    "## Create kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cc23d",
   "metadata": {},
   "source": [
    "we use kerne function to create covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a 1-D kernel with default parameters\n",
    "k=GPy.kern.RBF(1)\n",
    "#Preview of parameters\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22148d11",
   "metadata": {},
   "source": [
    "## Visualize kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32690361",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-4.,4.,100)[:,None] # we need none to reshape x into a column vector to use in Gaussian\n",
    "\n",
    "#first, sample kernel at x(dash)=0\n",
    "K=k.K(x,np.array([[0.]])) # k(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,K)\n",
    "plt.title(\"$\\kappa_{rbf}{x,x'}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babca6dc",
   "metadata": {},
   "source": [
    "## Lengthscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd86f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import rc\n",
    "ls = [0.05, 0.25, 0.5, 1., 2., 4]\n",
    "\n",
    "def update(iteration):\n",
    "    ax.cla()\n",
    "    k = GPy.kern.RBF(1)\n",
    "    k. lengthscale = ls[iteration]\n",
    "    # Calculate the new covariance function at k(x,0)\n",
    "    C = k.K(x, np.array([[0.]]))\n",
    "    # Plot the resulting covariance vector\n",
    "    ax.plot(x,C)\n",
    "    ax.set_title(f\"$\\kappa_{{rbf}}(x,x')$ Length scale = {k.lengthscale[0]}\")\n",
    "    ax.set_ylim((0, 1.2))\n",
    "\n",
    "num_iterations = len(ls)\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500)\n",
    "plt.close()\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef37b2",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b85ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import rc\n",
    "variances = [0.05, 0.25, 0.5, 1., 2., 4]\n",
    "\n",
    "def update(iteration):\n",
    "    ax.cla()\n",
    "    k = GPy.kern.RBF(1)\n",
    "    k. variance = variances[iteration]\n",
    "    # Calculate the new covariance function at k(x,0)\n",
    "    C = k.K(x, np.array([[0.]]))\n",
    "    # Plot the resulting covariance vector\n",
    "    ax.plot(x,C)\n",
    "    ax.set_title(f\"$\\kappa_{{rbf}}(x,x')$ Variance = {k.variance[0]}\")\n",
    "    ax.set_ylim((0, 1.2))\n",
    "\n",
    "num_iterations = len(ls)\n",
    "anim = FuncAnimation(fig, update, frames=np.arange(0, num_iterations-1, 1), interval=500)\n",
    "plt.close()\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b09e1b",
   "metadata": {},
   "source": [
    "## Kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Define the RBF kernel with gamma=1.0\n",
    "kernel = 1.0 * RBF(length_scale=1.0)\n",
    "\n",
    "# Create a grid of points to evaluate the kernel function\n",
    "x = np.linspace(-5, 5, 100)\n",
    "X = np.atleast_2d(x).T\n",
    "\n",
    "# Compute the kernel matrix\n",
    "K = kernel(X)\n",
    "\n",
    "# Plot the kernel function\n",
    "plt.figure()\n",
    "plt.plot(x, K[0], label='RBF kernel')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('k(x, 0)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686d76d",
   "metadata": {},
   "source": [
    "## Train Gaussian Process Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5705238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdca105",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([     0,      1,      2,      4,      5,      6,      7,      8,\\n                10,     13,\\n            ...\\n            799983, 799985, 799986, 799987, 799989, 799992, 799994, 799995,\\n            799996, 799998],\\n           dtype='int64', length=640000)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17940\\3512111634.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Split the data into training and testing sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mX_train_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mX_test_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3462\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test2\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1372\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([     0,      1,      2,      4,      5,      6,      7,      8,\\n                10,     13,\\n            ...\\n            799983, 799985, 799986, 799987, 799989, 799992, 799994, 799995,\\n            799996, 799998],\\n           dtype='int64', length=640000)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Define the Gaussian process classifier and set hyperparameters\n",
    "gp = GaussianProcessClassifier(kernel=None, n_restarts_optimizer=10, random_state=42)\n",
    "\n",
    "# Define the cross-validation method\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold, (train_index, test_index) in enumerate(kfold.split(X, y)):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_new, y_train_new = X[train_index], y[train_index]\n",
    "    X_test_new, y_test_new = X[test_index], y[test_index]\n",
    "\n",
    "    # Train the Gaussian process classifier\n",
    "    gp.fit(X_train_new, y_train_new)\n",
    "\n",
    "    # Evaluate the performance of the model on the testing data\n",
    "    y_pred = gp.predict(X_test_new)\n",
    "    y_prob = gp.predict_proba(X_test_new)[:, 1]\n",
    "    report = classification_report(y_test_new, y_pred)\n",
    "    auc = roc_auc_score(y_test_new, y_prob)\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"Classification report:\\n{report}\")\n",
    "    print(f\"AUC: {auc:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a38eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kfold.split(X_train_selected, y_train_resampled_final)):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, y_train = X_train_selected[train_index], y_train_resampled_final[train_index]\n",
    "    X_test, y_test = X_train_selected[test_index], y_train_resampled_final[test_index]\n",
    "\n",
    "    # Train the Gaussian process classifier\n",
    "    gp.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the performance of the model on the testing data\n",
    "    y_pred = gp.predict(X_test)\n",
    "    y_prob = gp.predict_proba(X_test)[:, 1]\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"Fold {fold}:\")\n",
    "    print(f\"Classification report:\\n{report}\")\n",
    "    print(f\"AUC: {auc:.3f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
