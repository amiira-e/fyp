{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f481781",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b218896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pywedge as pw\n",
    "import bqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1666793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85abffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data type of column 'A' from float64 to float32\n",
    "df['amount'] = df['amount'].astype('float32')\n",
    "df['oldbalanceOrg'] = df['oldbalanceOrg'].astype('float32')\n",
    "df['oldbalanceDest'] = df['oldbalanceDest'].astype('float32')\n",
    "df['newbalanceOrig'] = df['newbalanceOrig'].astype('float32')\n",
    "df['newbalanceDest'] = df['newbalanceDest'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['step'] = df['step'].astype('int32')\n",
    "df['isFlaggedFraud'] = df['isFlaggedFraud'].astype('int32') \n",
    "df['isFraud'] = df['isFraud'].astype('int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# filter the data to only include fraud transactions\n",
    "fraud_df = df[df['isFraud'] == 1]\n",
    "\n",
    "# group the fraud data by payment type and count the number of transactions in each group\n",
    "fraud_counts = fraud_df.groupby('type').size()\n",
    "\n",
    "# plot a bar chart to visualize the fraud counts by payment type\n",
    "fraud_counts.plot(kind='bar',color='gray',edgecolor =\"gray\")\n",
    "\n",
    "# set the chart title and axes labels\n",
    "plt.title('Count of fraudulent transactions per payment type')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# filter the data to only include fraud transactions\n",
    "fraud_df = df[df['isFraud'] == 1]\n",
    "\n",
    "# group the fraud data by payment type and count the number of transactions in each group\n",
    "fraud_counts = fraud_df.groupby('type').size()\n",
    "\n",
    "# plot a bar chart to visualize the fraud counts by payment type\n",
    "ax = fraud_counts.plot(kind='bar', color='gray', edgecolor='gray')\n",
    "\n",
    "# set the chart title and axes labels\n",
    "plt.title('Count of fraudulent transactions per payment type')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# display the count values over the top of each bar\n",
    "for i, v in enumerate(fraud_counts.values):\n",
    "    ax.text(i, v, str(v), ha='center', va='bottom')\n",
    "\n",
    "# display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"amount\", y=\"newbalanceOrig\", hue=\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99caf834",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"amount\", y=\"oldbalanceOrg\", hue=\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"amount\", y=\"oldbalanceDest\", hue=\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a83eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"amount\", y=\"newbalanceDest\", hue=\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828730b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db742e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_out_steps = df.loc[df['type'] == 'CASH_OUT', 'step'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8533d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cash_out_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1357e",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3fc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d02e13",
   "metadata": {},
   "source": [
    "## Check for any duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2774548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "print(duplicates)\n",
    "\n",
    "# subset DataFrame to show only duplicate rows\n",
    "duplicates_df = df[duplicates]\n",
    "print(duplicates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94e0fa0",
   "metadata": {},
   "source": [
    "## Check for class imbalance in target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isFraud.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f508ab",
   "metadata": {},
   "source": [
    "## Unimodial vs Multimodial (Shape distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fa629",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "plt.figure(figsize=(20,8))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for feature in features:\n",
    "    plt.subplot(2,3,features.index(feature)+1)\n",
    "    sns.distplot(df[feature],hist=True,color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ed10b",
   "metadata": {},
   "source": [
    "## Graph for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import font\n",
    "counts = df['isFraud'].value_counts().rename_axis('isFraud').reset_index(name='count')\n",
    "fig = plt.figure(figsize=(5.5, 5.5))\n",
    "ax = sns.barplot(x='isFraud', y='count', data=counts,color='gray', edgecolor =\"gray\")\n",
    "\n",
    "# Define Function in which we will pass new width\n",
    "def Width(ax, new_width) :\n",
    "    for patch in ax.patches :\n",
    "        current_width = patch.get_width()\n",
    "        difference = current_width - new_width\n",
    "        # Set new width\n",
    "        patch.set_width(new_width)\n",
    "        # Now Recenter the Bars\n",
    "        patch.set_x(patch.get_x() + difference * .5)\n",
    "        ax.tick_params(bottom=False,left=False)\n",
    "        \n",
    "#Adjust width to 0.4\n",
    "Width(ax, 0.4)\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.title('Count of fraudulent and non-fraudulent transactions',fontsize =10,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f249142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import font\n",
    "\n",
    "counts = df['isFraud'].value_counts().rename_axis('isFraud').reset_index(name='count')\n",
    "\n",
    "fig = plt.figure(figsize=(5.5, 5.5))\n",
    "ax = sns.barplot(x='isFraud', y='count', data=counts, color='gray', edgecolor='gray')\n",
    "\n",
    "# Define Function in which we will pass new width\n",
    "def Width(ax, new_width):\n",
    "    for patch in ax.patches:\n",
    "        current_width = patch.get_width()\n",
    "        difference = current_width - new_width\n",
    "        # Set new width\n",
    "        patch.set_width(new_width)\n",
    "        # Now Recenter the Bars\n",
    "        patch.set_x(patch.get_x() + difference * .5)\n",
    "        ax.tick_params(bottom=False, left=False)\n",
    "\n",
    "# Adjust width to 0.4\n",
    "Width(ax, 0.4)\n",
    "\n",
    "# Add labels to the graph\n",
    "ax.set_xticklabels(['Non-fraudulent', 'Fraudulent'])\n",
    "\n",
    "ax.bar_label(ax.containers[0])\n",
    "\n",
    "plt.title('Count of fraudulent and non-fraudulent transactions', fontsize=10, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c321721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times 0 and 1 appear in the isFraud column\n",
    "counts = df['isFraud'].value_counts()\n",
    "\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a78d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( '\\nSkewness for data : ', skew(df.step))\n",
    "print( '\\nSkewness for data : ', skew(df.amount))\n",
    "print( '\\nSkewness for data : ', skew(df.oldbalanceOrg))\n",
    "print( '\\nSkewness for data : ', skew(df.newbalanceOrig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51d43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0eb6d2",
   "metadata": {},
   "source": [
    "## Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create New DataFrame with Count\n",
    "new_df = df[\"type\"].value_counts().rename_axis('types_of_transaction').reset_index(name='counts')\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set lables and values\n",
    "my_labels = new_df.types_of_transaction\n",
    "print(my_labels)\n",
    "my_values = new_df.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the pie chart\n",
    "fig=plt.figure(figsize=(4,4)) # Resize\n",
    "wp= {'linewidth':0.5,'edgecolor':\"black\"}\n",
    "ax=fig.add_axes([0,0,1,1]) # Add axis to the figure\n",
    "ax.axis('equal')\n",
    "explode=(0.1,0.1,0.1,0.1,0.1)\n",
    "ax.pie(my_values, labels=my_labels, autopct='%1.2f%%',explode=explode,shadow=True,wedgeprops=wp)\n",
    "font = {'fontname':'Comic Sans MS'} # Change font\n",
    "plt.title('Type of Transactions',fontsize=20,color='purple',**font,fontweight='bold')\n",
    "plt.legend(['CASH_OUT', 'PAYMENT', 'CASH_IN','TRANSFER','DEBIT'])\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd6c34",
   "metadata": {},
   "source": [
    "## Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04fd4ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of label Encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Using .fit_transform function to fit label and return encoded label\n",
    "label = le.fit_transform(df['type'])\n",
    "\n",
    "# removing the column 'type' from df as it is of no use now.\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "# Appending the array to our dataFrame with column name 'type'\n",
    "df[\"type\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb67412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameDest'])\n",
    "label\n",
    "df.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df[\"nameDest\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5cde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameOrig'])\n",
    "label\n",
    "df.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b93afe",
   "metadata": {},
   "source": [
    "## Stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12290e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998709\n",
      "1    0.001291\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998709\n",
      "1    0.001291\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.99871\n",
      "1    0.00129\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=18)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98752f82",
   "metadata": {},
   "source": [
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "\n",
    "# Assume X_train and y_train are the original training data\n",
    "# resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='auto')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)\n",
    "\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='auto')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='auto')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673b1a3",
   "metadata": {},
   "source": [
    "## Save train, test and validation sets to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\train.csv\", index=False)\n",
    "X_test.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33600bd5",
   "metadata": {},
   "source": [
    "## Skewness after encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821c2398",
   "metadata": {},
   "source": [
    "### Assess skewness in X_train in Stata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784cdab6",
   "metadata": {},
   "source": [
    "## Correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971994bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df.corr(method='pearson')\n",
    "plt.figure(figsize=(10,8)) # Resize\n",
    "# Plot the correlation matrix as a heatmap\n",
    "sns.heatmap(corr_matrix, cmap='mako', center=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1aae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = df.corr(method='spearman')\n",
    "plt.figure(figsize=(10,8)) # Resize\n",
    "# Plot the correlation matrix as a heatmap\n",
    "sns.heatmap(corr_matrix, cmap='mako', center=0, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb834f7",
   "metadata": {},
   "source": [
    "## Outliers labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931b1af",
   "metadata": {},
   "source": [
    "## Modified Z-Socre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd208af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import weibull_min, beta, lognorm, chi2,gamma,expon\n",
    "\n",
    "# Select the columns you want to check for outliers\n",
    "columns_to_trim = ['amount', 'oldbalanceOrg', 'oldbalanceDest', 'newbalanceOrig', 'newbalanceDest']\n",
    "\n",
    "# Calculate modified Z-score for each column\n",
    "for col in columns_to_trim:\n",
    "    # Extract column values\n",
    "    col_values = X_train[col].values\n",
    "    \n",
    "    # Replace 0 or negative values with a small positive value\n",
    "    #col_values[col_values <= 0] = 1e-9\n",
    "    col_values += 1e-9\n",
    "    col_values[col_values == 0] = 1e-9\n",
    "\n",
    "\n",
    "    # Fit distribution and extract relevant parameters\n",
    "    if col == 'amount' or col == 'oldbalanceOrg' or col == 'oldbalanceDest':\n",
    "        shape, loc, scale = weibull_min.fit(col_values, loc=0)\n",
    "        med = weibull_min.median(shape, loc=loc, scale=scale)\n",
    "        n = len(col_values)\n",
    "        MAD = np.median(np.abs(col_values - np.median(col_values)))\n",
    "        k = 1.4826 * MAD\n",
    "        wmad = k * np.power(np.log(2), 1/k) * med\n",
    "        mod_z_score = k * (col_values - med) / wmad\n",
    "    \n",
    "    elif col == 'newbalanceDest':\n",
    "        # Fit gamma distribution and extract relevant parameters\n",
    "        shape, loc, scale = gamma.fit(col_values, floc=0)\n",
    "        med = gamma.median(shape, loc=loc, scale=scale)\n",
    "        n = len(col_values)\n",
    "        MAD = np.median(np.abs(col_values - np.median(col_values)))\n",
    "        k = 1.4826 * MAD\n",
    "        wmad = k * np.power(np.log(2), 1/k) * med\n",
    "        if wmad == 0:\n",
    "            #mod_z_score = np.zeros(n)\n",
    "            mod_z_score = np.full(n, np.nan)\n",
    "        else:\n",
    "            mod_z_score = k * (col_values - med) / wmad\n",
    "\n",
    "    # Fit distribution and extract relevant parameters\n",
    "    elif col == 'newbalanceOrig':\n",
    "        col_values[col_values == 0] = 1e-200\n",
    "        # Fit exponential distribution and extract relevant parameters\n",
    "        loc, scale = expon.fit(col_values, floc=0)\n",
    "        med = expon.median(loc=loc, scale=scale)\n",
    "        n = len(col_values)\n",
    "        MAD = np.median(np.abs(col_values - np.median(col_values)))\n",
    "        if MAD == 0:\n",
    "            k = 1e-200\n",
    "        else:\n",
    "            k = 1.4826 * MAD\n",
    "        wmad = k * np.power(np.log(2), 1/k) * med\n",
    "        #mod_z_score = k * (col_values - med) / wmad\n",
    "        mod_z_score = np.zeros(len(col_values))\n",
    "        mod_z_score[1:-1] = np.abs(col_values[1:-1] - np.median(col_values)) / (1.4826 * np.median(np.abs(col_values - np.median(col_values))) + 1e-9)\n",
    "   \n",
    "    # Count number of lower and upper outliers\n",
    "    lower_outliers = np.sum(mod_z_score < -2.5)\n",
    "    upper_outliers = np.sum(mod_z_score > 2.5)\n",
    "    \n",
    "    #Print results\n",
    "    print(f\"Column {col}\")\n",
    "    print(f\"Number of lower outliers for column: {lower_outliers}\")\n",
    "    print(f\"Number of upper outliers: {upper_outliers}\")\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b64b3",
   "metadata": {},
   "source": [
    "## IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da732cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select the columns you want to check for outliers\n",
    "columns_to_trim = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Calculate IQR for each column\n",
    "q1 = X_train[columns_to_trim].quantile(0.25)\n",
    "q3 = X_train[columns_to_trim].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "# Determine the lower and upper boundaries for outliers\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "# Count the number of lower and upper outliers in each column\n",
    "num_lower_outliers = (X_train[columns_to_trim] < lower_bound).sum()\n",
    "num_upper_outliers = (X_train[columns_to_trim] > upper_bound).sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of lower outliers:\\n\",num_lower_outliers)\n",
    "print(f\"\\n\")\n",
    "print(f\"Number of upper outliers:\\n\",num_upper_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de5c70",
   "metadata": {},
   "source": [
    "## Estimate trimming proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f4e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract the columns of interest\n",
    "cols = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "data = X_train[cols]\n",
    "\n",
    "# Loop over each column and determine the direction of trimming\n",
    "for col in cols:\n",
    "    # Calculate the median and interquartile range of the column\n",
    "    median = np.median(data[col])\n",
    "    iqr = np.percentile(data[col], 75) - np.percentile(data[col], 25)\n",
    "\n",
    "    # Calculate the skewness of the column\n",
    "    skewness = data[col].skew()\n",
    "\n",
    "    # Determine whether to use symmetric or asymmetric trimming based on the skewness and IQR\n",
    "    if abs(skewness) > 1.5 or iqr > 2 * abs(median):\n",
    "        print(f\"For column {col}, the data is skewed and heavy-tailed, so asymmetric trimming may be appropriate.\")\n",
    "    else:\n",
    "        print(f\"For column {col}, the data is roughly symmetric, so symmetric trimming may be appropriate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e106ad",
   "metadata": {},
   "source": [
    "## Outlier hadling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8d2cc",
   "metadata": {},
   "source": [
    "## Trimmed Means and Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31002dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed (20)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.15, 'oldbalanceOrg': 0.24, 'newbalanceOrig': 0.27, 'oldbalanceDest': 0.23, 'newbalanceDest': 0.22}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train.index, size=len(X_train), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train.loc[X_train[col_name] > np.percentile(X_train[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "\n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4b523",
   "metadata": {},
   "source": [
    "## Save train and test data after handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02847d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\trainAFTERTREATINGOUT.csv\", index=False)\n",
    "X_test.to_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\testAFTERTREATINGOUT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe05f59",
   "metadata": {},
   "source": [
    "## Assess skewness on Stata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b90b52",
   "metadata": {},
   "source": [
    "## Feature selection using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602084b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier (n_estimators=150,random_state=18)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc786e",
   "metadata": {},
   "source": [
    "## Random Forest Impurity based feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78784028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the feature importance values\n",
    "importance_vals = rfc.feature_importances_\n",
    "\n",
    "# Sort importance values\n",
    "indices = np.argsort(importance_vals[::-1])\n",
    "\n",
    "# Plot the feature importance of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Random Forest Impurity based feature importance\")\n",
    "plt.bar(range(X.shape[1]), importance_vals[indices][::-1])\n",
    "\n",
    "plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylim([0, 0.2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_names_in_ = list(X_train.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
