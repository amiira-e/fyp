{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3802b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f5aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\FraudDetectionData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c41793",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d587a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create an instance of label Encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Using .fit_transform function to fit label and return encoded label\n",
    "label = le.fit_transform(df['type'])\n",
    "\n",
    "# removing the column 'type' from df as it is of no use now.\n",
    "df.drop(\"type\", axis=1, inplace=True)\n",
    "\n",
    "# Appending the array to our dataFrame with column name 'type'\n",
    "df[\"type\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e67667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameDest'])\n",
    "label\n",
    "df.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df[\"nameDest\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df['nameOrig'])\n",
    "label\n",
    "df.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a690c",
   "metadata": {},
   "source": [
    "### Stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=18)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc5af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of train and test sets\n",
    "print(\"Train set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape (X_test, y_test):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34ab22",
   "metadata": {},
   "source": [
    "## Handle outliers in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed (11)\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Specify columns with outliers\n",
    "cols_with_outliers = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n",
    "\n",
    "# Specify the number of bootstrapped samples to create per column\n",
    "num_samples = 50\n",
    "\n",
    "# Specify the right trimming proportions for each column\n",
    "trim_props = {'amount': 0.12, 'oldbalanceOrg': 0.24, 'newbalanceOrig': 0.245, 'oldbalanceDest': 0.29, 'newbalanceDest': 0.3}\n",
    "\n",
    "# Initialize empty dictionaries to store the trimmed means for each column\n",
    "train_trimmed_means = {}\n",
    "\n",
    "# Loop over the specified columns\n",
    "for col_name in cols_with_outliers:\n",
    "    \n",
    "    # Check if the trimming proportion for this column is 0\n",
    "    if trim_props[col_name] == 0:\n",
    "        # If so, skip this column and move on to the next one\n",
    "        continue\n",
    "    \n",
    "    # Initialize empty lists to store the bootstrapped samples and the trimmed means for the training set\n",
    "    train_bootstrapped_samples = []\n",
    "    train_trimmed_means_list = []\n",
    "    \n",
    "    # Loop over the number of desired samples\n",
    "    for i in range(num_samples):\n",
    "        # Randomly select indices from the column in the training set\n",
    "        train_sample_indices = np.random.choice(X_train.index, size=len(X_train), replace=True)\n",
    "        \n",
    "        # Create a bootstrapped sample by indexing into the column with the selected indices for the training set\n",
    "        train_sample = X_train.loc[train_sample_indices, col_name]\n",
    "        \n",
    "        # Append the bootstrapped samples to the list for the training set\n",
    "        train_bootstrapped_samples.append(train_sample)\n",
    "        \n",
    "        # Calculate the right trimmed mean of the bootstrapped sample for the training set\n",
    "        train_right_trimmed_mean = np.mean(train_sample[train_sample <= np.percentile(train_sample, 100*(1-trim_props[col_name]))])\n",
    "        train_trimmed_means_list.append(train_right_trimmed_mean)\n",
    "        \n",
    "    # Calculate the mean of the right trimmed means for the training set and add it to the dictionary\n",
    "    train_trimmed_means[col_name] = np.mean(train_trimmed_means_list)\n",
    "\n",
    "    # Replace the outliers in the training set with the trimmed means\n",
    "    X_train.loc[X_train[col_name] > np.percentile(X_train[col_name], 100*(1-trim_props[col_name])), col_name] = train_trimmed_means[col_name]\n",
    "\n",
    "# Print the trimmed means for each column separately for the training set\n",
    "print(\"Train set trimmed means: \", train_trimmed_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28caa6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "skewness = skew(X_train.amount)\n",
    "print(skewness)\n",
    "skewness = skew(X_train.oldbalanceOrg)\n",
    "print(skewness)\n",
    "skewness = skew(X_train.newbalanceOrig)\n",
    "print(skewness)\n",
    "skewness = skew(X_train.oldbalanceDest)\n",
    "print(skewness)\n",
    "skewness = skew(X_train.newbalanceDest)\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee63f55",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5207c1",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier (n_estimators=150,random_state=18)\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23a42e",
   "metadata": {},
   "source": [
    "## Random Forest Impurity based feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the feature importance values\n",
    "importance_vals = rfc.feature_importances_\n",
    "\n",
    "# Sort importance values\n",
    "indices = np.argsort(importance_vals[::-1])\n",
    "\n",
    "# Plot the feature importance of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Random Forest Impurity based feature importance\")\n",
    "plt.bar(range(X.shape[1]), importance_vals[indices][::-1])\n",
    "\n",
    "plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylim([0, 0.2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_names_in_ = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b8d78",
   "metadata": {},
   "source": [
    "## Random Forest feature importance via permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.evaluate import feature_importance\n",
    "# from mlxtend.evaluate import feature_importance_permutation\n",
    "\n",
    "# imp_vals, imp_all = feature_importance_permutation(\n",
    "#     predict_method=rfc.predict,\n",
    "#     X=X_test.values,\n",
    "#     y=y_test.values,\n",
    "#     metric='accuracy',\n",
    "#     num_rounds=50,\n",
    "#     seed=18\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# std = np.std(imp_all, axis=1)\n",
    "# indices = np.argsort(imp_vals) [::-1]\n",
    "# plt. figure()\n",
    "# plt.title(\"Random Forest feature importance via permutation importance\")\n",
    "# plt.bar(range(X_train.shape[1]), imp_vals[indices], yerr=std[indices])\n",
    "\n",
    "# plt.xticks(range(X_train.shape[1]), df.columns[1:] [indices], rotation=90)\n",
    "# plt.xlim([-1, X_train.shape[1]])\n",
    "# plt.ylim( [0,0.2]\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt. show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9c76c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "# from mlxtend.evaluate import feature_importance_permutation\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Fit a random forest classifier to the training data\n",
    "# rfc=RandomForestClassifier (n_estimators=150,random_state=18)\n",
    "# rfc.fit(X_train,y_train)\n",
    "\n",
    "# # Calculate feature importance using permutation feature importance\n",
    "# imp_vals, imp_all = feature_importance_permutation(\n",
    "#     predict_method=rfc.predict,\n",
    "#     X=X_test.values,\n",
    "#     y=y_test,\n",
    "#     metric='accuracy',\n",
    "#     num_rounds=50,\n",
    "#     seed=18\n",
    "# )\n",
    "\n",
    "# # Get the feature names and sort the importance values\n",
    "# feat_names = X_test.columns\n",
    "# indices = np.argsort(imp_vals)[::-1]\n",
    "\n",
    "# # Plot the feature importance using a bar chart\n",
    "# plt.figure(figsize=(10,8))\n",
    "# plt.title(\"Random Forest Permutation Feature Importance\")\n",
    "# plt.bar(range(X_test.shape[1]), imp_vals[indices])\n",
    "# plt.xticks(range(X_test.shape[1]), feat_names[indices], rotation=90)\n",
    "# plt.xlim([-1, X_test.shape[1]])\n",
    "# plt.ylim([0, 0.2])\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a61f5f429d93a6bbb7c4c4e7bded7d11112cb15c4bcc13f16e4e4beb084be201"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
