{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f374f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90396195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bda14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41793035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f83c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a5f220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a3c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ced06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5184944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0        525   23819.260000   56731.220000             0.0        0.000000   \n",
      "1        138   44423.330000   48737.897272             0.0    49677.246485   \n",
      "2        325  131537.540000    4564.000000             0.0    49677.246485   \n",
      "3        308  300712.340000   51474.000000             0.0    49677.246485   \n",
      "4        349   47243.760000   11262.000000             0.0        0.000000   \n",
      "...      ...            ...            ...             ...             ...   \n",
      "423776   276  111168.880136  111168.880136             0.0    49677.246485   \n",
      "423777   274  131537.540000   48737.897272             0.0    49677.246485   \n",
      "423778    60  131537.540000   48737.897272             0.0        0.000000   \n",
      "423779   449   44882.356239   44882.356239             0.0        0.000000   \n",
      "423780   220   39953.091459   29059.334627             0.0    49677.246485   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0             0.000000               0     3    261386    620814  \n",
      "1        211221.120000               0     0     92881    180374  \n",
      "2        211221.120000               0     1     80756    482539  \n",
      "3        654217.020000               0     1    175711    597630  \n",
      "4             0.000000               0     3    162614     26253  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "423776   211221.120000               0     1     90379    472585  \n",
      "423777   211221.120000               0     1    112071    494845  \n",
      "423778        0.000000               0     1    154830    240268  \n",
      "423779    36237.626509               0     1    122579     88980  \n",
      "423780   211221.120000               0     1     93537    130866  \n",
      "\n",
      "[423781 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "485376  278.0   22928.58            0.0             0.0           0.000   \n",
      "642214   45.0    8606.90         5764.0             0.0           0.000   \n",
      "192982  237.0  220046.83            0.0             0.0      130797.505   \n",
      "99091   328.0   83938.53        13653.5             0.0      130797.505   \n",
      "203398  307.0   74636.86            0.0             0.0      130797.505   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "230877  154.0  195805.05        31725.0             0.0           0.000   \n",
      "315026  301.0   36352.03        13653.5             0.0           0.000   \n",
      "661254  238.5  163969.90        13653.5             0.0      130797.505   \n",
      "688112  280.0    3092.79            0.0             0.0           0.000   \n",
      "642560   35.0   74636.86        30807.0             0.0      130797.505   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "485376           0.000               0     3    291184    424837  \n",
      "642214           0.000               0     3    363649    442961  \n",
      "192982      214326.245               0     1      1853    410946  \n",
      "99091       537297.070               0     0    252825    347652  \n",
      "203398      214326.245               0     1    201182    417173  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "230877      195805.050               0     1    181881    192704  \n",
      "315026           0.000               0     3    458861    630843  \n",
      "661254      706564.020               0     0     37270    676511  \n",
      "688112           0.000               0     3    455345    152073  \n",
      "642560      214326.245               0     1    214954    689599  \n",
      "\n",
      "[70000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0385cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import module\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # compute required values\n",
    "# scaler = StandardScaler()\n",
    "# model = scaler.fit(X_train_resampled_final)\n",
    "# X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc22192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute required values\n",
    "# scaler = StandardScaler()\n",
    "# model = scaler.fit(X_test)\n",
    "# X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8d4fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the prior weight distribution as Normal of mean=0 and stddev=1.\n",
    "# # Note that, in this example, the we prior distribution is not trainable,\n",
    "# # as we fix its parameters.\n",
    "\n",
    "# # Import TensorFlow\n",
    "# import tensorflow as tf\n",
    "# def prior(kernel_size, bias_size, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     prior_model = keras.Sequential(\n",
    "#         [\n",
    "#             tfp.layers.DistributionLambda(\n",
    "#                 lambda t: tfp.distributions.MultivariateNormalDiag(\n",
    "#                     loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
    "#                 )\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     return prior_model\n",
    "\n",
    "\n",
    "# # Define variational posterior weight distribution as multivariate Gaussian.\n",
    "# # Note that the learnable parameters for this distribution are the means,\n",
    "# # variances, and covariances.\n",
    "# def posterior(kernel_size, bias_size, dtype=None):\n",
    "#     n = kernel_size + bias_size\n",
    "#     posterior_model = keras.Sequential(\n",
    "#         [\n",
    "#             tfp.layers.VariableLayer(\n",
    "#                 tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
    "#             ),\n",
    "#             tfp.layers.MultivariateNormalTriL(n),\n",
    "#         ]\n",
    "#     )\n",
    "#     return posterior_model\n",
    "\n",
    "\n",
    "# def create_bnn_model(train_size):\n",
    "#     inputs = create_model_inputs()\n",
    "#     features = keras.layers.concatenate(list(inputs.values()))\n",
    "#     features = layers.BatchNormalization()(features)\n",
    "\n",
    "#     # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
    "#     for units in hidden_units:\n",
    "#         features = tfp.layers.DenseVariational(\n",
    "#             units=units,\n",
    "#             make_prior_fn=prior,\n",
    "#             make_posterior_fn=posterior,\n",
    "#             kl_weight=1 / train_size,\n",
    "#             activation=\"sigmoid\",\n",
    "#         )(features)\n",
    "\n",
    "#     # The output is deterministic: a single point estimate.\n",
    "#     outputs = layers.Dense(units=1)(features)\n",
    "#     model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# # Example values for function calls\n",
    "# kernel_size = 64\n",
    "# bias_size = 32\n",
    "# dtype = tf.float32\n",
    "# train_size = 1000\n",
    "# hidden_units = [128, 64, 32]\n",
    "\n",
    "# # Call the prior function\n",
    "# prior_model = prior(kernel_size, bias_size, dtype)\n",
    "\n",
    "# # Call the posterior function\n",
    "# posterior_model = posterior(kernel_size, bias_size, dtype)\n",
    "\n",
    "# # Call the create_bnn_model function\n",
    "# model = create_bnn_model(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "train_sample_size = int(train_size * 0.3)\n",
    "small_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)\n",
    "\n",
    "bnn_model_small = create_bnn_model(train_sample_size)\n",
    "run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5920d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train_resampled_final, y=train_resampled_final, epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fea733",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "train_sample_size = int( * 0.3)\n",
    "small_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)\n",
    "\n",
    "bnn_model_small = create_bnn_model(train_sample_size)\n",
    "run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aa80fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled_final_first_50 = X_train_resampled_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b55d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled_final_first_50 = y_train_resampled_final[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c44ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testnew = X_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_testnew = y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "class KernelDensityTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bandwidth=1.0, kernel='gaussian'):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.kernel = kernel\n",
    "        self.kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.kde.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.kde.score_samples(X).reshape(-1, 1)\n",
    "\n",
    "parzen = Pipeline([\n",
    "    ('kde', KernelDensityTransformer()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "\n",
    "print(\"a\")\n",
    "# train the Parzen window classifier on the training data\n",
    "parzen.fit(X_train_resampled_final,y_train_resampled_final)\n",
    "print(\"b\")\n",
    "\n",
    "# use PNN to estimate the probability density function of each class\n",
    "from neupy import algorithms, layers\n",
    "\n",
    "print(\"c\")\n",
    "pnn = algorithms.PNN(std=0.1, verbose=False)\n",
    "pnn.train(X_train_resampled_final, y_train_resampled_final)\n",
    "print(\"d\")\n",
    "\n",
    "# combine the probability density functions of the PNN and Parzen Window classifiers\n",
    "def combine_probability_density_functions(X):\n",
    "    pnn_probabilities = pnn.predict_proba(X)\n",
    "    parzen_probabilities = np.exp(parzen.score_samples(X))\n",
    "    combined_probabilities = np.multiply(pnn_probabilities, parzen_probabilities)\n",
    "    return combined_probabilities\n",
    "\n",
    "print(\"e\")\n",
    "# make predictions on the test data using the combined probability density functions\n",
    "#y_pred = np.argmax(combine_probability_density_functions(X_testnew ), axis=1)\n",
    "print(\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a26d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Compute predictions for test data\n",
    "y_pred = pnn.predict(X_testnew)\n",
    "\n",
    "# Compute metrics\n",
    "print(classification_report(y_testnew, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b2b35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
