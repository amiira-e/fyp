{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2391b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_sample=pd.read_csv(r\"C:\\Users\\23059\\OneDrive\\Desktop\\Amiira\\Y3S1\\fyp\\sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75736240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ea507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['type'])\n",
    "label\n",
    "df_sample.drop(\"type\", axis=1, inplace=True)\n",
    "df_sample[\"type\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameDest'])\n",
    "label\n",
    "df_sample.drop(\"nameDest\", axis=1, inplace=True)\n",
    "df_sample[\"nameDest\"] = label\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(df_sample['nameOrig'])\n",
    "label\n",
    "df_sample.drop(\"nameOrig\", axis=1, inplace=True)\n",
    "df_sample[\"nameOrig\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54afa3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n",
      "0    0.998743\n",
      "1    0.001257\n",
      "Name: isFraud, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sample.drop('isFraud', axis=1)\n",
    "# Separate the target variable\n",
    "y = df_sample['isFraud']\n",
    "\n",
    "# Print class distribution before split\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X contains your features and y contains your target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=2)\n",
    "\n",
    "# Print class distribution after split\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d670a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#Upsampling via SMOTE\n",
    "smote = SMOTE(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "#Downsample via RandomUnderSampler\n",
    "rus = RandomUnderSampler(sampling_strategy=0.4, random_state=0)\n",
    "\n",
    "#Application of the resampling methods\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecbc65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "#Resample using TomekLinks first\n",
    "tomek_links = TomekLinks(sampling_strategy='majority')\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c7076ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, EditedNearestNeighbours,OneSidedSelection\n",
    "# resample the output of TomekLinks using EditedNearestNeighbours\n",
    "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
    "X_train_resampled_new, y_train_resampled_new = enn.fit_resample(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d368fd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import OneSidedSelection\n",
    "# resample the output of EditedNearestNeighbours using One-Sided Selection\n",
    "oss = OneSidedSelection(sampling_strategy='majority')\n",
    "X_train_resampled_final, y_train_resampled_final = oss.fit_resample(X_train_resampled_new, y_train_resampled_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40224c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        step         amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "0        332  228610.170000       0.000000             0.0    23311.081165   \n",
      "1        138   44423.330000   49546.000000             0.0    23311.081165   \n",
      "2        325  121452.605000    4564.000000             0.0    23311.081165   \n",
      "3        308  300712.340000   51474.000000             0.0    23311.081165   \n",
      "4        349   47243.760000   11262.000000             0.0        0.000000   \n",
      "...      ...            ...            ...             ...             ...   \n",
      "435569   276  111168.880136  111168.880136             0.0    23311.081165   \n",
      "435570   274  121452.605000   49546.000000             0.0    23311.081165   \n",
      "435571    60  121452.605000   49546.000000             0.0        0.000000   \n",
      "435572   449   44882.356239   44882.356239             0.0        0.000000   \n",
      "435573   220   39953.091459   29059.334627             0.0    23311.081165   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "0        167916.140000               0     1     43505    247504  \n",
      "1        167916.140000               0     0     92881    180374  \n",
      "2        167916.140000               0     1     80756    482539  \n",
      "3        167916.140000               0     1    175711    597630  \n",
      "4             0.000000               0     3    167971     26253  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "435569   167916.140000               0     1     90379    472585  \n",
      "435570   167916.140000               0     1    112071    494845  \n",
      "435571        0.000000               0     1    154830    240268  \n",
      "435572    36237.626509               0     1    122579     88980  \n",
      "435573   167916.140000               0     1     93537    130866  \n",
      "\n",
      "[435574 rows x 10 columns]\n",
      "         step     amount  oldbalanceOrg  newbalanceOrig  oldbalanceDest  \\\n",
      "485376  278.0   22928.58            0.0             0.0           0.000   \n",
      "642214   45.0    8606.90         5764.0             0.0           0.000   \n",
      "192982  237.0  220046.83            0.0             0.0      130797.505   \n",
      "99091   328.0   83938.53        13653.5             0.0      130797.505   \n",
      "203398  307.0   74636.86            0.0             0.0      130797.505   \n",
      "...       ...        ...            ...             ...             ...   \n",
      "230877  154.0  195805.05        31725.0             0.0           0.000   \n",
      "315026  301.0   36352.03        13653.5             0.0           0.000   \n",
      "661254  238.5  163969.90        13653.5             0.0      130797.505   \n",
      "688112  280.0    3092.79            0.0             0.0           0.000   \n",
      "642560   35.0   74636.86        30807.0             0.0      130797.505   \n",
      "\n",
      "        newbalanceDest  isFlaggedFraud  type  nameDest  nameOrig  \n",
      "485376           0.000               0     3    291184    424837  \n",
      "642214           0.000               0     3    363649    442961  \n",
      "192982      214326.245               0     1      1853    410946  \n",
      "99091       537297.070               0     0    252825    347652  \n",
      "203398      214326.245               0     1    201182    417173  \n",
      "...                ...             ...   ...       ...       ...  \n",
      "230877      195805.050               0     1    181881    192704  \n",
      "315026           0.000               0     3    458861    630843  \n",
      "661254      706564.020               0     0     37270    676511  \n",
      "688112           0.000               0     3    455345    152073  \n",
      "642560      214326.245               0     1    214954    689599  \n",
      "\n",
      "[70000 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# convert X_test to a pandas dataframe\n",
    "X_test = pd.DataFrame(X_test, columns=X_train.columns)\n",
    "# define a function to replace outliers with MAD for a single column\n",
    "def replace_outliers_with_mad(column):\n",
    "    median = np.median(column)\n",
    "    mad = np.median(np.abs(column - median))\n",
    "    threshold = 2.5 * mad\n",
    "    column[np.abs(column - median) > threshold] = median\n",
    "    return column\n",
    "\n",
    "# apply the function to all columns of X_train_resampled_final\n",
    "for i in range(X_train_resampled_final.shape[1]):\n",
    "    X_train_resampled_final.iloc[:, i] = replace_outliers_with_mad(X_train_resampled_final.iloc[:, i])\n",
    "\n",
    "# apply the function to all columns of X_test\n",
    "for i in range(X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = replace_outliers_with_mad(X_test.iloc[:, i])\n",
    "\n",
    "# convert the numpy arrays back to pandas dataframes\n",
    "X_train_resampled_final = pd.DataFrame(X_train_resampled_final, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test.columns)\n",
    "\n",
    "# print the modified dataframes\n",
    "print(X_train_resampled_final)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43dc79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_train_resampled_final)\n",
    "X_train_resampled_final = model.transform(X_train_resampled_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec20d071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute required values\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_test)\n",
    "X_test = model.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000023131ABDD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000023131ABDD38> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\23059\\anaconda3\\envs\\test2\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3372/3403 [============================>.] - ETA: 0s - loss: 0.6174WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000023136CE1AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000023136CE1AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.6162 - val_loss: 0.5757\n",
      "Epoch 2/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4818 - val_loss: 0.5332\n",
      "Epoch 3/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4581 - val_loss: 0.5180\n",
      "Epoch 4/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4467 - val_loss: 0.5091\n",
      "Epoch 5/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4395 - val_loss: 0.5031\n",
      "Epoch 6/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4345 - val_loss: 0.4989\n",
      "Epoch 7/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4309 - val_loss: 0.4958\n",
      "Epoch 8/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4282 - val_loss: 0.4933\n",
      "Epoch 9/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4260 - val_loss: 0.4913\n",
      "Epoch 10/10\n",
      "3403/3403 [==============================] - 9s 3ms/step - loss: 0.4243 - val_loss: 0.4899\n",
      "Epoch 1, Train MSE: 0.61624, Validation MSE: 0.57571\n",
      "Epoch 2, Train MSE: 0.48179, Validation MSE: 0.53315\n",
      "Epoch 3, Train MSE: 0.45812, Validation MSE: 0.51798\n",
      "Epoch 4, Train MSE: 0.44675, Validation MSE: 0.50909\n",
      "Epoch 5, Train MSE: 0.43951, Validation MSE: 0.50308\n",
      "Epoch 6, Train MSE: 0.43453, Validation MSE: 0.49886\n",
      "Epoch 7, Train MSE: 0.43091, Validation MSE: 0.49575\n",
      "Epoch 8, Train MSE: 0.42818, Validation MSE: 0.4933\n",
      "Epoch 9, Train MSE: 0.42603, Validation MSE: 0.49133\n",
      "Epoch 10, Train MSE: 0.4243, Validation MSE: 0.48987\n",
      "\n",
      "Fold 2\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4889 - val_loss: 0.4225\n",
      "Epoch 2/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4877 - val_loss: 0.4216\n",
      "Epoch 3/10\n",
      "3403/3403 [==============================] - 9s 3ms/step - loss: 0.4867 - val_loss: 0.4207\n",
      "Epoch 4/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4859 - val_loss: 0.4198\n",
      "Epoch 5/10\n",
      "3403/3403 [==============================] - 9s 3ms/step - loss: 0.4851 - val_loss: 0.4193\n",
      "Epoch 6/10\n",
      "3403/3403 [==============================] - 8s 2ms/step - loss: 0.4845 - val_loss: 0.4185\n",
      "Epoch 7/10\n",
      "3403/3403 [==============================] - 9s 3ms/step - loss: 0.4839 - val_loss: 0.4180\n",
      "Epoch 8/10\n",
      "3403/3403 [==============================] - 9s 3ms/step - loss: 0.4834 - val_loss: 0.4175\n",
      "Epoch 9/10\n",
      "3383/3403 [============================>.] - ETA: 0s - loss: 0.4830"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "np.random.seed(42)\n",
    "# Define the autoencoder architecture\n",
    "input_dim = X_train_resampled_final.shape[1]\n",
    "encoding_dim = 17\n",
    "decoding_dim = 10\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "#hidden_layer = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Add L1 regularization to the hidden layer\n",
    "hidden_layer = Dense(encoding_dim, activation='relu', kernel_regularizer=regularizers.l1(0.00991))(input_layer)\n",
    "\n",
    "output_layer = Dense(decoding_dim, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "# Define the optimizer with the desired learning rate\n",
    "\n",
    "opt = Adam(lr= 0.00087)\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "#autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define lists to store the MSE of training and validation sets for each fold\n",
    "train_mse = []\n",
    "val_mse = []\n",
    "recon_errors = []\n",
    "test_mse = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_train_resampled_final)):\n",
    "    # Print the fold number\n",
    "    print(f\"\\nFold {fold+1}\")\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_resampled_final[train_index], X_train_resampled_final[val_index]\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "    # Fit the autoencoder on the training set for the current fold\n",
    "    history = autoencoder.fit(X_train_fold, X_train_fold, epochs=10, batch_size=64, verbose=1, validation_data=(X_val_fold, X_val_fold), callbacks=[early_stopping])\n",
    "    \n",
    "    for i in range(len(history.history['loss'])):\n",
    "        train_mse_epoch = round(history.history['loss'][i], 5)\n",
    "        val_mse_epoch = round(history.history['val_loss'][i], 5)\n",
    "        print(f\"Epoch {i+1}, Train MSE: {train_mse_epoch}, Validation MSE: {val_mse_epoch}\")\n",
    "        train_mse.append(train_mse_epoch)\n",
    "        val_mse.append(val_mse_epoch)\n",
    "\n",
    "    # Append the mean MSE of training and validation sets for the current fold to the lists\n",
    "    train_mse.append(np.mean(history.history['loss']))\n",
    "    val_mse.append(np.mean(history.history['val_loss']))\n",
    "\n",
    "    \n",
    "#     # compute the reconstruction error for the test data\n",
    "#     recon_error = autoencoder.evaluate(X_test, X_test, verbose=0)\n",
    "#     recon_errors.append(recon_error)\n",
    "\n",
    "# Print the final mean and standard deviation of MSE for training and validation sets across all folds\n",
    "print(\"\\nFinal results:\")\n",
    "print(f\"Mean Train MSE: {np.mean(train_mse):.5f}\")\n",
    "print(f\"Mean Validation MSE: {np.mean(val_mse):.5f}\")\n",
    "\n",
    "# Plot the MSE of training and validation sets against the number of epochs\n",
    "epochs = range(1, len(mean_train_mse)+1)\n",
    "plt.plot(epochs, mean_train_mse, 'b', label='Training MSE')\n",
    "plt.fill_between(epochs, mean_train_mse - std_train_mse, mean_train_mse + std_train_mse, alpha=0.2, color='b')\n",
    "plt.plot(epochs, mean_val_mse, 'r', label='Validation MSE')\n",
    "plt.fill_between(epochs, mean_val_mse - std_val_mse, mean_val_mse + std_val_mse, alpha=0.2, color='r')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bcf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = autoencoder.evaluate(X_test, X_test)\n",
    "print('Mean squared error on test data:', mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
